# Model criticism

```{python}
#| echo : false
import arviz as az
import numpy as np
import preliz as pz
import pymc as pm
import matplotlib.pyplot as plt
```

```{python}
#| echo : false
az.style.use("arviz-doc")
SEED = 3592
np.random.seed(SEED)
y = np.random.normal(0, 1, 100)
```

Models are simplifications of reality, sometimes even very crude simplifications. Thus, we can never fully trust them. While hoping they are good enough is an option, we should try to do better. One general approach to criticizing model is to judge them by their predictions. If a model is robust, its predictions should align well with observations, domain knowledge, or some other benchmark. There are at least four avenues to explore:

* Compare to prior knowledge. For example, if we are modeling the size of planets we can evaluate if the model is making predictions in a sensible range. Even if we are equipped with a very rudimentary knowledge of astronomy we know that planets are larger than persons and smaller than galaxies. So if the model is predicting that the size of a planet is 1 meter, then we know that the model is not that good. The better your prior knowledge the better you will be able to critique your models, thus if you are not an expert in the field (and maybe even if you are), you should try to get some help from someone who is.

* Compare to observed data. We fit a model and compare the predictions to the same data that we used to fit the model. This is an internal consistency check of the model, and we should expect good agreement. But reality is complex and models can be too simple or they can be misspecified so there is a lot of potential in these types of checks. Additionally, even very good models might be good at recovering some aspects of the data but not others, for instance, a model could be good at predicting the bulk of the data, but it could overestimate extreme values.

* Compared to unobserved data. We fit a model to one dataset and then evaluate it on a different dataset. This is similar to the previous point, but it offers a more robust evaluation because we are assessing the model on data it has not encountered before. How similar the observed and unobserved data are can vary a lot. 

* Compare to other models. We fit different models to the same data and then compare the predictions of the models. This particular case is discussed in detail on @sec-model-comparison.

As we can see there are plenty of options to evaluate models. But we still have one additional ingredient to add to the mix, we have omitted the fact that we have different types of predictions. An attractive feature of the Bayesian model is that they are generative. This means that we can simulate synthetic data from models as long as the parameters are assigned a proper probability distribution, computationally we need a distribution from which we can generate random samples. We can take advantage of this feature to check models before or after fitting the data: 

* Prior predictive: We generate synthetic observations without conditioning on the observed data. These are predictions that we can make before we have seen the actual data. 
* Posterior predictive: We generate synthetic observations after conditioning on the observed data. These are predictions that we can make after we have seen the data.

Additionally, for models like linear regression where we have a set of covariates, we can generate synthetic data evaluated at the observed covariates (our "Xs") or at different values ("X_new"). If we do the first we call it in-sample predictions, and if we do the second we call it out-of-sample predictions.

With so many options we can feel overwhelmed. Which ones we should use will depend on what we want to evaluate. We can use a combination of the previous options to evaluate models for different purposes. In the next sections, we will see how to implement some of these checks.


## Prior predictive checks


The general algorithm for prior predictive checks is:

1. Draw $N$ realizations from a prior distribution.
2. For each draw, simulate new data from the likelihood.
3. Plot the results.
4. Use domain knowledge to assess whether simulated values reflect prior knowledge.
5. If simulated values do not reflect prior knowledge, change the prior distribution, likelihood, or both and repeat the simulation from step 1.
6. If simulated values reflect prior knowledge, compute the posterior.

Notice that in step 4 we use domain knowledge, NOT observed data!

In steps 1 and 2 what we are doing is approximating this integral:
$$
p(y^\ast) = \int_{\Theta} p(y^\ast \mid \theta) \; p(\theta) \; d\theta
$$

where $y^\ast$ represents unobserved but potentially observable data, according to our model. The data generated is predictive since it is the data that the model will expect to see, that is, it is unobserved but potentially observable data.

Notice that what we are doing is marginalizing the likelihood by integrating over all possible values ​​of the prior. Therefore, from the perspective of our model, we are describing the marginal distribution of the data, that is, regardless of the values of the parameters. 

To exemplify a prior predictive check, let's try with a super simple example. Let's say we want to model the height of humans. We know that the heights are positive numbers, so we should use a distribution that assigns zero mass to negative values. But we also know that at least for adults using a normal distribution could be a good approximation. So we create the following model, without too much thought, and then draw 500 samples from the prior predictive distribution.

::: {.panel-tabset}
## PyMC

``` {python}
with pm.Model() as model: 
    # Priors for unknown model parameters
    mu = pm.Normal('mu', mu=0, sigma=10)
    sigma = pm.HalfNormal('sigma', sigma=10)
    # Likelihood (sampling distribution) of observations
    y_obs = pm.Normal('Y_obs', mu=mu, sigma=sigma, observed=y)
    # draw 500 samples from the prior
    idata = pm.sample_prior_predictive(samples=500, random_seed=SEED)
```

## PyStan

``` {.python}
## coming soon
```
:::

In the following plot, we can see samples from the prior predictive distribution (blue solid lines). If we aggregate all the individual samples into a single large sample we get the dashed cyan line. To help us interpret this plot we have added two **reference lines**, the average length/height of a newborn and the average (male) adult height. These references are approximate values, but they are useful to provide a sense of the scale of the expected data. We can see that our model is bananas, not only heights can be negative, but the bulk of the prior predictive distribution is outside of our reference values.

```{python}
ax = az.plot_ppc(idata, group="prior")
ax.axvline(50, color="0.5", linestyle=":", label="newborn")
ax.axvline(175, color="0.5", linestyle="--", label="adult")
plt.legend()
```

We can tighten up the priors. There is no general rule to do this. For most problems, it is usually a good idea to set priors using some broad domain knowledge in such a way that we get a prior predictive distribution that allocates most of its mass in a reasonable region. These priors are called weakly informative priors. While there isn't a formal definition of what a weakly informative prior is, we can think of them as priors that will generate a prior predictive with none to very little mass in disallowed or unlikely regions. For instance, we can use a normal distribution with a mean of 175 and a standard deviation of 10. This distribution doesn't exclude negative values, but it assigns very little mass to them. Also is broad enough to allow for a wide range of values.

::: {.panel-tabset}
## PyMC

``` {python}
with pm.Model() as model: 
    # Priors for unknown model parameters
    mu = pm.Normal('mu', mu=175, sigma=10)
    sigma = pm.HalfNormal('sigma', sigma=10)
    # Likelihood (sampling distribution) of observations
    y_obs = pm.Normal('Y_obs', mu=mu, sigma=sigma, observed=y)
    # draw 500 samples from the prior
    idata = pm.sample_prior_predictive(samples=500, random_seed=SEED)
```

## PyStan

``` {.python}
## coming soon
```
:::

We repeat the previous plot with the new prior predictive distribution. We can see that the bulk of the prior predictive distribution is now within the reference values. The model predicts values above 200 cm and below 150 cm, which are indeed possible but are less likely. You are free to pick other priors and other reference values. Maybe you can use the historical record for the taller and shorter persons in the world as reference values.

```{python}
ax = az.plot_ppc(idata, group="prior")
ax.axvline(50, color="0.5", linestyle=":", label="newborn")
ax.axvline(175, color="0.5", linestyle="--", label="adult")
plt.legend()
```

Weakly informative priors can have a practical advantage over informative priors because usually we do not have enough information to set very informative priors or getting good informative priors can be very time-consuming. What's more weakly informative priors can also have practical advantages over very vague priors because adding some prior information is usually better than adding none. By adding some information we reduce the chances of spurious results or non-sensical results. Additionally, weakly informative priors can even help with faster sampling. 

One aspect that is often overlooked is that even if the priors have little effect on the actual posterior, by doing prior predictive checks and *playing* with the priors we can get a better understanding of our models and problems. If you want to learn more about prior elicitation you can check the [PreliZ](https://preliz.readthedocs.io/) library.

One final note. When plotting many distributions, where each one spans a narrow range of values compared to the range spanned but the collection of distributions, it is usually a good idea to plot the cumulative distribution. This is because the individual distributions can be hard to see, but the cumulative distribution is easier to interpret. 

```{python}
ax = az.plot_ppc(idata, group="prior", kind="cumulative")
ax.axvline(50, color="0.5", linestyle=":", lw=3, label="newborn")
ax.axvline(175, color="0.5", linestyle="--", lw=3, label="adult")
plt.legend()
```

## Posterior predictive checks

The idea behind posterior predictive checks is very general and simple: if a model is good it should be able generate data resembling the observed data. We call these checks, *posterior predictive* because we are generating synthetic data after seeing the data.

The general algorithm for posterior predictive checks is:

1. Draw $N$ realizations from the posterior distribution.
2. For each draw, simulate new data from the likelihood.
3. Plot the results.
4. Use observed data to assess whether simulated values agree with observed values.
5. If simulated values do not agree with observations, change the prior distribution, likelihood, or both and repeat the simulation from step 1.
6. If simulated values reflect prior knowledge, compute the posterior.

Notice that in contrast with prior predictive checks, we use observations here. Of course, we can also include domain knowledge to assess whether the simulated values are reasonable, but because we are using observations we do more stringents evaluations. 

In steps 1 and 2 what we are doing is approximating this integral:
$$
p(\tilde y) = \int_{\Theta} p(\tilde y \mid \theta) \; p(\theta \mid y) \; d\theta
$$

where $\tilde y$ represents new observations, according to our model. The data generated is predictive since it is the data that the model expects to see.

Notice that what we are doing is marginalizing the likelihood by integrating all possible values ​​of the posterior. Therefore, from the perspective of our model, we are describing the marginal distribution of data, that is, regardless of the values of the parameters. 

Continuing with our height example, we can generate synthetic data from the posterior predictive distribution.


::: {.panel-tabset}
## PyMC

``` {python}
with model: 
    idata = pm.sample()
    pm.sample_posterior_predictive(idata, random_seed=SEED, extend_inferencedata=True)
```

## PyStan

``` {.python}
## coming soon
```
:::

And then we use ArviZ to plot the comparison. We can see that the model is doing a good job at predicting the data. The observed data (black line) is within the bulk of the posterior predictive distribution (blue lines).

The dashed orange line, labeled as "posterior predictive mean", is the aggregated posterior predictive distribution. If you combine all the individual densities into a single density, that's what you would get. 

```{python}
az.plot_ppc(idata, num_pp_samples=200, colors=["C0", "k", "C2"]);
```

Other common visualizations to compare observed and precitive values are empirical CDFs, histograms and less often quantile dotplots. Like with other types of visulizations, you may want to try different options, to be sure visualizations are not missleading and you may also want to adapt the visualization to your audience.

### Using summary statistics

Besides directly comparing observations and predictions in terms of their densities, we can do comparisons in terms of summary statistics. Which ones, we decide to use may vary  from one data-analysis problem to another, and ideally it should be informed by the data-analysis goals. 

The following plot shows a comparison in terms of the mean, median and interquantile range (IQR). The dots at the bottom of each subplots corresponds to the summmary statistics computed for the observed data and the KDE is for the model's predictions.

```{python}
_, ax = plt.subplots(1, 3, figsize=(12, 3))

def iqr(x, a=-1):
    """interquartile range"""
    return np.subtract(*np.percentile(x, [75, 25], axis=a))

az.plot_bpv(idata, kind="t_stat", t_stat="mean", ax=ax[0])
az.plot_bpv(idata, kind="t_stat", t_stat="median", ax=ax[1])
az.plot_bpv(idata, kind="t_stat", t_stat=iqr, ax=ax[2])
ax[0].set_title("mean")
ax[1].set_title("median")
ax[2].set_title("IQR");
```

The numerical values labeled as `bpv` correspond to the following probability:

$$
p(T_{\text{sim}} \le T_{\text{obs}} \mid \tilde y)
$$


Where $T$ is the summary statistic of our choice, computed for both the observed data $T_{\text{obs}}$ and the simulated data $T_{\text{sim}}$. 

This probabilities are often called Bayesian p-values, and their ideal value is 0.5, meaning that half of the predictions are below the observed values and half above.


The following paragraph is for those of you that are familiar with the p-values as used in frequentist statistics for null hypotesis testing and computation of "statistical significance". If that's the case you should be aware that the Bayesian p-values are something different. While frequentist p-values measure the probability of obtaining results at least as extreme as the observed data under the assumption that the null hypothesis is true, Bayesian p-values assess the discrepancy between the observed data and simulated data based on the posterior predictive distribution. They are used as a diagnostic tool to evaluate the adequacy of a model rather than as a measure of "statistical significance" or evidence against a null hypothesis.

Bayesian p-values are not tied to a null hypothesis and are not subject to the same kind of binary decision-making framework (reject/accept). Instead, they provide insight into how well the model, given the data, predicts the summary statistics. Values close to 0.5 indicates a well-calibrated model. However, the interpretation is more nuanced, as the goal is model assessment and improvement rather than hypothesis testing.

If you want to use `plot_bpv` to do the plots, but you prefer to ommit the Bayesian p-values pass `bpv=False`. `plot_bpv` supports to other types of plots `kind="p_value"`, that corresponds to computing 

$$
p(\tilde y \le y_{\text{obs}} \mid y)
$$

that is, instead of using a summary statistics, as before, we directly compare observations and predictions. In the following plots the dashed black line represent the expected distribution.

```{python}
az.plot_bpv(idata, kind="p_value");
```

Another possibility is to perform the comparison per observation.

$$
p(\tilde y_i \le y_i \mid y)
$$

This is often called the marginal p-value and the ideal distribution is the standard uniform distribution.

The white line in the following figure represents the ideal scenario and the grey band the expected deviation given the size of the data. The x values corresponds to the quantiles of the original distribution, i.e. the central values represent the "bulk" of the distribution and the extreme values the "tails".

```{python}
az.plot_bpv(idata);
```

To build intuition, let's look at some synthetic data. The following plot shows four different scenarios, where the observed data follows a standard normal distribution ($\mu=0, \sigma^2=1$). In each case, we compare the observed data to predictions where:

* The mean of the predictions is shifted to the right.
* The mean of the predictions is shifted to the left.
* The predictions have a wider spread (larger variance).
* The predictions have a narrower spread (smaller variance).

```{python}
observed = pz.Normal(0, 1).rvs(500)

_, axes = plt.subplots(2, 2, sharex=True, sharey=True)

for ax, (mu, sigma) in zip(axes.ravel(), 
                           [(0.5, 1),   # shifted right
                            (-0.5, 1),  # shifted left
                            (0, 2),     # wider
                            (0, 0.5)]): # narrower
    
    idata_i = az.from_dict(posterior_predictive={"y":pz.Normal(mu, sigma).rvs((50, 500))},
                           observed_data={"y":observed})
    
    az.plot_bpv(idata_i, ax=ax);
    ax.set_ylim(0, 2.5)
```

### Hypothetical Outcome Plots

Another strategy that can be useful for posterior predictive plots is to use animations. Rather than showing a continuous probability distribution, Hypothetical Outcome Plots (HOPs) visualize a set of draws from a distribution, where each draw is shown as a new plot in either a small multiples or animated form. HOPs enable a user to experience uncertainty in terms of countable events, just like we experience probability in our day to day lives.

`az.plot_ppc` support animations using the option `animation=True`. 

You can read more about HPOs [here](https://medium.com/hci-design-at-uw/hypothetical-outcomes-plots-experiencing-the-uncertain-b9ea60d7c740).

## Posterior predictive checks for count data

Rootograms are a graphical tool to assess the fit of count data models [Kleiber and Zeileis, 2016]. Instead of plotting the observed counts against the expected counts, rootograms plot the square root of the observed counts against the square root of the expected counts. One common variation of rootogram is called hanging rootograms, as the predictions are ploted as dots and the observations as bars hanging from those plots. In the original rootograms, the predictions are ploted as dots and the predictions as bars, like in a histogram.

Here we are going to discuss hanging rootograms, but because we are interested in ploting the uncertainty in the predictions we are going to plot the observations as dots and the predictions are going to hang from the observations.

To illustrate rootograms, we are going to use the horseshoe crab dataset [Brockmann, 1996]. You don't need to understand the details of the dataset or model to understand this example, you just neeed to know that we want to predict the number of satellile male crabs based on fetures on the female crabs. And for this example we have two models for one we have used a Poisson likelihood and for the other a Hurdle NegativeBinomial distribution. The following figure shows one rootograms from each of these two models

![Bayesian rootograms](img/rootogram.png){#fig-rotograms width=70%}

How we read them. As we said the observations are the black dots. Hanging from them we have the predictions (thin line). At each count a predictions that perfectly match an observed count will look like a line that goes from the observation to 0, if the line is shorter the model is underestimating that particular count and if the line is below 0 the model is overestimating that count. To ease the comparison there is a horizontal dashed line at 0. Because we are Bayesian, we have predictions with uncertainty. The uncertainty is described by the box at the botoom of each vertical line. The semitransparent box represent the HDI 94%. As usual we could have chosen a diffent value for the HDI.

Just with a glimp we can see that the Hurdle Negative Binomial is a better match, as most of the boxes are around 0. Instead, for the Poisson model we see larger missmatchs, in particular the model is underestimating the zeros, and overestimating the 1s, 2s and 3s.

ArviZ currently does not currently support rootograms, but it will in the [future](https://arviz-plots.readthedocs.io/en/latest/gallery/plot_rootogram.html).

## Posterior predictive checks for binary data



## Prior sensitivity checks

##