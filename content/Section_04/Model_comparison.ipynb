{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import pymc3 as pm\n",
    "import arviz as az"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "To perform a Bayesian analysis we need to define a model (a combination of prior and likelihood) and then apply Bayes theorem. In a magical world we should be able to the define the *true* model corresponding to the *true* data generation process. In the real world, we can only hope to define a reasonable enough model to make predictions and/or explain the data. The process of finding this useful model is generally iterative and involves creating more than one candidate model. \n",
    "\n",
    "When faced with more than one model for the same data is *natural* to ask how these models compare to each other. One way to do this is to perform posterior predictive checks as we saw in the previous chapter. Another one is to evaluate the accuracy of the predictions on new data, *i.e.* data not used to fit the model in the first place. It is generally assumed that both datasets come from the *the true generating process*. \n",
    "\n",
    "The inconvenience of using new data, is that in general we can not afford the luxury of putting aside a portion of our data and not use it to fit a model. Even in a *big-data regime* we have good reasons to use all the available data, two of them are:\n",
    "\n",
    "* Reduce the uncertainty of our estimates\n",
    "* Increase the number of questions we can ask the data. \n",
    "\n",
    "For this reason a number of strategies have been develop in order to evaluate the accuracy o predictions using just the same data used to fit the model, as we will see next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Predictive accuracy\n",
    "\n",
    "Ideally, any measure of predictive accuracy should take into account the application at hand and it should include benefits and cost of the model's predictions. We discus such an example in the \"chapter XXX Decision-theory\". In this chapter we will, instead, discuss very general methods that are applicable to a wide range of models and problems.\n",
    "\n",
    "A pretty common way of measuring how well a model fits the data is to compute the quadratic mean error between a data-point ($y_i$) and a poitwise prediction $\\operatorname{E} (y_i \\mid \\theta))$:\n",
    "\n",
    "$$\\frac{1}{n} \\sum_{i=1}^{n}  (y_i - \\operatorname{E} (y_i \\mid \\theta))^2$$\n",
    "\n",
    "\n",
    "This is the average of the quadratic differences between observed and predicted data. By taking the square we ensure that (positive and negative) errors do not cancel out. Also by taking the square we penalize higher larger deviations, compared to using for example the absolute value of the differences \n",
    "\n",
    "\n",
    "When doing probabilistic forecasting measures of predictive accuracy are generally known as [scoring rules](https://en.wikipedia.org/wiki/Scoring_rule). Given a probability vector $\\mathbf{r}$ with a probability for each of the $i$ outcomes. A scoring rule will give a reward of $S({\\mathbf {r}}, i)$ if the $i$th event occurs. We say we have a proper scoring rule, if the highest expected reward is obtained by reporting the true probability distribution. A proper scoring rule is said to be local if its value depends only on the probability $r_{i}$.  It can be [shown](https://www.stat.washington.edu/raftery/Research/PDF/Gneiting2007jasa.pdf) that the logarithmic scoring rule is the only local and proper scoring rule. This is true up to an affine transformation, that is if $S(\\mathbf {r} ,i)$ is a strictly proper scoring rule then $a+b S({\\mathbf {r}},i)$ with $b>0$ is also a strictly proper scoring rule. Long story short, it is pretty common to use the log-likelihood $\\log p(y_i \\mid \\theta)$ as a measure of the point-wise predictive accuracy.\n",
    "\n",
    "When the likelihood is Gaussian, then the average log-likelihood will be proportional to the quadratic mean error. For historical reasons people use the *deviance* scale when talking about predictive accuracy, this is simply multiplying the log-likelihood by $-2$:\n",
    "\n",
    "$$-2\\ \\sum_{i=1}^{n} \\log \\ p(y_i \\mid \\theta)$$\n",
    "\n",
    "The *deviance* is used in both Bayesians and non-Bayesians context, in the later $\\theta$ is a probability distribution and in the former a point-estimate.\n",
    "\n",
    "\n",
    "> The lower the deviance, the larger the log-likelihood and thus the greater the agreement between model's predictions and data. We want smaller values of deviance.\n",
    "\n",
    "In principle the more complex a model (the more parameters to tune) the higher the deviance will be. This is reflecting the intuition that a model with more parameters will be in general more flexible and thus it will fit the data better. Thus relying only in the deviance could lead us to choose models prone to [overfitting](https://en.wikipedia.org/wiki/Overfitting). Over-fitting is the tendency of a model to adjust so well to the data used to fit it that it will be very bad at fitting (or generalizing) to new data. For this reason the deviance is used together with a term penalizing the over-complexity of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Akaike Information Criterion  \n",
    "\n",
    "This is probably the most well known IC, specially for non-Bayesians and is defined as the sum of two terms:\n",
    "\n",
    "$$AIC = -2 \\sum_{i=1}^{n} \\log p(y_i \\mid \\hat{\\theta}_{mle}) + 2 p_{AIC} $$\n",
    "\n",
    "\n",
    "The $\\log p(y_i \\mid \\hat{\\theta}_{mle})$, measures how well the model fits the data and the penalization term $p_{AIC}$. Here $\\hat{\\theta}_{mle}$ is the [maximum-likelihood estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) of $\\theta$ and $ p_{AIC}$ is just the number of parameters in the model. \n",
    "\n",
    "AIC performs well in non-bayesian settings, but is not well equipped to deal with the generality of Bayesian models. It does not use the full *a posteriori* distribution, discarding potentially useful information. On average AIC will behave worst as we increase the information in the priors, or in general the structure in our model. Thus it is not compatible with informative and weakly informative priors, neither with hierarchical models. AIC assumes that the posterior can be well represented (at least asymptotically) by a Gaussian distribution, but this is not true for a number of models, including hierarchical models, mixture models, neural networks, etc. Fortunately, we have better alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Widely available information criterion \n",
    "\n",
    "WAIC, generally pronounced as W-A-I-C, even when something like *wæɪk* is less of a mouthful ;-) can be regarded as a fully Bayesian extension of AIC. It also has two terms, although computed in a different way, being the most important difference that the terms are computed using the full posterior distribution, including the *effective* number of parameters. For details on the computation of WAIC please read the WAIC in depth section.\n",
    "\n",
    "\n",
    "## Pareto smoothed importance sampling leave-one-out cross validation\n",
    "\n",
    "[Cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) (CV) is another method of estimating out-of-sample prediction accuracy. This method requires re-fitting a model many times, each time excluding a portion of the data, the excluded portion is then used to measure the accuracy of them model. This process is repeated many times and the estimated accuracy of the model will be the average of each run. Then the entire data is used to fit the model one more time and this is the model used for further analysis and/or predictions. Leave-one-out cross-validation (LOO-CV) is a particular type of cross-validation when the data excluded is a single data-point. \n",
    "\n",
    "As CV can be quite time consuming (specially for Bayesian models) it is interesting to note that in theory it is possible to approximate LOO-CV. A practical and computational efficient way to do it requieres using a combination or strategies that includes was is call [Pareto smoothed importance sampling](https://arxiv.org/abs/1507.02646). The resulting method is known as PSIS-LOO-CV, while very useful, has a very complicated name, thus we just call it LOO. \n",
    "\n",
    "\n",
    "While LOO and WAIC approximate two different quantities, asymptotically they converge to the same numerical value, and also in practice they generally agree. The main advantage of LOO is that it is more informative as it provides [useful diagnostics](https://arxiv.org/abs/1507.04544) and other goodies such as effective sample size and Monte Carlo standard error estimates.\n",
    "\n",
    "Using ArviZ, both LOO and WAIC can be computed just by calling a function. Let's try on an arbitrary pre-loaded model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this to some good example\n",
    "model0 = az.load_arviz_data('regression1d')\n",
    "model1 = az.load_arviz_data('regression1d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Computed from 2000 by 100 log-likelihood matrix\n",
       "\n",
       "        Estimate       SE\n",
       "IC_waic   291.70    11.42\n",
       "p_waic      2.73        -"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "az.waic(model0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Computed from 2000 by 100 log-likelihood matrix\n",
       "\n",
       "       Estimate       SE\n",
       "IC_loo   291.71    11.42\n",
       "p_loo      2.73        -"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "az.loo(model0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see both WAIC and LOO return similar values. ArviZ comes equipped with `compare(.)` function. That is more convenient than using `loo(.)` or `waic(.)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Computed from 2000 by 100 log-likelihood matrix\n",
       "\n",
       "       Estimate       SE\n",
       "IC_loo   291.71    11.42\n",
       "p_loo      2.73        -"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "az.loo(model0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The compare function\n",
    "\n",
    "This function takes a dictionary of name (keys) models (values) as input and returns a DataFrame ordered (row-wise) from best to worst model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>waic</th>\n",
       "      <th>p_waic</th>\n",
       "      <th>d_waic</th>\n",
       "      <th>weight</th>\n",
       "      <th>se</th>\n",
       "      <th>dse</th>\n",
       "      <th>warning</th>\n",
       "      <th>waic_scale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>m0</th>\n",
       "      <td>0</td>\n",
       "      <td>291.702</td>\n",
       "      <td>2.72889</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>11.7583</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>deviance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m1</th>\n",
       "      <td>1</td>\n",
       "      <td>291.702</td>\n",
       "      <td>2.72889</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>11.7583</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>deviance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank     waic   p_waic d_waic weight       se dse warning waic_scale\n",
       "m0    0  291.702  2.72889      0    0.5  11.7583   0   False   deviance\n",
       "m1    1  291.702  2.72889      0    0.5  11.7583   0   False   deviance"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmp = az.compare({\"m0\":model0, \"m1\":model1,})\n",
    "cmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have many columns, so let check one by one their meaning:\n",
    "\n",
    "0) The index are the names of the models taken from the keys of the dictionary passed to `compare(.)`.\n",
    "\n",
    "1) The first column has the ranking on the models starting from 0 (best model) to the number of models.\n",
    "\n",
    "2) The second column contains the values of WAIC/LOO. The DataFrame is always sorted from best WAIC/LOO to worst. \n",
    "\n",
    "2) The third column has the value of the penalization term. We can roughly think of this value as the estimated effective number of parameters (but do not take that too seriously).\n",
    "\n",
    "3) The third column has the relative difference between the value of WAIC/LOO for the top-ranked model and the value of WAIC/LOO for each model. For this reason we will always get a value of 0 for the first model.\n",
    "\n",
    "4) The fourth column has the weights assigned to each model. These weights can be loosely interpreted as the probability of each model (among the compared models) given the data. See model averaging section for more details.\n",
    "\n",
    "5) The fifth column records the standard error for the WAIC/LOO computations. The standard error can be useful to assess the uncertainty of the WAIC/LOO estimates. By default this errors are computed using bootstrapping.\n",
    "\n",
    "6) In sixth column we have the standard error of the differences between two values of WAIC/LOO. The same way that we can compute the standard error for each value of WAIC/LOO, we can compute the standard error of the differences between two values of WAIC/LOO. Notice that both quantities are not necessarily the same, the reason is that the uncertainty about WAIC/LOO is correlated between models. This quantity is always 0 for the top-ranked model.\n",
    "\n",
    "7) The fifth column records if we got *warnings* when computing WAIC/LOO, the possible values can be `True` or `False`. If `True` the computation of WAIC/LOO may not be reliable, this warning for WAIC is based on an empirical determined cutoff value and need to be interpreted with caution. The warning for value has better empirical and theoretical support.\n",
    "\n",
    "7) Finally we have the last column named *scale*. The default is the deviance scale as previously mentioned this is obtained by multiplying the log-score by -2. Other options are `log`, this is the log-score multiplied by 1 (this reverts the order a higher WAIC/LOO will be better) and `negative-log`, this is the log-score multiplied by 1, as with the `deviance` scale a lower value is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The plot_compare function\n",
    "\n",
    "ArviZ also provides aanother convenience function that takes the output of `compare(.)` and produces a summary plot in the style of the one used in the book Statistical Rethinking by Richard McElreath."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "az.plot_compare(cmp);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The empty circle represents the values of WAIC/LOO and the black error bars associated with them are the values of the standard deviation of WAIC/LOO.\n",
    "\n",
    "The value of the best WAIC/LOO is also indicated with a vertical dashed grey line to ease comparison with other WAIC/LOO values.\n",
    "\n",
    "The filled black dots are the in-sample deviance of each model, i.e. the log-score without the penalty term.\n",
    "\n",
    "For all models except the top-ranked one we also get a triangle indicating the value of the difference of WAIC between that model and the top model and a grey errorbar indicating the standard error of the differences between the top-ranked WAIC/LOO and WAIC/LOO for each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point-wise model comparison\n",
    "\n",
    "Comparing models is a good way to get a better understanding about them... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes factors\n",
    "\n",
    "Bayes factors are a common alternative to information criteria. In fact this are a diving line between to wide group of Bayesian, those who use Bayes Factors and those who dislike them.\n",
    "\n",
    "To understand Bayes factors let us begin by writing Bayes theorem in a way that explicitly shows that our inference are always model dependent.\n",
    "\n",
    "$$p(\\theta \\mid y, M_k) = {\\frac {p(y \\mid \\theta, M_k)p(\\theta \\mid M_k)}{p(y \\mid M_k)}}$$\n",
    "\n",
    "Where, $y$ represents the data and $M$ the model:\n",
    "\n",
    "The term in the denominator is known as marginal likelihood (or evidence). When doing inference we do not need to compute this normalizing constant, so in practice we often compute the posterior up to a constant factor. However, for model comparison and model averaging the marginal likelihood becomes a relevant quantity. If our main objective is to choose only one model, the best one, from a set of $k$ models we can just choose the one with the largest $p(y \\mid M_k)$. As a general rule $p(y \\mid M_k)$ are tiny number and does not tell us too much on its own; like with information criteria, what matters are the relative values. So in practice people often compute the ratio of two marginal likelihoods, and this is call a Bayes factor:\n",
    "\n",
    "\n",
    "$$BF = \\frac{p(y \\mid M_0)}{p(y \\mid M_1)}$$\n",
    "\n",
    "Then when BF > 1, model 0 explains data better than model 1.\n",
    "\n",
    "Some authors have proposed tables with ranges to discretize and ease BF interpretation. The following table indicates the strength of the evidence favoring model 0 against model 1:\n",
    "\n",
    "* 1-3: anecdotal\n",
    "* 3-10: moderate\n",
    "* 10-30: strong\n",
    "* 30-100: very strong\n",
    "* \\> 100: extreme\n",
    "\n",
    "Remember, these rules are just conventions, simple guides at best. But results should\n",
    "always be put into context and should be accompanied with enough detail so \n",
    "other could potentially check if they agree with our conclusions. The evidence\n",
    "necessary to make a claim is not the same in particle physics, a court, or to\n",
    "evacuate a town to prevent hundreds of deaths.\n",
    "\n",
    "Using $p(y \\mid M_k)$ to compare model is totally fine if all models are assumed to have the same prior probability[<sup>1</sup>](#fn1). Otherwise, we have to compute the *posterior odds*:\n",
    "\n",
    "$$\\underbrace{\\frac{p(M_0 \\mid y)}{p(M_1 \\mid y)}}_\\text{posterior odds} = \\underbrace{\\frac{p(y \\mid M_0)}{p(y \\mid M_1)}}_\\text{Bayes factors} \\, \\underbrace{\\frac{p(\\ M_0 \\ )}{p(\\ M_1 \\ )}}_\\text{prior odds}$$\n",
    "\n",
    "<span id=\"fn1\"> <sup>1</sup> Notice that we are taking about the prior probability we assign to models and not about the priors we assign to parameters for each model</span>\n",
    "\n",
    "\n",
    "## All that glitters is not gold\n",
    "\n",
    "Now we will briefly discuss some key facts about the marginal likelihood. By carefully inspecting the definition of marginal likelihood we can understand their properties and consequences for their practical use:\n",
    "\n",
    "$$p(y \\mid M_k) = \\int_{\\theta_k} p(y \\mid \\theta_k, M_k) p(\\theta_k, M_k) d\\theta_k$$\n",
    "\n",
    "\n",
    "* The good: Models with more parameters have a larger penalization than models with fewer parameters. Bayes factor has a built-in Occam Razor! The intuitive reason is that the larger the number of parameters the more spread the prior with respect to the likelihood. Thus when computing the above integral you will get a smaller value than with a more concentrated prior.  \n",
    "\n",
    "* The bad: Computing the marginal likelihood is, generally, a hard task because the above it's an integral of a highly variable function over a high dimensional parameter space. In general this integral needs to be solved numerically using more or less sophisticated methods (see and example [here](https://docs.pymc.io/notebooks/Bayes_factor.html)).\n",
    "\n",
    "\n",
    "* The ugly: The marginal likelihood depends *sensitively* on the values of the priors\n",
    "\n",
    "Using the marginal likelihood to compare models is a good idea because a penalization for complex models is already included (thus preventing us from overfitting). At the same time, a change in the prior will affect the computations of the marginal likelihood. At first this sounds a little bit silly we already know that priors affect computations (otherwise we could simply avoid them), but the point here is the word sensitively. We are talking about changes in the prior that will keep inference of $\\theta$ more or less the same, but could have a big impact in the value of the marginal likelihood. You may have noticed in previous examples that, in general,  having a normal prior with a standard deviation of 100 is the same as having one with a standard deviation of 1,000, Instead, Bayes factors will be affected by these kind of changes.\n",
    "\n",
    "Another source of criticism to Bayes factors is that they can be used as a Bayesian way of doing hypothesis testing; there is  nothing wrong in this *per se*, but many authors have pointed out that an inference approach, similar to the one used in this book, is better suited to  most problems than the generally taught approach of hypothesis testing (whether  Bayesian or not Bayesian).\n",
    "\n",
    "## Bayes Factors vs Information Criteria\n",
    "\n",
    "WAIC/LOO uses the log-likelihood and the priors are not directly part of the computations, they are indirectly included because they will have an effect on the values of the models parameters. Instead, Bayes factor use priors directly as we need to average the likelihood over the prior. Conceptually we can say that Bayes factor are focused on identifying the best model (and the prior is part of the model) while WAIC and LOO are focused on which parameters will give the better predictions. As in most scenarios priors are used for their regularizing properties and when possible to provide some background-knowledge more than because we *really really believe* they reflect some *truth*, we think information criteria are a more robust approach in practice, also their computation is far less problematic and in generally is more robust without the need of using special samplers or methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Averaging\n",
    "\n",
    "Sometimes when comparing models, we do not want to select *the best* model, instead we want to perform predictions by averaging along all the models (or at least several models). Ideally we would like to perform a weighted average, giving more weight to the model that seems to explain/predict the data better. There are many approaches to perform this task, one of them is to use Akaike weights based on the values of WAIC for each model. These weights can be loosely interpreted as the probability of each model (among the compared models) given the data. One caveat of this approach is that the weights are based on point estimates of WAIC/LOO (i.e. the uncertainty is ignored).\n",
    "Ideally we would like to perform a weighted average, giving more weight to the model that seems to explain/predict the data better. There are many approaches to perform this task, one of them is to use Akaike weights based on the values of WAIC for each model. These weights can be loosely interpreted as the probability of each model (among the compared models) given the data. One caveat of this approach is that the weights are based on point estimates of WAIC/LOO (i.e. the uncertainty is ignored)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pp_weighted_samples = pm.sample_posterior_predictive_w(traces, weights=cmp.weight)\n",
    "# do a plot comparing best model predictions vs pp_weighted_samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
