{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WAIC in depth\n",
    "\n",
    "A useful conceptual scaffold when thinking about Bayesian statistics is to assume we have a sample $X^n = \\{x1, x2, \\dots, x_n\\}$ taken from some unknown probability distribution $q(x)$. Because we do not known $q(x)$ we somehow define a likelihood $p(x | \\theta)$ and a prior $p(\\theta)$, which we combine using Bayes' theorem to obtain:\n",
    "\n",
    "$$p(\\theta \\mid X^n) = \\frac{p(x | \\theta) p(\\theta)}{p(x)}$$\n",
    "\n",
    "knowing the posterior distribution $p(\\theta \\mid X^n)$, we can then compute the posterior predictive distribution:\n",
    "\n",
    "$$\\hat p(x) = p(x \\mid X^n) = \\int p(\\theta \\mid X^n)   p(x | \\theta) d\\theta$$\n",
    "\n",
    "We do all this hopping $\\hat p(x) \\approx q(x)$, but as the scientific-minded people we are we should do something better than hopping and at least try a honest attempt to evaluate our models. \n",
    "\n",
    "A naive attempt will be to compute the training loss, defined as:\n",
    "\n",
    "\n",
    "$$T_n = - \\frac{1}{n} \\sum_{i=1}^n \\log p(X_i \\mid X^n)$$\n",
    "\n",
    "The problem with the training loss is that in principle we can create a flexible enough model to make $p(X_i \\mid X^n)$ follows the empirical distribution of $X^n$ arbitrarily close. Such model will perform very good on $X^n$ bad very bad on new data, i.e. our model will overfits the data. \n",
    "\n",
    "Instead of $T_n$ we want to compute the generalization loss: \n",
    "\n",
    "$$G_n = - \\int q(x) \\log \\hat p(x) dx$$\n",
    "\n",
    "Notice that while we can numerically compute $T_n$ using just samples (and a model), without needing to know the true distribution $q(x)$. We can not numerically compute $G_n$ just from a set of samples. \n",
    "\n",
    "To compute $G_n$ the predictive distribution $\\hat p(x)$ is weighted by the true distribution $q(x)$. Why we want this? Because now the closer $\\hat p(x)$ becomes to $q(x)$ the lower the generalization error. In other words the generalization error will not get fooled by a overly-complex model. \n",
    "\n",
    "We can see that this is true by introducing the entropy of $q(x)$:\n",
    "\n",
    "\n",
    "$$S = - \\int q(x) \\log q(x) dx$$\n",
    "\n",
    "\n",
    "If we now take \n",
    "\n",
    "$$G_n - S = - \\int q(x) \\log \\hat p(x)  dx + \\int q(x) \\log q(x) dx$$\n",
    "\n",
    "$G_n - S$ is known as the generalization error. We can rewrite the above equation by first using the associative property and then the properties of logarithms:\n",
    "\n",
    "$$G_n - S = \\int q(x) \\log \\frac{q(x)}{\\hat p(x)} dx$$\n",
    "\n",
    "The term on the right hand side is known as Kullback-Leibler (KL) divergence from q(x) to $\\hat p(x)$, $KL(q(x) || \\hat p(x))$ and is a measure of how close two distributions are. Sometimes it is referred as a distance, but it is not a distance because it is not symmetric, the value of $KL(q(x) || \\hat p(x))$ is not necessarily the same as $KL(\\hat p(x) || q(x))$\n",
    "\n",
    "Two important properties of the Kullback-Leibler divergence are:\n",
    "\n",
    "* $KL \\geq 0$\n",
    "* $KL = 0$ only when the two distributions are equal, in our case when $q(x) = p(x \\mid X^n)$.\n",
    "\n",
    "\n",
    "Thus we can conclude that:\n",
    "\n",
    "* $G_n \\geq S$\n",
    "* $G_n - S = 0$ only when $q(x) = p(x \\mid X^n)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "WAIC, generally pronounced as W-A-I-C, even when something like *wæɪk* is much more easier ;-) is an estimator of the generalization loss which is defined as:\n",
    "\n",
    "\n",
    "$$G_n = - \\int q(x) \\log p(x \\mid X^n) dx$$\n",
    "\n",
    "Where $X^n$ is a sample taking from the *true* distribution $q(x)$ and $p(x \\mid X^n)$ is the predictive density using a likelihood $p(x | \\theta)$ and a prior $p(\\theta)$.\n",
    "\n",
    "> ACLARAR: $G_n$ is connected with posterior predictive distributions (and posterior predictive checks) we approximate $p(x \\mid X^n)$, which in some sense provides a mathematical justification for posterior predictive checks and a connection between Information Criteria and posterior predictive checks. \n",
    "\n",
    "\n",
    "The generalization error is related to the concept of entropy that we can define as:\n",
    "\n",
    "$$S = - \\int q(x) \\log q(x) dx$$\n",
    "\n",
    "The entropy is....\n",
    "\n",
    "If we take \n",
    "\n",
    "$$G_n - S = - \\int q(x) \\log p(x \\mid X^n) dx + \\int q(x) \\log q(x) dx$$\n",
    "\n",
    "Using the associative property and properties of logarithms\n",
    "\n",
    "$$G_n - S = \\int q(x) \\log \\frac{q(x)}{p(x \\mid X^n)} dx$$\n",
    "\n",
    "The term on the right hand side is known as Kullback-Leibler divergence from q(x) to $p(x \\mid X^n)$\n",
    "\n",
    "\n",
    "So we can think of $G_n$ as the theoretical \n",
    "\n",
    "\n",
    "AIC is founded on information theory. When a statistical model is used to represent the process that generated the data, the representation will almost never be exact; so some information will be lost by using the model to represent the process. AIC estimates the relative amount of information lost by a given model: the less information a model loses, the higher the quality of that model.\n",
    "\n",
    "In estimating the amount of information lost by a model, AIC deals with the trade-off between the goodness of fit of the model and the simplicity of the model. In other words, AIC deals with both the risk of overfitting and the risk of underfitting.\n",
    "\n",
    "The Akaike information criterion is named after the statistician Hirotugu Akaike, who formulated it. It now forms the basis of a paradigm for the foundations of statistics; as well, it is widely used for statistical inference. \n",
    "\n",
    "## LOO in depth"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
