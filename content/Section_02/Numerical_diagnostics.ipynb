{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import pymc3 as pm\n",
    "import arviz as az"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Diagnostics\n",
    "\n",
    "We will discuss 3 numerical diagnostics available in ArviZ, those are:\n",
    "\n",
    "* Effective Sampler Size\n",
    "* $\\hat R$ (R hat)\n",
    "* mcse error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help us understand these diagnostics we are going to create two _synthetic posteriors_. The first one is a sample from a uniform distribution. We generate it using SciPy and we call it `good_chains`. This is an example of a \"good\" sample because we are generating independent and identically distributed (iid) samples and ideally this is what we want to approximate the posterior. The second one is called `bad_chains`, and it will represent a poor sample from the posterior. `bad_chains` is a poor _sample_ for two reasons:\n",
    "\n",
    "* Values are not independent. On the contrary they are highly correlated, meaning that given any number at any position in the sequence we can compute the exact sequence of number both before and after the given number. Highly correlation is the opposite of independence.\n",
    "* Values are not identically distributed, as you will see we are creating and array of 2 columns, the first one with numbers from 0 to 0.5 and the second one from 0.5 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_chains = stats.uniform.rvs(0, 1,size=(2,500))\n",
    "bad_chains = np.linspace(0, 1, 1000).reshape(2, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Effective Sample Size (ESS)\n",
    "\n",
    "When using sampling methods like MCMC is common to wonder if a particular sample is large enough to compute what I want? Answering in terms of the number of samples is generally not a good idea, the main reason is that samples from MCMC methods will be autocorrelated and autocorrelation decrease the actual amount of information contained in a sample. Instead, a better idea is to try to estimate the **effective Sample Size**, this is the number of samples we would have if our sample were actually iid. \n",
    "\n",
    "Using ArviZ we can compute it using `az.ess(⋅)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1103.1612083905097, 2.284600376742084)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "az.ess(good_chains), az.ess(bad_chains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is telling us that even when in both cases we have 1000 samples, `bad_chains` is somewhat equivalent to a iid sample of size $\\approx 2$. While `good_chains` is equivalent to something closer to 1000. If you resample `good_chains` you will see that the effective sample size you get will be different for each sample. Nevertheless, on average number will be lower than the $N$ number of samples. Notice, however, that ESS could be in fact larger! When using NUTS as a sampler this can happen for parameters which posterior distribution close to Gaussian and which are almost independent of other parameters.\n",
    "\n",
    "As a general rule of thumb we recommend an ess greater than 400 (!!!)\n",
    "\n",
    "We can also compute the effective sample size using `az.summary(⋅)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hpd_3%</th>\n",
       "      <th>hpd_97%</th>\n",
       "      <th>mcse_mean</th>\n",
       "      <th>mcse_sd</th>\n",
       "      <th>ess_mean</th>\n",
       "      <th>ess_sd</th>\n",
       "      <th>ess_bulk</th>\n",
       "      <th>ess_tail</th>\n",
       "      <th>r_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x</th>\n",
       "      <td>0.513</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.006</td>\n",
       "      <td>1113.0</td>\n",
       "      <td>1105.0</td>\n",
       "      <td>1103.0</td>\n",
       "      <td>958.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean     sd  hpd_3%  hpd_97%  mcse_mean  mcse_sd  ess_mean  ess_sd  \\\n",
       "x  0.513  0.286   0.067    0.993      0.009    0.006    1113.0  1105.0   \n",
       "\n",
       "   ess_bulk  ess_tail  r_hat  \n",
       "x    1103.0     958.0    1.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "az.summary(good_chains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effective Sample Size in depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## $\\hat R$ (aka R hat)\n",
    "\n",
    "\n",
    "Under very general conditions MCMC methods have theoretical guarantees that you will get the right answer irrespective of the starting point. Unfortunately, we only have guarantee for infinite samples. One way to attempt to get a useful estimate of convergence for finite samples we can run more than one chain, starting from very different starting points and check if they _look similar_ to each other. $\\hat R$ is a formalization of this idea. In it's most basic conception we want to compare the _in chain_ variance to the _between chain_ variance. Ideally they should be the same. Thus, if we compute the ratio $\\frac{\\text{in chain variance}}{\\text{between chain variance}}$ we should get 1.\n",
    "\n",
    "Conceptually $\\hat R$ can be interpreted as the overestimation of variance due to MCMC finite sampling. If you continue sampling infinitely you should get a reduction of the variance of your estimation by a $\\hat R$ factor.\n",
    "\n",
    "From a practical point of view $\\hat R \\lessapprox 1.01$ are considered safe\n",
    "\n",
    "Using ArviZ we can compute it using `az.rhat(⋅)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.999163853551514, 3.0393728260009483)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "az.rhat(good_chains), az.rhat(bad_chains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\hat R$ in depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# mcse error \n",
    "\n",
    "When using MCMC methods we introduce an additional layer of uncertainty, due to the finite sampling, we call this mcse error. The mcse error takes into account that the samples are not truly independent of each other. If we want to report the value of an estimated parameter to the second decimal we need to be sure the mcse error is below the second decimal otherwise we will be, wrongly, reporting a higher precision than we really have. We should check the mcse error once we are sure $\\hat R$ is low enough and ESS is high enough, otherwise mcse error is of no use.  \n",
    "\n",
    "Using ArviZ we can compute it using `az.mcse(⋅)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.00856715]), array([0.19772141]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "az.mcse(good_chains), az.mcse(bad_chains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mcse in depth"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
