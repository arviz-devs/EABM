[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploratory Analysis of Bayesian Models",
    "section": "",
    "text": "‎\nWhile conceptually simple, Bayesian methods can be mathematically and numerically challenging. Probabilistic programming languages (PPLs) implement functions to easily build Bayesian models together with efficient automatic inference methods. This helps separate the model building from the inference, allowing practitioners to focus on their specific problems and leaving the PPLs to handle the computational details for them (Bessiere et al. 2013; Daniel Roy 2015; Ghahramani 2015). The inference process generates a posterior distribution - which has a central role in Bayesian statistics - together with other distributions like the posterior predictive distribution and the prior predictive distribution. The correct visualization, analysis, and interpretation of these distributions is key to properly answer the questions that motivated the inference process.\nWhen working with Bayesian models there are a series of related tasks that need to be addressed besides inference itself:\n\nDiagnoses of the quality of the inference (as this is generally done using numerical approximation methods)\nModel criticism, including evaluations of both model assumptions and model predictions\nComparison of models, including model selection or model averaging\nPreparation of the results for a particular audience\n\nWe collectivelly call all these tasks Exploratory analysis of Bayesian model, as we take many ideas from Exploratory data analysis and apply them to analyze Bayesian modesl.\nIn the words of Persi Diaconis (Diaconis 2011):\n\n“Exploratory data analysis seeks to reveal structure, or simple descriptions in data. We look at numbers or graphs and try to find patterns. We pursue leads suggested by background information, imagination, patterns perceived, and experience with other data analyses”.\n\nIn this book we will discuss how to use both numerical and visual summaries to help successfully performing such tasks that are central to the iterative and interactive modeling process. To do so, we first discuss some general principles of data visualization and uncertainty representation that are not exclusibe of Bayesian statistics.\n\n\n\n\nBessiere, Pierre, Emmanuel Mazer, Juan Manuel Ahuactzin, and Kamel Mekhnacha. 2013. Bayesian Programming. 1 edition. Boca Raton: Chapman; Hall/CRC. https://www.crcpress.com/Bayesian-Programming/Bessiere-Mazer-Ahuactzin-Mekhnacha/p/book/9781439880326.\n\n\nDaniel Roy. 2015. Probabilistic Programming. http://probabilistic-programming.org.\n\n\nDiaconis, Persi. 2011. “Theories of Data Analysis: From Magical Thinking Through Classical Statistics.” In Exploring Data Tables, Trends, and Shapes, 1–36. John Wiley & Sons, Ltd. https://doi.org/10.1002/9781118150702.ch1.\n\n\nGhahramani, Zoubin. 2015. “Probabilistic Machine Learning and Artificial Intelligence.” Nature 521 (7553): 452–59. https://doi.org/10.1038/nature14541."
  },
  {
    "objectID": "elements_of_visualization.html#coordinate-systems-and-axes",
    "href": "elements_of_visualization.html#coordinate-systems-and-axes",
    "title": "1  Elements of visualization",
    "section": "1.1 Coordinate systems and axes",
    "text": "1.1 Coordinate systems and axes\nData visualization requires defining position scales to determine where different data values are located in a graphic. In 2D visualizations, two numbers are required to uniquely specify a point. Thus, we need two position scales. The arrangement of these scales is known as a coordinate system. The most common coordinate system is the 2D Cartesian system, using x and y values with orthogonal axes. Conventionally with the x-axis running horizontally and the y-axis vertically. Figure 1.1 shows a Cartesian coordinate system.\n\n\n\nFigure 1.1: Cartesian coordinate system\n\n\nIn practice, we typically shift the axes so that they do not necessarily pass through the origin (0,0), and instead their location is determined by the data. We do this because it is usually more convenient and easier to read to have the axes to the left and bottom of the figure than in the middle. For instance Figure 1.2 plots the exact same points shown in Figure 1.1 but with the axes placed automatically by matplotlib.\n\n\n\nFigure 1.2: Cartesian coordinate system with axes automatically placed by matplotlib based on the data\n\n\nUsually, data has units, such as degrees Celsius for temperature, centimeters for length, or kilograms for weight. In case we are plotting variables of different types (and hence different units) we can adjust the aspect ratio of the axes as we wish. We can make a figure short and wide if it fits better on a page or screen. But we can also change the aspect ratio to highlight important differences, for example, if we want to emphasize changes along the y-axis we can make the figure tall and narrow. When both the x and y axes use the same units, it’s important to maintain an equal ratio to ensure that the relationship between data points on the graph accurately reflects their quantitative values.\nAfter the cartesian coordinate system, the most common coordinate system is the polar coordinate system. In this system, the position of a point is determined by the distance from the origin and the angle with respect to a reference axis. Polar coordinates are useful for representing periodic data, such as days of the week, or data that is naturally represented in a circular shape, such as wind direction. Figure Figure 1.3 shows a polar coordinate system.\n\n\n\nFigure 1.3: Polar coordinate system"
  },
  {
    "objectID": "elements_of_visualization.html#plot-elements",
    "href": "elements_of_visualization.html#plot-elements",
    "title": "1  Elements of visualization",
    "section": "1.2 Plot elements",
    "text": "1.2 Plot elements\nTo convey visual information we generally use shapes, including lines, circles, squares, etc. These elements have properties associated with them like, position, shape, and color. In addition, we can add text to the plot to provide additional information.\nArviZ uses both matplotlib and bokeh as plotting backends. While for basic use of ArviZ is not necessary to know about these libraries, being familiar with them is useful to better understand some of the arguments in ArviZ’s plots and/or to tweak the default plots generated with ArviZ. If you need to learn more about these libraries we recommend the official tutorials for matplotlib and bokeh."
  },
  {
    "objectID": "elements_of_visualization.html#good-practices-and-sources-of-error",
    "href": "elements_of_visualization.html#good-practices-and-sources-of-error",
    "title": "1  Elements of visualization",
    "section": "1.3 Good practices and sources of error",
    "text": "1.3 Good practices and sources of error\nUsing visualization to deceive third parties should not be the goal of an intellectually honest person, and you must also be careful not to deceive yourself. For example, it has been known for decades that a bar chart is more effective for comparing values than a pie chart. The reason is that our perceptual apparatus is quite good at evaluating lengths, but not very good at evaluating areas. Figure 1.4 shows different visual elements ordered according to the precision with which the human brain can detect differences and make comparisons between them (Cleveland and McGill 1984; Heer and Bostock 2010).\n\n\n\nFigure 1.4: Scale of elementary perceptual tasks, taken from The Truthful Art\n\n\n\n1.3.1 General principles for using colors\nHuman eyes work by essentially perceiving 3 wavelengths, this feature is used in technological devices such as screens to generate all colors from combinations of 3 components, Red, Green, and Blue. This is known as the RGB color model. But this is not the only possible system. A very common alternative is the CYMK color model, Cyan, Yellow, Magenta, and Black.\nTo analyze the perceptual attributes of color, it is better to think in terms of Hue, Saturation, and Lightness, HSL is an alternative representation of the RGB color model.\nThe hue is what we colloquially call “different colors”. Green, red, etc. Saturation is how colorful or washed out we perceive a given color. Two colors with different hues will look more different when they have more saturation. The lightness corresponds to the amount of light emitted (active screens) or reflected (impressions), ranging from black to white:\nVarying the tone is useful to easily distinguish categories as shown in Figure 1.5.\n\n\n\nFigure 1.5: Tone varitaions can be help to distinguish categories.\n\n\nIn principle, most humans are capable of distinguishing millions of tones, but if we want to associate categories with colors, the effectiveness of distinguishing them decreases drastically as the number of categories increases. This happens not only because the tones will be increasingly closer to each other, but also because we have a limited working memory. Associating a few colors (say 4) with categories (countries, temperature ranges, etc.) is usually easy. But unless there are pre-existing associations, remembering many categories becomes challenging and this exacerbates when colors are close to each other. This requires us to continually alternate between the graphic and the legend or text where the color-category association is indicated. Adding other elements besides color such as shapes can help, but in general, it will be more useful to try to keep the number of categories relatively low. In addition, it is important to take into account the presentation context, if we want to show a figure during a presentation where we only have a few seconds to dedicate to that figure, it is advisable to keep the figure as simple as possible. This may involve removing items and displaying only a subset of the data. If the figure is part of a text, where the reader will have the time to analyze for a longer period, perhaps the complexity can be somewhat greater.\nAlthough we mentioned before that human eyes are capable of distinguishing three main colors (red, green, and blue), the ability to distinguish these 3 colors varies between people, to the point that many individuals have difficulty distinguishing some colors. The most common case occurs with red and green. This is why it is important to avoid using those colors. An easy way to avoid this problem is to use colorblind-friendly palettes. We’ll see later that this is an easy thing to do when using ArviZ.\nVarying the lightness as in Figure 1.6 is useful when we want to represent a continuous scale. With the hue-based palette (left), it’s quite difficult to determine that our data shows two “spikes”, whereas this is easier to see with the lightness-modifying palette (right). Varying the lightness helps to see the structure of the data since changes in lightness are more intuitively processed as quantitative changes.\n\n\n\nFigure 1.6: Hue-based palette (left) vs lightness-modifying palette (right)\n\n\nOne detail that we should note is that the graph on the right of Figure 1.6 does not change only the lightness, it is not a map in gray or blue scales. That palette also changes the hue but in a very subtle way. This makes it aesthetically more pleasing and the subtle variation in hue contributes to increasing the perceptual distance between two values and therefore the ability to distinguish small differences.\nWhen using colors to represent numerical variables it is important to use uniformly perceptual maps like those offered by matplotlib or colorcet. These are maps where the colors vary in such a way that they adequately reflect changes in the data. Not all colormaps are perceptually uniform. Obtaining them is not trivial. Figure 1.7 shows the same image using different colormaps. We can see that widely used maps such as jet (also called rainbow) generate distortions in the image. In contrast viridis, a perceptually uniform color map does not generate such distortions.\n\n\n\nFigure 1.7: non-uniformly perceptual maps like jet can be very missleading\n\n\nA common criticism of perceptually smooth maps is that they appear more “flat” or “boring” at first glance. And instead maps like Jet, show greater contrast. But that is precisely one of the problems with maps like Jet, the magnitude of these contrasts does not correlate with changes in the data, so even extremes can occur, such as showing contrasts that are not there and hiding differences that are truly there."
  },
  {
    "objectID": "elements_of_visualization.html#style-sheets",
    "href": "elements_of_visualization.html#style-sheets",
    "title": "1  Elements of visualization",
    "section": "1.4 Style sheets",
    "text": "1.4 Style sheets\nMatplotlib allows users to easily switch between plotting styles by defining style sheets. ArviZ is delivered with a few additional styles that can be applied globally by writing az.style.use(name_of_style) or inside a with statement.\n\naz.style.use('arviz-white')\nx = np.linspace(0, 1, 100)\ndist = pz.Beta(2, 5).pdf(x)\n\nfig = plt.figure()\nfor i in range(10):\n    plt.plot(x, dist - i, f'C{i}', label=f'C{i}')\nplt.xlabel('x')\nplt.ylabel('f(x)', rotation=0, labelpad=15);\n\n\n\n\nFigure 1.8: arviz-white style use a color-blind friendly palette\n\n\n\n\naz.style is just an alias of matplotlib.pyplot.style, so everything you can do with one of them you can do with the other.\nThe styles arviz-colors, arviz-white, arviz-darkgrid, and arviz-whitegrid use the same color-blind friendly palette. This palette was designed using colorcyclepicker. arviz-doc, the style used in ArviZ documentation, uses another color-blind friendly and it looks gorgeous on-screen and in printed documents, although it can be tricky for projections if the room is not dark enough.\nIf you need to do plots in grey-scale we recommend restricting yourself to the first 3 colors of the ArviZ default palette (“C0”, “C1” and “C2”), otherwise, you may need to use different line styles or different markers. Another useful palette when the number of colors is restricted is the grayscale which assigns greys to the first four colors (“C0”, “C1”, “C2” and “C3”) and the ArviZ’s blue to “C5”. Check the style gallery to see all the available ArviZ’s styles.\n\n\n\n\nCleveland, William S., and Robert McGill. 1984. “Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods.” Journal of the American Statistical Association 79 (387): 531–54. https://doi.org/10.1080/01621459.1984.10478080.\n\n\nHeer, Jeffrey, and Michael Bostock. 2010. “Crowdsourcing Graphical Perception: Using Mechanical Turk to Assess Visualization Design.” In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 203–12. CHI ’10. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/1753326.1753357."
  },
  {
    "objectID": "distributions.html",
    "href": "distributions.html",
    "title": "2  Distributions",
    "section": "",
    "text": "A discrete distribution represents variables which can only take a countable number of values. Some examples of discrete random variables are the number of coins in your pocket, spots on a giraffe, red cars in a city and people with flu. As we generally use integers to represent discrete variables, when ArviZ is asked to plot integer data it will use histograms to represent them. ArviZ always tries to associate the discrete data to individuals bins. For example in the following plot each bar is associated with an integer in the interval [0, 9].\n\nd_values = pz.Poisson(3).rvs(500)\naz.plot_dist(d_values);\n\n\n\n\nWhen the discrete values takes higher values, like in the following example, bins are still associated with integers but not every integer will be associated with a single different bins. Instead many integers will be binned together.\n\nd_values = pz.Poisson(100).rvs(500)\naz.plot_dist(d_values);\n\n\n\n\nIf you don’t like the default binning criteria of ArviZ, you can change it by passing the bins argument using the hist_kwargs.\n\nd_values = pz.Poisson(100).rvs(500)\nax = az.plot_dist(d_values, hist_kwargs={\"bins\":\"auto\"})\nplt.setp(ax.get_xticklabels(), rotation=45);\n\n\n\n\nA continuous distribution represents variables taking an uncountable number of values. Some examples of continuous random variables are the temperature during summer, the blood pressure of a patience and the time needed to finish a task. By default ArviZ uses kernel density estimation (KDE) to represent continuous distributions.\n\nc_values = pz.Gamma(2, 3).rvs(500)\naz.plot_dist(c_values);\n\n\n\n\nKernel density estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable.\nConceptually you place a kernel function like a gaussian on top of a data-point, then you sum all the gaussians, generally evaluated over a grid and not over the data points. Results are normalized so the total area under the curve is one.\nThe following block of code shows a very simple example of a KDE.\n\n_, ax = plt.subplots(figsize=(12, 4))\nbw = 0.4\nnp.random.seed(19)\ndatapoints = 7\ny = np.random.normal(7, size=datapoints)\nx = np.linspace(y.min() - bw * 3, y.max() + bw * 3, 100)\nkernels = np.transpose([pz.Normal(i, bw).pdf(x) for i in y])\nkernels *= 1/datapoints  # normalize the results\nax.plot(x, kernels, 'k--', alpha=0.5)\nax.plot(y, np.zeros(len(y)), 'C1o')\nax.plot(x, kernels.sum(1))\nax.set_xticks([])\nax.set_yticks([]);\n\n\n\n\nCompared to other KDEs in the Python ecosystem, the KDE implemented in ArviZ automatically handles the boundaries of a distribution. Basically, ArviZ will assign a density of zero to any point outside the range of the data. Another nice feature of ArviZ’s KDE is the method it uses to estimate the bandwith. The bandwidth of a kernel density estimator is a parameter that controls its degree of smoothness. ArviZ’s method works well for a wide range of distributions including multimodal ones."
  },
  {
    "objectID": "MCMC_diagnostics.html",
    "href": "MCMC_diagnostics.html",
    "title": "3  MCMC Diagnostics",
    "section": "",
    "text": "The Achilles heel of computing posterior distributions is, most often than not, the computation of the denominator of Bayes’s theorem. Markov Chain Monte Carlo Methods (MCMC), such as Metropolis, Hamiltonian Monte Carlo (and its variants like NUTS) are clever mathematical and computational devices that let’s us circumvent this problem. The main idea is based on sampling values of the parameters of interest from an easy to sample distribution and then applying a rule, known as metropolis acceptance criterion, to decide if you accept or not that proposal. This rule is necessary as it provides the proper way of correcting the proposed samples to get samples from the true distribution, i.e the posterior distribution. The better performance of NUTS over Metropolis can be explained by the fact that the former uses a clever way to propose values.\nWhile MCMC methods can be successfully used to solve a huge variety of Bayesian models, this have some trade-offs. Most notably, MCMC methods can be slow for some problems, such as Big Data problems and what is most important for our current discussion finite MCMC chains are not guaranteed to converge to the true parameter value. Thus, a key step is to check whether we have a valid sample, otherwise any analysis from it will be totally flawed.\nThere are several tests we can perform, some are visual and some are numerical. These tests are designed to spot problems with our samples, but they are unable to prove we have the correct distribution; they can only provide evidence that the sample seems reasonable.\nIn this chapter we will cover the following topics:\n\nTrace plots\nRank plots\n\\(\\hat R\\)\nAutocorrelation\nEffective Sample Size\nDivergences"
  },
  {
    "objectID": "Presenting_results.html",
    "href": "Presenting_results.html",
    "title": "6  Presentation of results",
    "section": "",
    "text": "Summarizing results\nMonte Carlo Standard error and accuracy"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bessiere, Pierre, Emmanuel Mazer, Juan Manuel Ahuactzin, and Kamel\nMekhnacha. 2013. Bayesian Programming. 1 edition.\nBoca Raton: Chapman; Hall/CRC. https://www.crcpress.com/Bayesian-Programming/Bessiere-Mazer-Ahuactzin-Mekhnacha/p/book/9781439880326.\n\n\nCleveland, William S., and Robert McGill. 1984. “Graphical\nPerception: Theory, Experimentation, and Application to the Development\nof Graphical Methods.” Journal of the American Statistical\nAssociation 79 (387): 531–54. https://doi.org/10.1080/01621459.1984.10478080.\n\n\nDaniel Roy. 2015. Probabilistic Programming. http://probabilistic-programming.org.\n\n\nDiaconis, Persi. 2011. “Theories of Data\nAnalysis: From Magical\nThinking Through Classical\nStatistics.” In Exploring Data\nTables, Trends, and Shapes,\n1–36. John Wiley & Sons, Ltd. https://doi.org/10.1002/9781118150702.ch1.\n\n\nGhahramani, Zoubin. 2015. “Probabilistic Machine\nLearning and Artificial\nIntelligence.” Nature 521 (7553): 452–59.\nhttps://doi.org/10.1038/nature14541.\n\n\nHeer, Jeffrey, and Michael Bostock. 2010. “Crowdsourcing Graphical\nPerception: Using Mechanical Turk to Assess Visualization\nDesign.” In Proceedings of the SIGCHI\nConference on Human Factors in\nComputing Systems, 203–12.\nCHI ’10. New York, NY, USA: Association for Computing\nMachinery. https://doi.org/10.1145/1753326.1753357."
  }
]