[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploratory Analysis of Bayesian Models",
    "section": "",
    "text": "‎\nWhile conceptually simple, Bayesian methods can be mathematically and numerically challenging. Probabilistic programming languages (PPLs) implement functions to easily build Bayesian models together with efficient automatic inference methods. This helps separate the model building from the inference, allowing practitioners to focus on their specific problems and leaving the PPLs to handle the computational details for them (Bessiere et al. 2013; Daniel Roy 2015; Ghahramani 2015). The inference process generates a posterior distribution - which has a central role in Bayesian statistics - together with other distributions like the posterior predictive distribution and the prior predictive distribution. The correct visualization, analysis, and interpretation of these distributions is key to properly answer the questions that motivated the inference process.\nWhen working with Bayesian models there are a series of related tasks that need to be addressed besides inference itself:\n\nDiagnoses of the quality of the inference (as this is generally done using numerical approximation methods)\nModel criticism, including evaluations of both model assumptions and model predictions\nComparison of models, including model selection or model averaging\nPreparation of the results for a particular audience\n\nWe collectively call all these tasks Exploratory analysis of Bayesian models, building on concepts from Exploratory data analysis to examine and gain deeper insights into Bayesian models.\nIn the words of Persi Diaconis (Diaconis 2011):\n\n“Exploratory data analysis seeks to reveal structure, or simple descriptions in data. We look at numbers or graphs and try to find patterns. We pursue leads suggested by background information, imagination, patterns perceived, and experience with other data analyses”.\n\nIn this book we discuss how to use both numerical and visual summaries to successfully perform the many tasks that are central to the iterative and interactive modeling process. To do so, we first discuss some general principles of data visualization and uncertainty representation that are not exclusive of Bayesian statistics.\n\n\n\n\nBessiere, Pierre, Emmanuel Mazer, Juan Manuel Ahuactzin, and Kamel Mekhnacha. 2013. Bayesian Programming. 1 edition. Boca Raton: Chapman; Hall/CRC. https://www.crcpress.com/Bayesian-Programming/Bessiere-Mazer-Ahuactzin-Mekhnacha/p/book/9781439880326.\n\n\nDaniel Roy. 2015. Probabilistic Programming. http://probabilistic-programming.org.\n\n\nDiaconis, Persi. 2011. “Theories of Data Analysis: From Magical Thinking Through Classical Statistics.” In Exploring Data Tables, Trends, and Shapes, 1–36. John Wiley & Sons, Ltd. https://doi.org/10.1002/9781118150702.ch1.\n\n\nGhahramani, Zoubin. 2015. “Probabilistic Machine Learning and Artificial Intelligence.” Nature 521 (7553): 452–59. https://doi.org/10.1038/nature14541.",
    "crumbs": [
      "‎"
    ]
  },
  {
    "objectID": "Chapters/Elements_of_visualization.html",
    "href": "Chapters/Elements_of_visualization.html",
    "title": "1  Elements of visualization",
    "section": "",
    "text": "1.1 Coordinate systems and axes\nData visualization requires defining position scales to determine where different data values are located in a graphic. In 2D visualizations, two numbers are required to uniquely specify a point. Thus, we need two position scales. The arrangement of these scales is known as a coordinate system. The most common coordinate system is the 2D Cartesian system, using x and y values with orthogonal axes. Conventionally with the x-axis running horizontally and the y-axis vertically. Figure 1.1 shows a Cartesian coordinate system.\nIn practice, we typically shift the axes so that they do not necessarily pass through the origin (0,0), and instead their location is determined by the data. We do this because it is usually more convenient and easier to read to have the axes to the left and bottom of the figure than in the middle. For instance Figure 1.2 plots the exact same points shown in Figure 1.1 but with the axes placed automatically by matplotlib.\nUsually, data has units, such as degrees Celsius for temperature, centimetres for length, or kilograms for weight. In case we are plotting variables of different types (and hence different units) we can adjust the aspect ratio of the axes as we wish. We can make a figure short and wide if it fits better on a page or screen. But we can also change the aspect ratio to highlight important differences, for example, if we want to emphasize changes along the y-axis we can make the figure tall and narrow. When both the x and y axes use the same units, it’s important to maintain an equal ratio to ensure that the relationship between data points on the graph accurately reflects their quantitative values.\nAfter the cartesian coordinate system, the most common coordinate system is the polar coordinate system. In this system, the position of a point is determined by the distance from the origin and the angle with respect to a reference axis. Polar coordinates are useful for representing periodic data, such as days of the week, or data that is naturally represented in a circular shape, such as wind direction. Figure Figure 1.3 shows a polar coordinate system.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Elements of visualization</span>"
    ]
  },
  {
    "objectID": "Chapters/Elements_of_visualization.html#coordinate-systems-and-axes",
    "href": "Chapters/Elements_of_visualization.html#coordinate-systems-and-axes",
    "title": "1  Elements of visualization",
    "section": "",
    "text": "Figure 1.1: Cartesian coordinate system\n\n\n\n\n\n\n\n\n\n\nFigure 1.2: Cartesian coordinate system with axes automatically placed by matplotlib based on the data\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.3: Polar coordinate system",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Elements of visualization</span>"
    ]
  },
  {
    "objectID": "Chapters/Elements_of_visualization.html#plot-elements",
    "href": "Chapters/Elements_of_visualization.html#plot-elements",
    "title": "1  Elements of visualization",
    "section": "1.2 Plot elements",
    "text": "1.2 Plot elements\nTo convey visual information we generally use shapes, including lines, circles, squares, etc. These elements have properties associated with them like, position, shape, and color. In addition, we can add text to the plot to provide additional information.\nArviZ uses both matplotlib and bokeh as plotting backends. While for basic use of ArviZ is not necessary to know about these libraries, being familiar with them is useful to better understand some of the arguments in ArviZ’s plots and/or to tweak the default plots generated with ArviZ. If you need to learn more about these libraries we recommend the official tutorials for matplotlib and bokeh.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Elements of visualization</span>"
    ]
  },
  {
    "objectID": "Chapters/Elements_of_visualization.html#good-practices-and-sources-of-error",
    "href": "Chapters/Elements_of_visualization.html#good-practices-and-sources-of-error",
    "title": "1  Elements of visualization",
    "section": "1.3 Good practices and sources of error",
    "text": "1.3 Good practices and sources of error\nUsing visualization to deceive third parties should not be the goal of an intellectually honest person, and you must also be careful not to deceive yourself. For example, it has been known for decades that a bar chart is more effective for comparing values than a pie chart. The reason is that our perceptual apparatus is quite good at evaluating lengths, but not very good at evaluating areas. Figure 1.4 shows different visual elements ordered according to the precision with which the human brain can detect differences and make comparisons between them (Cleveland and McGill 1984; Heer and Bostock 2010).\n\n\n\n\n\n\nFigure 1.4: Scale of elementary perceptual tasks, taken from The Truthful Art\n\n\n\n\n1.3.1 General principles for using colours\nHuman eyes work by essentially perceiving 3 wavelengths, this feature is used in technological devices such as screens to generate all colours from combinations of 3 components, Red, Green, and Blue. This is known as the RGB color model. But this is not the only possible system. A very common alternative is the CYMK color model, Cyan, Yellow, Magenta, and Black.\nTo analyze the perceptual attributes of color, it is better to think in terms of Hue, Saturation, and Lightness, HSL is an alternative representation of the RGB color model.\nThe hue is what we colloquially call “different colours”. Green, red, etc. Saturation is how colourful or washed out we perceive a given color. Two colours with different hues will look more different when they have more saturation. The lightness corresponds to the amount of light emitted (active screens) or reflected (impressions), ranging from black to white:\nVarying the tone is useful to easily distinguish categories as shown in Figure 1.5.\n\n\n\n\n\n\nFigure 1.5: Tone variations can be help to distinguish categories.\n\n\n\nIn principle, most humans are capable of distinguishing millions of tones, but if we want to associate categories with colours, the effectiveness of distinguishing them decreases drastically as the number of categories increases. This happens not only because the tones will be increasingly closer to each other, but also because we have a limited working memory. Associating a few colours (say 4) with categories (countries, temperature ranges, etc.) is usually easy. But unless there are pre-existing associations, remembering many categories becomes challenging and this exacerbates when colours are close to each other. This requires us to continually alternate between the graphic and the legend or text where the color-category association is indicated. Adding other elements besides color such as shapes can help, but in general, it will be more useful to try to keep the number of categories relatively low. In addition, it is important to take into account the presentation context, if we want to show a figure during a presentation where we only have a few seconds to dedicate to that figure, it is advisable to keep the figure as simple as possible. This may involve removing items and displaying only a subset of the data. If the figure is part of a text, where the reader will have the time to analyze for a longer period, perhaps the complexity can be somewhat greater.\nAlthough we mentioned before that human eyes are capable of distinguishing three main colours (red, green, and blue), the ability to distinguish these 3 colours varies between people, to the point that many individuals have difficulty distinguishing some colours. The most common case occurs with red and green. This is why it is important to avoid using those colours. An easy way to avoid this problem is to use color-blind-friendly palettes. We’ll see later that this is an easy thing to do when using ArviZ.\nVarying the lightness as in Figure 1.6 is useful when we want to represent a continuous scale. With the hue-based palette (left), it’s quite difficult to determine that our data shows two “spikes”, whereas this is easier to see with the lightness-modifying palette (right). Varying the lightness helps to see the structure of the data since changes in lightness are more intuitively processed as quantitative changes.\n\n\n\n\n\n\nFigure 1.6: Hue-based palette (left) vs lightness-modifying palette (right)\n\n\n\nOne detail that we should note is that the graph on the right of Figure 1.6 does not change only the lightness, it is not a map in gray or blue scales. That palette also changes the hue but in a very subtle way. This makes it aesthetically more pleasing and the subtle variation in hue contributes to increasing the perceptual distance between two values and therefore the ability to distinguish small differences.\nWhen using colours to represent numerical variables it is important to use uniformly perceptual maps like those offered by matplotlib or colorcet. These are maps where the colours vary in such a way that they adequately reflect changes in the data. Not all colormaps are perceptually uniform. Obtaining them is not trivial. Figure 1.7 shows the same image using different colormaps. We can see that widely used maps such as jet (also called rainbow) generate distortions in the image. In contrast viridis, a perceptually uniform color map does not generate such distortions.\n\n\n\n\n\n\nFigure 1.7: non-uniformly perceptual maps like jet can be very misleading\n\n\n\nA common criticism of perceptually smooth maps is that they appear more “flat” or “boring” at first glance. And instead maps like Jet, show greater contrast. But that is precisely one of the problems with maps like Jet, the magnitude of these contrasts does not correlate with changes in the data, so even extremes can occur, such as showing contrasts that are not there and hiding differences that are truly there.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Elements of visualization</span>"
    ]
  },
  {
    "objectID": "Chapters/Elements_of_visualization.html#style-sheets",
    "href": "Chapters/Elements_of_visualization.html#style-sheets",
    "title": "1  Elements of visualization",
    "section": "1.4 Style sheets",
    "text": "1.4 Style sheets\nMatplotlib allows users to easily switch between plotting styles by defining style sheets. ArviZ is delivered with a few additional styles that can be applied globally by writing az.style.use(name_of_style) or inside a with statement.\n\nazp.style.use('arviz-clean')\nx = np.linspace(0, 1, 100)\ndist = pz.Beta(2, 5).pdf(x)\n\nfig = plt.figure()\nfor i in range(10):\n    plt.plot(x, dist - i, f'C{i}', label=f'C{i}', lw=3)\nplt.xlabel('x')\nplt.ylabel('f(x)', rotation=0, labelpad=15);\n\n\n\n\n\n\n\nFigure 1.8: arviz-clean style use a color-blind friendly palette\n\n\n\n\n\nThe color palettes in ArviZ were designed with the help of colorcyclepicker. Other palettes distributed with ArviZ are 'arviz-cetrino', and 'arviz-vibrant'. To list all available styles use azp.style.available().\nIf you need to do plots in grey-scale we recommend restricting yourself to the first 3 colours of the ArviZ palettes (“C0”, “C1” and “C2”), otherwise, you may need to use different line styles or different markers.\n\n\n\n\nCleveland, William S., and Robert McGill. 1984. “Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods.” Journal of the American Statistical Association 79 (387): 531–54. https://doi.org/10.1080/01621459.1984.10478080.\n\n\nHeer, Jeffrey, and Michael Bostock. 2010. “Crowdsourcing Graphical Perception: Using Mechanical Turk to Assess Visualization Design.” In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 203–12. CHI ’10. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/1753326.1753357.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Elements of visualization</span>"
    ]
  },
  {
    "objectID": "Chapters/DataTree.html",
    "href": "Chapters/DataTree.html",
    "title": "2  Working with DataTree",
    "section": "",
    "text": "2.1 InferenceData\nDuring a modern Bayesian analysis we usually generate many sets of data including posterior samples and posterior predictive samples. But we also have observed data, and statistics generated by the sampling method, samples from the prior and/or prior predictive distribution, etc. To keep all this data tidy and avoid confusion ArviZ relies on the data-structures provided by (Hoyer and Hamman 2017). If you are not familiar with xarray this chapter introduced some basic elements in the context of Bayesian stats. For a deeper understanding of xarray data-structures and functionally we recommend that you check their documentation, you may find xarray useful for problems outside Bayesian analysis.\nWe need to become familiar with 3 Data Structures:\nThe best way to understand this data-structure is to explore them. ArviZ comes equipped with a few DataTree objects so we can start playing with them even without the need to fit a model. Let’s start by loading the centered_eight DataTree.\ndt = azb.load_arviz_data(\"centered_eight\")\nIn the context of Bayesian Stats a DataTree has groups like posterior, observed_data, posterior_predictive, log_likelihood, etc.\ndt\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DatasetView&gt; Size: 0B\nDimensions:  ()\nData variables:\n    *empty*xarray.DataTreeGroups: (8)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DatasetView&gt; Size: 165kB\nDimensions:  (chain: 4, draw: 500, school: 8)\nCoordinates:\n  * chain    (chain) int64 32B 0 1 2 3\n  * draw     (draw) int64 4kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\n  * school   (school) &lt;U16 512B 'Choate' 'Deerfield' ... 'Mt. Hermon'\nData variables:\n    mu       (chain, draw) float64 16kB ...\n    theta    (chain, draw, school) float64 128kB ...\n    tau      (chain, draw) float64 16kB ...\nAttributes: (6)posteriorGroups: (0)Dimensions:chain: 4draw: 500school: 8Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (3)mu(chain, draw)float64...[2000 values with dtype=float64]theta(chain, draw, school)float64...[16000 values with dtype=float64]tau(chain, draw)float64...[2000 values with dtype=float64]Attributes: (6)created_at :2022-10-13T14:37:37.315398arviz_version :0.13.0.dev0inference_library :pymcinference_library_version :4.2.2sampling_time :7.480114936828613tuning_steps :1000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DatasetView&gt; Size: 133kB\nDimensions:  (chain: 4, draw: 500, school: 8)\nCoordinates:\n  * chain    (chain) int64 32B 0 1 2 3\n  * draw     (draw) int64 4kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\n  * school   (school) &lt;U16 512B 'Choate' 'Deerfield' ... 'Mt. Hermon'\nData variables:\n    obs      (chain, draw, school) float64 128kB ...\nAttributes: (4)posterior_predictiveGroups: (0)Dimensions:chain: 4draw: 500school: 8Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (1)obs(chain, draw, school)float64...[16000 values with dtype=float64]Attributes: (4)arviz_version :0.13.0.dev0created_at :2022-10-13T14:37:41.460544inference_library :pymcinference_library_version :4.2.2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DatasetView&gt; Size: 133kB\nDimensions:  (chain: 4, draw: 500, school: 8)\nCoordinates:\n  * chain    (chain) int64 32B 0 1 2 3\n  * draw     (draw) int64 4kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\n  * school   (school) &lt;U16 512B 'Choate' 'Deerfield' ... 'Mt. Hermon'\nData variables:\n    obs      (chain, draw, school) float64 128kB ...\nAttributes: (4)log_likelihoodGroups: (0)Dimensions:chain: 4draw: 500school: 8Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (1)obs(chain, draw, school)float64...[16000 values with dtype=float64]Attributes: (4)arviz_version :0.13.0.dev0created_at :2022-10-13T14:37:37.487399inference_library :pymcinference_library_version :4.2.2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DatasetView&gt; Size: 246kB\nDimensions:              (chain: 4, draw: 500)\nCoordinates:\n  * chain                (chain) int64 32B 0 1 2 3\n  * draw                 (draw) int64 4kB 0 1 2 3 4 5 ... 495 496 497 498 499\nData variables: (12/16)\n    max_energy_error     (chain, draw) float64 16kB ...\n    energy_error         (chain, draw) float64 16kB ...\n    lp                   (chain, draw) float64 16kB ...\n    index_in_trajectory  (chain, draw) int64 16kB ...\n    acceptance_rate      (chain, draw) float64 16kB ...\n    diverging            (chain, draw) bool 2kB ...\n    ...                   ...\n    smallest_eigval      (chain, draw) float64 16kB ...\n    step_size_bar        (chain, draw) float64 16kB ...\n    step_size            (chain, draw) float64 16kB ...\n    energy               (chain, draw) float64 16kB ...\n    tree_depth           (chain, draw) int64 16kB ...\n    perf_counter_diff    (chain, draw) float64 16kB ...\nAttributes: (6)sample_statsGroups: (0)Dimensions:chain: 4draw: 500Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])Data variables: (16)max_energy_error(chain, draw)float64...[2000 values with dtype=float64]energy_error(chain, draw)float64...[2000 values with dtype=float64]lp(chain, draw)float64...[2000 values with dtype=float64]index_in_trajectory(chain, draw)int64...[2000 values with dtype=int64]acceptance_rate(chain, draw)float64...[2000 values with dtype=float64]diverging(chain, draw)bool...[2000 values with dtype=bool]process_time_diff(chain, draw)float64...[2000 values with dtype=float64]n_steps(chain, draw)float64...[2000 values with dtype=float64]perf_counter_start(chain, draw)float64...[2000 values with dtype=float64]largest_eigval(chain, draw)float64...[2000 values with dtype=float64]smallest_eigval(chain, draw)float64...[2000 values with dtype=float64]step_size_bar(chain, draw)float64...[2000 values with dtype=float64]step_size(chain, draw)float64...[2000 values with dtype=float64]energy(chain, draw)float64...[2000 values with dtype=float64]tree_depth(chain, draw)int64...[2000 values with dtype=int64]perf_counter_diff(chain, draw)float64...[2000 values with dtype=float64]Attributes: (6)arviz_version :0.13.0.dev0created_at :2022-10-13T14:37:37.324929inference_library :pymcinference_library_version :4.2.2sampling_time :7.480114936828613tuning_steps :1000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DatasetView&gt; Size: 45kB\nDimensions:  (chain: 1, draw: 500, school: 8)\nCoordinates:\n  * chain    (chain) int64 8B 0\n  * draw     (draw) int64 4kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\n  * school   (school) &lt;U16 512B 'Choate' 'Deerfield' ... 'Mt. Hermon'\nData variables:\n    tau      (chain, draw) float64 4kB ...\n    theta    (chain, draw, school) float64 32kB ...\n    mu       (chain, draw) float64 4kB ...\nAttributes: (4)priorGroups: (0)Dimensions:chain: 1draw: 500school: 8Coordinates: (3)chain(chain)int640array([0])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (3)tau(chain, draw)float64...[500 values with dtype=float64]theta(chain, draw, school)float64...[4000 values with dtype=float64]mu(chain, draw)float64...[500 values with dtype=float64]Attributes: (4)arviz_version :0.13.0.dev0created_at :2022-10-13T14:37:26.602116inference_library :pymcinference_library_version :4.2.2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DatasetView&gt; Size: 37kB\nDimensions:  (chain: 1, draw: 500, school: 8)\nCoordinates:\n  * chain    (chain) int64 8B 0\n  * draw     (draw) int64 4kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\n  * school   (school) &lt;U16 512B 'Choate' 'Deerfield' ... 'Mt. Hermon'\nData variables:\n    obs      (chain, draw, school) float64 32kB ...\nAttributes: (4)prior_predictiveGroups: (0)Dimensions:chain: 1draw: 500school: 8Coordinates: (3)chain(chain)int640array([0])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (1)obs(chain, draw, school)float64...[4000 values with dtype=float64]Attributes: (4)arviz_version :0.13.0.dev0created_at :2022-10-13T14:37:26.604969inference_library :pymcinference_library_version :4.2.2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DatasetView&gt; Size: 576B\nDimensions:  (school: 8)\nCoordinates:\n  * school   (school) &lt;U16 512B 'Choate' 'Deerfield' ... 'Mt. Hermon'\nData variables:\n    obs      (school) float64 64B ...\nAttributes: (4)observed_dataGroups: (0)Dimensions:school: 8Coordinates: (1)school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (1)obs(school)float64...[8 values with dtype=float64]Attributes: (4)arviz_version :0.13.0.dev0created_at :2022-10-13T14:37:26.606375inference_library :pymcinference_library_version :4.2.2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DatasetView&gt; Size: 576B\nDimensions:  (school: 8)\nCoordinates:\n  * school   (school) &lt;U16 512B 'Choate' 'Deerfield' ... 'Mt. Hermon'\nData variables:\n    scores   (school) float64 64B ...\nAttributes: (4)constant_dataGroups: (0)Dimensions:school: 8Coordinates: (1)school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (1)scores(school)float64...[8 values with dtype=float64]Attributes: (4)arviz_version :0.13.0.dev0created_at :2022-10-13T14:37:26.607471inference_library :pymcinference_library_version :4.2.2Dimensions:Coordinates: (0)Inherited coordinates: (0)Data variables: (0)Attributes: (0)\nThis is an HTML representation of a DataTree, so if you are reading this from a browser you should be able to interact with it. If you click on the posterior group you will see that we have three dimensions, with the names chain, draw, and school, you can think of dimensions as the axes of a plot. This means that the posterior samples were generated by running an MCMC sampler with 4 chains, each one of 500 draws. At least for one of the parameters in the posterior we have and additional dimension called school. If you click on coordinates you will be able to see the actual values that each dimension can take, like the integers [0, 1, 2, 3] for chain and the strings ['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter', 'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'] for school, notice that we have an array of dtype=object. Furthermore, if you click on the  symbol by the school coordinate, you will be able to see the names of each school.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with DataTree</span>"
    ]
  },
  {
    "objectID": "Chapters/DataTree.html#inferencedata",
    "href": "Chapters/DataTree.html#inferencedata",
    "title": "2  Working with DataTree",
    "section": "",
    "text": "DataArray: A labeled, N-dimensional array. In other words this is like NumPy but you can access the data using meaningful labels instead of numerical indexes. You may also think of this as the N-D generalization of a pandas or polars Series.\nDataSet: It is a dict-like container of DataArray objects aligned along any number of shared dimensions. You may also think of this as the N-D generalization of a pandas or polars DataFrame.\nDataTree: This is a container of DataSets, each DataSet is associated with a group.\n\n\n\n\n\n\n\n2.1.1 Get the dataset corresponding to a single group\nWe can access each group using a dictionary-like notation:\n\ndt[\"posterior\"]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DatasetView&gt; Size: 165kB\nDimensions:  (chain: 4, draw: 500, school: 8)\nCoordinates:\n  * chain    (chain) int64 32B 0 1 2 3\n  * draw     (draw) int64 4kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\n  * school   (school) &lt;U16 512B 'Choate' 'Deerfield' ... 'Mt. Hermon'\nData variables:\n    mu       (chain, draw) float64 16kB ...\n    theta    (chain, draw, school) float64 128kB ...\n    tau      (chain, draw) float64 16kB ...\nAttributes: (6)xarray.DataTreeGroups: (0)Dimensions:chain: 4draw: 500school: 8Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Inherited coordinates: (0)Data variables: (3)mu(chain, draw)float64...[2000 values with dtype=float64]theta(chain, draw, school)float64...[16000 values with dtype=float64]tau(chain, draw)float64...[2000 values with dtype=float64]Attributes: (6)created_at :2022-10-13T14:37:37.315398arviz_version :0.13.0.dev0inference_library :pymcinference_library_version :4.2.2sampling_time :7.480114936828613tuning_steps :1000\n\n\nAlternatively, we can use the dot notation, as groups are attributes of the InferenceData object. For instance, to access the posterior group we can write:\n\ndt.posterior;\n\nThe dot notation works at the group level and for DataSets and DataArrays as long as there is no conflict with a method or attribute of these objects. If there is a conflict, you can always use the dictionary-like notation.\nNotice that we still get a DataTree, but with 0 groups. If you want the DataSet you can do.\n\ndt[\"posterior\"].to_dataset()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 165kB\nDimensions:  (chain: 4, draw: 500, school: 8)\nCoordinates:\n  * chain    (chain) int64 32B 0 1 2 3\n  * draw     (draw) int64 4kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\n  * school   (school) &lt;U16 512B 'Choate' 'Deerfield' ... 'Mt. Hermon'\nData variables:\n    mu       (chain, draw) float64 16kB ...\n    theta    (chain, draw, school) float64 128kB ...\n    tau      (chain, draw) float64 16kB ...\nAttributes: (6)xarray.DatasetDimensions:chain: 4draw: 500school: 8Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (3)mu(chain, draw)float64...[2000 values with dtype=float64]theta(chain, draw, school)float64...[16000 values with dtype=float64]tau(chain, draw)float64...[2000 values with dtype=float64]Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))schoolPandasIndexPandasIndex(Index(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', 'St. Paul's', 'Mt. Hermon'],\n      dtype='object', name='school'))Attributes: (6)created_at :2022-10-13T14:37:37.315398arviz_version :0.13.0.dev0inference_library :pymcinference_library_version :4.2.2sampling_time :7.480114936828613tuning_steps :1000\n\n\n\n\n2.1.2 Get coordinate values\nAs we have seen, we have 8 schools with their names. If we want to programmatically access the names we can do\n\ndt.posterior.school\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'school' (school: 8)&gt; Size: 512B\n'Choate' 'Deerfield' 'Phillips Andover' ... \"St. Paul's\" 'Mt. Hermon'\nCoordinates:\n  * school   (school) &lt;U16 512B 'Choate' 'Deerfield' ... 'Mt. Hermon'xarray.DataArray'school'school: 8'Choate' 'Deerfield' 'Phillips Andover' ... \"St. Paul's\" 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Coordinates: (1)school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Indexes: (1)schoolPandasIndexPandasIndex(Index(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', 'St. Paul's', 'Mt. Hermon'],\n      dtype='object', name='school'))Attributes: (0)\n\n\nWhich returns a DataArray with the names of the schools. To obtain a NumPy array we can do\n\ndt.posterior.school.values\n\narray(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'],\n      dtype='&lt;U16')\n\n\nIf we want to get the number of schools we can write:\n\nlen(dt.observed_data.school)\n\n8\n\n\nNotice that we do not need to first obtain the NumPy array and then compute the length. When working with DataTree/Sets/Arrays, you may feel tempted to reduce them to NumPy arrays, as you are more familiar with those. But for many problems that is not needed and for many other that is not even recommended as you may loose the benefit of working with labeled array-like structures.\n\n\n2.1.3 Get a subset of chains\nBecause we have labels for the names of the schools we can use them to access their associated information. Labels are usually much easier to remember than numerical indices. For instance, to access the posterior samples of the school Choate we can write:\n\ndt.posterior.sel(school=\"Choate\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DatasetView&gt; Size: 52kB\nDimensions:  (chain: 4, draw: 500)\nCoordinates:\n  * chain    (chain) int64 32B 0 1 2 3\n  * draw     (draw) int64 4kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\n    school   &lt;U16 64B 'Choate'\nData variables:\n    mu       (chain, draw) float64 16kB ...\n    theta    (chain, draw) float64 16kB ...\n    tau      (chain, draw) float64 16kB ...\nAttributes: (6)xarray.DataTreeGroups: (0)Dimensions:chain: 4draw: 500Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])school()&lt;U16'Choate'array('Choate', dtype='&lt;U16')Inherited coordinates: (0)Data variables: (3)mu(chain, draw)float64...[2000 values with dtype=float64]theta(chain, draw)float64...[2000 values with dtype=float64]tau(chain, draw)float64...[2000 values with dtype=float64]Attributes: (6)created_at :2022-10-13T14:37:37.315398arviz_version :0.13.0.dev0inference_library :pymcinference_library_version :4.2.2sampling_time :7.480114936828613tuning_steps :1000\n\n\nThe draw and chain coordinates are indexed using numbers, the following code will return the last draw from chain 1 and chain 2:\n\ndt.posterior.sel(draw=499, chain=[1, 2])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DatasetView&gt; Size: 696B\nDimensions:  (chain: 2, school: 8)\nCoordinates:\n  * chain    (chain) int64 16B 1 2\n    draw     int64 8B 499\n  * school   (school) &lt;U16 512B 'Choate' 'Deerfield' ... 'Mt. Hermon'\nData variables:\n    mu       (chain) float64 16B ...\n    theta    (chain, school) float64 128B ...\n    tau      (chain) float64 16B ...\nAttributes: (6)xarray.DataTreeGroups: (0)Dimensions:chain: 2school: 8Coordinates: (3)chain(chain)int641 2array([1, 2])draw()int64499array(499)school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Inherited coordinates: (0)Data variables: (3)mu(chain)float64...[2 values with dtype=float64]theta(chain, school)float64...[16 values with dtype=float64]tau(chain)float64...[2 values with dtype=float64]Attributes: (6)created_at :2022-10-13T14:37:37.315398arviz_version :0.13.0.dev0inference_library :pymcinference_library_version :4.2.2sampling_time :7.480114936828613tuning_steps :1000\n\n\nUsually, in Bayesian statistics, we don’t need to access individual draws or chains, a more common operation is to select a range. For that purpose, we can use Python’s slice function. For example, the following line of code returns the first 200 draws from all chains:\n\ndt.posterior.sel(draw=slice(0, 200))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DatasetView&gt; Size: 66kB\nDimensions:  (chain: 4, draw: 201, school: 8)\nCoordinates:\n  * chain    (chain) int64 32B 0 1 2 3\n  * draw     (draw) int64 2kB 0 1 2 3 4 5 6 7 ... 194 195 196 197 198 199 200\n  * school   (school) &lt;U16 512B 'Choate' 'Deerfield' ... 'Mt. Hermon'\nData variables:\n    mu       (chain, draw) float64 6kB ...\n    theta    (chain, draw, school) float64 51kB ...\n    tau      (chain, draw) float64 6kB ...\nAttributes: (6)xarray.DataTreeGroups: (0)Dimensions:chain: 4draw: 201school: 8Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 196 197 198 199 200array([  0,   1,   2, ..., 198, 199, 200])school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Inherited coordinates: (0)Data variables: (3)mu(chain, draw)float64...[804 values with dtype=float64]theta(chain, draw, school)float64...[6432 values with dtype=float64]tau(chain, draw)float64...[804 values with dtype=float64]Attributes: (6)created_at :2022-10-13T14:37:37.315398arviz_version :0.13.0.dev0inference_library :pymcinference_library_version :4.2.2sampling_time :7.480114936828613tuning_steps :1000\n\n\nUsing the slice function we can also remove the first 100 samples.\n\ndt.posterior.sel(draw=slice(100, None))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DatasetView&gt; Size: 132kB\nDimensions:  (chain: 4, draw: 400, school: 8)\nCoordinates:\n  * chain    (chain) int64 32B 0 1 2 3\n  * draw     (draw) int64 3kB 100 101 102 103 104 105 ... 495 496 497 498 499\n  * school   (school) &lt;U16 512B 'Choate' 'Deerfield' ... 'Mt. Hermon'\nData variables:\n    mu       (chain, draw) float64 13kB ...\n    theta    (chain, draw, school) float64 102kB ...\n    tau      (chain, draw) float64 13kB ...\nAttributes: (6)xarray.DataTreeGroups: (0)Dimensions:chain: 4draw: 400school: 8Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int64100 101 102 103 ... 496 497 498 499array([100, 101, 102, ..., 497, 498, 499])school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Inherited coordinates: (0)Data variables: (3)mu(chain, draw)float64...[1600 values with dtype=float64]theta(chain, draw, school)float64...[12800 values with dtype=float64]tau(chain, draw)float64...[1600 values with dtype=float64]Attributes: (6)created_at :2022-10-13T14:37:37.315398arviz_version :0.13.0.dev0inference_library :pymcinference_library_version :4.2.2sampling_time :7.480114936828613tuning_steps :1000\n\n\nWe can not apply the same operations to the entire DataTree object, \"draw\" is not a valid dimension for all groups. Nut we can filter those group that have \"draw\".\n\ndt.filter(lambda node: \"draw\" in node.dims).sel(draw=slice(100, None))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DatasetView&gt; Size: 0B\nDimensions:  ()\nData variables:\n    *empty*xarray.DataTreeGroups: (6)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DatasetView&gt; Size: 132kB\nDimensions:  (chain: 4, draw: 400, school: 8)\nCoordinates:\n  * chain    (chain) int64 32B 0 1 2 3\n  * draw     (draw) int64 3kB 100 101 102 103 104 105 ... 495 496 497 498 499\n  * school   (school) &lt;U16 512B 'Choate' 'Deerfield' ... 'Mt. Hermon'\nData variables:\n    mu       (chain, draw) float64 13kB ...\n    theta    (chain, draw, school) float64 102kB ...\n    tau      (chain, draw) float64 13kB ...\nAttributes: (6)posteriorGroups: (0)Dimensions:chain: 4draw: 400school: 8Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int64100 101 102 103 ... 496 497 498 499array([100, 101, 102, ..., 497, 498, 499])school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (3)mu(chain, draw)float64...[1600 values with dtype=float64]theta(chain, draw, school)float64...[12800 values with dtype=float64]tau(chain, draw)float64...[1600 values with dtype=float64]Attributes: (6)created_at :2022-10-13T14:37:37.315398arviz_version :0.13.0.dev0inference_library :pymcinference_library_version :4.2.2sampling_time :7.480114936828613tuning_steps :1000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DatasetView&gt; Size: 106kB\nDimensions:  (chain: 4, draw: 400, school: 8)\nCoordinates:\n  * chain    (chain) int64 32B 0 1 2 3\n  * draw     (draw) int64 3kB 100 101 102 103 104 105 ... 495 496 497 498 499\n  * school   (school) &lt;U16 512B 'Choate' 'Deerfield' ... 'Mt. Hermon'\nData variables:\n    obs      (chain, draw, school) float64 102kB ...\nAttributes: (4)posterior_predictiveGroups: (0)Dimensions:chain: 4draw: 400school: 8Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int64100 101 102 103 ... 496 497 498 499array([100, 101, 102, ..., 497, 498, 499])school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (1)obs(chain, draw, school)float64...[12800 values with dtype=float64]Attributes: (4)arviz_version :0.13.0.dev0created_at :2022-10-13T14:37:41.460544inference_library :pymcinference_library_version :4.2.2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DatasetView&gt; Size: 106kB\nDimensions:  (chain: 4, draw: 400, school: 8)\nCoordinates:\n  * chain    (chain) int64 32B 0 1 2 3\n  * draw     (draw) int64 3kB 100 101 102 103 104 105 ... 495 496 497 498 499\n  * school   (school) &lt;U16 512B 'Choate' 'Deerfield' ... 'Mt. Hermon'\nData variables:\n    obs      (chain, draw, school) float64 102kB ...\nAttributes: (4)log_likelihoodGroups: (0)Dimensions:chain: 4draw: 400school: 8Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int64100 101 102 103 ... 496 497 498 499array([100, 101, 102, ..., 497, 498, 499])school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (1)obs(chain, draw, school)float64...[12800 values with dtype=float64]Attributes: (4)arviz_version :0.13.0.dev0created_at :2022-10-13T14:37:37.487399inference_library :pymcinference_library_version :4.2.2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DatasetView&gt; Size: 197kB\nDimensions:              (chain: 4, draw: 400)\nCoordinates:\n  * chain                (chain) int64 32B 0 1 2 3\n  * draw                 (draw) int64 3kB 100 101 102 103 ... 496 497 498 499\nData variables: (12/16)\n    max_energy_error     (chain, draw) float64 13kB ...\n    energy_error         (chain, draw) float64 13kB ...\n    lp                   (chain, draw) float64 13kB ...\n    index_in_trajectory  (chain, draw) int64 13kB ...\n    acceptance_rate      (chain, draw) float64 13kB ...\n    diverging            (chain, draw) bool 2kB ...\n    ...                   ...\n    smallest_eigval      (chain, draw) float64 13kB ...\n    step_size_bar        (chain, draw) float64 13kB ...\n    step_size            (chain, draw) float64 13kB ...\n    energy               (chain, draw) float64 13kB ...\n    tree_depth           (chain, draw) int64 13kB ...\n    perf_counter_diff    (chain, draw) float64 13kB ...\nAttributes: (6)sample_statsGroups: (0)Dimensions:chain: 4draw: 400Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int64100 101 102 103 ... 496 497 498 499array([100, 101, 102, ..., 497, 498, 499])Data variables: (16)max_energy_error(chain, draw)float64...[1600 values with dtype=float64]energy_error(chain, draw)float64...[1600 values with dtype=float64]lp(chain, draw)float64...[1600 values with dtype=float64]index_in_trajectory(chain, draw)int64...[1600 values with dtype=int64]acceptance_rate(chain, draw)float64...[1600 values with dtype=float64]diverging(chain, draw)bool...[1600 values with dtype=bool]process_time_diff(chain, draw)float64...[1600 values with dtype=float64]n_steps(chain, draw)float64...[1600 values with dtype=float64]perf_counter_start(chain, draw)float64...[1600 values with dtype=float64]largest_eigval(chain, draw)float64...[1600 values with dtype=float64]smallest_eigval(chain, draw)float64...[1600 values with dtype=float64]step_size_bar(chain, draw)float64...[1600 values with dtype=float64]step_size(chain, draw)float64...[1600 values with dtype=float64]energy(chain, draw)float64...[1600 values with dtype=float64]tree_depth(chain, draw)int64...[1600 values with dtype=int64]perf_counter_diff(chain, draw)float64...[1600 values with dtype=float64]Attributes: (6)arviz_version :0.13.0.dev0created_at :2022-10-13T14:37:37.324929inference_library :pymcinference_library_version :4.2.2sampling_time :7.480114936828613tuning_steps :1000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DatasetView&gt; Size: 36kB\nDimensions:  (chain: 1, draw: 400, school: 8)\nCoordinates:\n  * chain    (chain) int64 8B 0\n  * draw     (draw) int64 3kB 100 101 102 103 104 105 ... 495 496 497 498 499\n  * school   (school) &lt;U16 512B 'Choate' 'Deerfield' ... 'Mt. Hermon'\nData variables:\n    tau      (chain, draw) float64 3kB ...\n    theta    (chain, draw, school) float64 26kB ...\n    mu       (chain, draw) float64 3kB ...\nAttributes: (4)priorGroups: (0)Dimensions:chain: 1draw: 400school: 8Coordinates: (3)chain(chain)int640array([0])draw(draw)int64100 101 102 103 ... 496 497 498 499array([100, 101, 102, ..., 497, 498, 499])school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (3)tau(chain, draw)float64...[400 values with dtype=float64]theta(chain, draw, school)float64...[3200 values with dtype=float64]mu(chain, draw)float64...[400 values with dtype=float64]Attributes: (4)arviz_version :0.13.0.dev0created_at :2022-10-13T14:37:26.602116inference_library :pymcinference_library_version :4.2.2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DatasetView&gt; Size: 29kB\nDimensions:  (chain: 1, draw: 400, school: 8)\nCoordinates:\n  * chain    (chain) int64 8B 0\n  * draw     (draw) int64 3kB 100 101 102 103 104 105 ... 495 496 497 498 499\n  * school   (school) &lt;U16 512B 'Choate' 'Deerfield' ... 'Mt. Hermon'\nData variables:\n    obs      (chain, draw, school) float64 26kB ...\nAttributes: (4)prior_predictiveGroups: (0)Dimensions:chain: 1draw: 400school: 8Coordinates: (3)chain(chain)int640array([0])draw(draw)int64100 101 102 103 ... 496 497 498 499array([100, 101, 102, ..., 497, 498, 499])school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (1)obs(chain, draw, school)float64...[3200 values with dtype=float64]Attributes: (4)arviz_version :0.13.0.dev0created_at :2022-10-13T14:37:26.604969inference_library :pymcinference_library_version :4.2.2Dimensions:Coordinates: (0)Inherited coordinates: (0)Data variables: (0)Attributes: (0)\n\n\nIf you check the object you will see that the groups posterior, posterior_predictive, log_likelihood, sample_stats, prior, and prior_predictive have 400 draws compared to the original 500. The group observed_data has not been affected because it does not have the draw dimension.\n\n\n2.1.4 Compute posterior mean\nWe can perform operations on the InferenceData object. For instance, to compute the mean of the first 200 draws we can write:\n\ndt.posterior.sel(draw=slice(0, 200)).mean()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DatasetView&gt; Size: 24B\nDimensions:  ()\nData variables:\n    mu       float64 8B 4.542\n    theta    float64 8B 5.005\n    tau      float64 8B 4.196xarray.DataTreeGroups: (0)Dimensions:Coordinates: (0)Inherited coordinates: (0)Data variables: (3)mu()float644.542array(4.54161222)theta()float645.005array(5.00504117)tau()float644.196array(4.19590805)Attributes: (0)\n\n\nIn NumPy, it is common to perform operations like this along a given axis. We can do the same by specifying the dimension along which we want to operate. For instance, to compute the mean along the draw dimension we can write:\n\ndt.posterior.mean(\"draw\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DatasetView&gt; Size: 864B\nDimensions:  (chain: 4, school: 8)\nCoordinates:\n  * chain    (chain) int64 32B 0 1 2 3\n  * school   (school) &lt;U16 512B 'Choate' 'Deerfield' ... 'Mt. Hermon'\nData variables:\n    mu       (chain) float64 32B 4.246 4.184 4.659 4.855\n    theta    (chain, school) float64 256B 5.793 4.648 3.742 ... 4.14 6.74 5.275\n    tau      (chain) float64 32B 3.682 4.247 4.656 3.912xarray.DataTreeGroups: (0)Dimensions:chain: 4school: 8Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Inherited coordinates: (0)Data variables: (3)mu(chain)float644.246 4.184 4.659 4.855array([4.24630224, 4.18354806, 4.65892852, 4.8549536 ])theta(chain, school)float645.793 4.648 3.742 ... 6.74 5.275array([[5.79273861, 4.64846555, 3.74178447, 4.59127846, 3.55348781,\n        4.02845245, 5.96228791, 4.73727946],\n       [6.33445224, 4.67680497, 3.65081796, 4.43324764, 3.3964076 ,\n        3.60127667, 6.48283092, 4.55211921],\n       [6.83321937, 5.51281457, 3.92680194, 5.28074754, 3.98373065,\n        4.1289177 , 7.13866968, 4.52550204],\n       [6.87984671, 5.27213322, 4.43271831, 5.18117578, 3.73373858,\n        4.14010165, 6.7399058 , 5.27474343]])tau(chain)float643.682 4.247 4.656 3.912array([3.6818728 , 4.24683679, 4.65603863, 3.91214293])Attributes: (0)\n\n\nThis returns the mean for each chain and school. Can you anticipate how different this would be if the dimension was chain instead of draw? And what about if we use school?\nWe can also specify multiple dimensions. For instance, to compute the mean along the draw and chain dimensions we can write:\n\ndt.posterior.mean([\"chain\", \"draw\"])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DatasetView&gt; Size: 592B\nDimensions:  (school: 8)\nCoordinates:\n  * school   (school) &lt;U16 512B 'Choate' 'Deerfield' ... 'Mt. Hermon'\nData variables:\n    mu       float64 8B 4.486\n    theta    (school) float64 64B 6.46 5.028 3.938 4.872 3.667 3.975 6.581 4.772\n    tau      float64 8B 4.124xarray.DataTreeGroups: (0)Dimensions:school: 8Coordinates: (1)school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Inherited coordinates: (0)Data variables: (3)mu()float644.486array(4.4859331)theta(school)float646.46 5.028 3.938 ... 6.581 4.772array([6.46006423, 5.02755458, 3.93803067, 4.87161236, 3.66684116,\n       3.97468712, 6.58092358, 4.77241104])tau()float644.124array(4.12422279)Attributes: (0)\n\n\n\n\n2.1.5 Combine chains and draws\nOur primary goal is usually to obtain posterior samples and thus we aren’t concerned with chains and draws. In those cases, we can use the az.extract function. This combines the chain and draw into a sample coordinate which can make further operations easier. By default, az.extract works on the posterior, but you can specify other groups using the group argument.\n\nazb.extract(dt)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 209kB\nDimensions:  (sample: 2000, school: 8)\nCoordinates:\n  * school   (school) &lt;U16 512B 'Choate' 'Deerfield' ... 'Mt. Hermon'\n  * sample   (sample) object 16kB MultiIndex\n  * chain    (sample) int64 16kB 0 0 0 0 0 0 0 0 0 0 0 ... 3 3 3 3 3 3 3 3 3 3 3\n  * draw     (sample) int64 16kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\nData variables:\n    mu       (sample) float64 16kB 7.872 3.385 9.1 7.304 ... 1.767 3.486 3.404\n    theta    (school, sample) float64 128kB 12.32 11.29 5.709 ... 8.452 1.295\n    tau      (sample) float64 16kB 4.726 3.909 4.844 1.857 ... 2.741 2.932 4.461\nAttributes: (6)xarray.DatasetDimensions:sample: 2000school: 8Coordinates: (4)school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')sample(sample)objectMultiIndexarray([(0, 0), (0, 1), (0, 2), ..., (3, 497), (3, 498), (3, 499)], dtype=object)chain(sample)int640 0 0 0 0 0 0 0 ... 3 3 3 3 3 3 3 3array([0, 0, 0, ..., 3, 3, 3])draw(sample)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])Data variables: (3)mu(sample)float647.872 3.385 9.1 ... 3.486 3.404array([7.87179637, 3.38455431, 9.10047569, ..., 1.76673325, 3.48611194,\n       3.40446391])theta(school, sample)float6412.32 11.29 5.709 ... 8.452 1.295array([[12.32068558, 11.28562322,  5.70850575, ...,  3.53251469,\n         4.1827505 ,  0.19295578],\n       [ 9.90536689,  9.12932369,  5.7579324 , ...,  2.00890089,\n         7.55425055,  6.4984278 ],\n       [14.9516155 ,  3.13926327, 10.94458539, ...,  0.51080602,\n         4.45603444, -0.89442402],\n       ...,\n       [16.90179529,  2.39308809,  8.14332707, ...,  4.70724881,\n         1.5289581 ,  7.93646013],\n       [13.19805933, 10.05522282,  7.60475341, ...,  3.07331446,\n         1.09609826,  6.76245459],\n       [15.06136584,  6.17672421,  8.76764689, ..., -2.62306892,\n         8.45228161,  1.2950506 ]])tau(sample)float644.726 3.909 4.844 ... 2.932 4.461array([4.72574006, 3.90899361, 4.8440252 , ..., 2.74060666, 2.93237926,\n       4.46124596])Indexes: (2)schoolPandasIndexPandasIndex(Index(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', 'St. Paul's', 'Mt. Hermon'],\n      dtype='object', name='school'))samplechaindrawPandasMultiIndexPandasIndex(MultiIndex([(0,   0),\n            (0,   1),\n            (0,   2),\n            (0,   3),\n            (0,   4),\n            (0,   5),\n            (0,   6),\n            (0,   7),\n            (0,   8),\n            (0,   9),\n            ...\n            (3, 490),\n            (3, 491),\n            (3, 492),\n            (3, 493),\n            (3, 494),\n            (3, 495),\n            (3, 496),\n            (3, 497),\n            (3, 498),\n            (3, 499)],\n           name='sample', length=2000))Attributes: (6)created_at :2022-10-13T14:37:37.315398arviz_version :0.13.0.dev0inference_library :pymcinference_library_version :4.2.2sampling_time :7.480114936828613tuning_steps :1000\n\n\nYou can achieve the same result using dt.posterior.stack(sample=(\"chain\", \"draw\")). But extract can be more flexible because it takes care of the most common subsetting operations with MCMC samples. It can:\n\nCombine chains and draws\nReturn a subset of variables (with optional filtering with regular expressions or string matching)\nReturn a subset of samples. Moreover, by default, it returns a random subset to prevent getting non-representative samples due to bad mixing.\nAccess any group\n\nTo get a subsample we can specify the number of samples we want with the num_samples argument. For instance, to get 100 samples we can write:\n\nazb.extract(dt, num_samples=100);\n\nIf you need to extract subsets from multiple groups, you should use a random seed. This will ensure that subsamples match. For example, if you do\n\nposterior = azb.extract(dt, num_samples=100, random_seed=124)\nll = azb.extract(dt, group=\"log_likelihood\", num_samples=100, random_seed=124)\n\nYou can inspect the samples in the posterior and ll variables and see that they match.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with DataTree</span>"
    ]
  },
  {
    "objectID": "Chapters/DataTree.html#ploting",
    "href": "Chapters/DataTree.html#ploting",
    "title": "2  Working with DataTree",
    "section": "2.2 Ploting",
    "text": "2.2 Ploting\nXarray has some plotting capabilities, for instance, we can do:\n\ndt.posterior[\"mu\"].plot.hist(figsize=(9, 3));\n\n\n\n\n\n\n\n\nBut in most scenarios calling a plotting function from ArviZ and passing the InfereceData as an argument will be a much better idea.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with DataTree</span>"
    ]
  },
  {
    "objectID": "Chapters/DataTree.html#add-a-new-variable",
    "href": "Chapters/DataTree.html#add-a-new-variable",
    "title": "2  Working with DataTree",
    "section": "2.3 Add a new variable",
    "text": "2.3 Add a new variable\nWe can add variables to existing groups. For instance, we may want to transform a parameter from the posterior. Like computing and adding the \\(\\log\\) of the parameter \\(\\tau\\) to the posterior group.\n\nposterior[\"log_tau\"] = np.log(posterior[\"tau\"])\nposterior\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 12kB\nDimensions:  (sample: 100, school: 8)\nCoordinates:\n  * school   (school) &lt;U16 512B 'Choate' 'Deerfield' ... 'Mt. Hermon'\n  * sample   (sample) object 800B MultiIndex\n  * chain    (sample) int64 800B 3 1 0 2 1 0 2 1 3 1 3 ... 1 3 0 3 2 3 0 1 0 2 1\n  * draw     (sample) int64 800B 80 443 428 346 215 341 ... 424 169 116 493 477\nData variables:\n    mu       (sample) float64 800B 5.949 2.821 8.765 4.16 ... 1.501 1.963 2.821\n    theta    (school, sample) float64 6kB 5.783 3.258 13.21 ... -5.54 3.351\n    tau      (sample) float64 800B 1.637 0.8965 2.171 ... 4.327 5.192 0.8965\n    log_tau  (sample) float64 800B 0.4928 -0.1093 0.7754 ... 1.465 1.647 -0.1093\nAttributes: (6)xarray.DatasetDimensions:sample: 100school: 8Coordinates: (4)school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')sample(sample)objectMultiIndexarray([(3, 80), (1, 443), (0, 428), (2, 346), (1, 215), (0, 341), (2, 24),\n       (1, 305), (3, 477), (1, 37), (3, 335), (1, 141), (2, 496), (2, 473),\n       (1, 349), (1, 3), (3, 197), (1, 322), (2, 73), (0, 100), (2, 220),\n       (1, 136), (2, 7), (3, 448), (3, 405), (2, 252), (0, 360), (2, 266),\n       (3, 125), (2, 169), (3, 372), (1, 181), (0, 290), (0, 349), (2, 427),\n       (0, 50), (3, 103), (3, 355), (1, 289), (3, 141), (0, 304), (0, 204),\n       (0, 340), (2, 97), (2, 284), (2, 276), (1, 252), (0, 190), (3, 450),\n       (1, 472), (0, 486), (2, 383), (2, 278), (1, 56), (3, 76), (2, 381),\n       (1, 381), (3, 254), (0, 495), (0, 350), (0, 337), (0, 67), (0, 363),\n       (3, 213), (3, 43), (3, 366), (2, 212), (3, 233), (3, 295), (2, 70),\n       (3, 147), (2, 289), (3, 492), (2, 359), (3, 347), (1, 400), (3, 44),\n       (2, 76), (0, 137), (0, 265), (3, 175), (0, 306), (2, 313), (3, 363),\n       (0, 87), (2, 6), (3, 332), (3, 198), (3, 465), (1, 86), (3, 171),\n       (0, 212), (3, 176), (2, 127), (3, 409), (0, 424), (1, 169), (0, 116),\n       (2, 493), (1, 477)], dtype=object)chain(sample)int643 1 0 2 1 0 2 1 ... 3 2 3 0 1 0 2 1array([3, 1, 0, 2, 1, 0, 2, 1, 3, 1, 3, 1, 2, 2, 1, 1, 3, 1, 2, 0, 2, 1, 2, 3,\n       3, 2, 0, 2, 3, 2, 3, 1, 0, 0, 2, 0, 3, 3, 1, 3, 0, 0, 0, 2, 2, 2, 1, 0,\n       3, 1, 0, 2, 2, 1, 3, 2, 1, 3, 0, 0, 0, 0, 0, 3, 3, 3, 2, 3, 3, 2, 3, 2,\n       3, 2, 3, 1, 3, 2, 0, 0, 3, 0, 2, 3, 0, 2, 3, 3, 3, 1, 3, 0, 3, 2, 3, 0,\n       1, 0, 2, 1])draw(sample)int6480 443 428 346 ... 169 116 493 477array([ 80, 443, 428, 346, 215, 341,  24, 305, 477,  37, 335, 141, 496, 473,\n       349,   3, 197, 322,  73, 100, 220, 136,   7, 448, 405, 252, 360, 266,\n       125, 169, 372, 181, 290, 349, 427,  50, 103, 355, 289, 141, 304, 204,\n       340,  97, 284, 276, 252, 190, 450, 472, 486, 383, 278,  56,  76, 381,\n       381, 254, 495, 350, 337,  67, 363, 213,  43, 366, 212, 233, 295,  70,\n       147, 289, 492, 359, 347, 400,  44,  76, 137, 265, 175, 306, 313, 363,\n        87,   6, 332, 198, 465,  86, 171, 212, 176, 127, 409, 424, 169, 116,\n       493, 477])Data variables: (4)mu(sample)float645.949 2.821 8.765 ... 1.963 2.821array([ 5.94936049,  2.82139573,  8.76533267,  4.16045136,  3.76987221,\n        3.11233101, 15.94353048,  9.61728443,  6.36172813, -0.81431913,\n        8.50295603,  9.85548018,  3.28989621,  6.15531179,  6.31222318,\n        6.32860189,  5.80188688,  5.68987353,  4.47022303, 11.69695356,\n       14.14504516,  2.50750304,  5.47418953,  6.45785806,  7.11453727,\n        8.52432129,  5.81253179,  2.94283265,  6.55184214,  8.47871082,\n        3.87915295, -1.10251994,  0.33876008,  4.9455064 ,  5.46463267,\n        5.96003438,  6.55184214, -0.31636493,  7.26841364,  6.26523551,\n        8.56808621,  5.52917054,  7.6916741 ,  5.20369319,  4.2885066 ,\n        3.44857834,  5.11340005,  6.95543974,  8.70773394,  2.82139573,\n        4.74332249,  5.5780453 ,  9.54604962,  1.99486785,  8.59263845,\n        3.24256039, -1.50870863, -1.12193332,  5.34165403, -3.46193612,\n        6.91303718,  3.34974272,  2.83746997,  9.31961653,  9.11086845,\n       -2.07268954,  0.64251625,  0.87262557,  6.13965598, -1.32255661,\n        5.51713885,  4.42377154, -1.07427123,  3.06679932,  2.57941332,\n        7.15761793,  9.11086845,  5.3733278 ,  5.36731924,  4.41724637,\n        4.53622774,  8.64494464,  8.72847262, 13.816329  ,  5.68025903,\n        4.42519597, -0.27180086,  5.80188688,  0.14245598,  2.39242767,\n       10.14760757,  6.23164697,  2.57718552,  2.29952034, -1.03020989,\n        0.7634314 , 10.91509296,  1.50071772,  1.96342185,  2.82139573])theta(school, sample)float645.783 3.258 13.21 ... -5.54 3.351array([[ 5.78332001e+00,  3.25808233e+00,  1.32139237e+01,\n         3.88647637e+00,  3.27531513e+00,  3.06489534e+00,\n         1.62319685e+01,  9.67286631e+00, -3.77563324e-01,\n         1.65678231e+00,  1.55032000e+01,  7.00472165e+00,\n         3.93528061e+00,  7.87525652e+00,  1.68769505e+01,\n         7.38045201e+00,  7.25006163e+00,  1.35503511e+01,\n         4.75398239e+00,  1.42338706e+01,  9.97797388e+00,\n         7.22610649e+00,  7.34034281e+00,  5.95451186e+00,\n         1.11538260e+01,  2.82216435e+01,  4.87593796e+00,\n         1.50467592e+01,  5.36521047e+00,  9.61476686e+00,\n         1.60094064e+01,  4.29664121e+00,  1.54374354e+00,\n        -9.92247511e+00,  9.79959946e+00,  1.37715380e+01,\n         5.36521047e+00,  8.98223080e+00,  9.75288135e+00,\n         5.82519719e+00,  8.47108056e+00,  1.05365446e+01,\n         3.51242913e+00,  9.31726415e+00,  6.58714002e+00,\n        -5.37572195e-01,  1.11704690e+01,  7.91077802e+00,\n         9.71868802e+00,  3.25808233e+00,  5.52203735e+00,\n         6.47199316e+00,  5.26285351e+00,  7.52593644e+00,\n         6.08689742e+00,  7.09929855e+00, -7.11695124e-01,\n         3.21324974e-01,  8.89077916e+00,  7.93778775e+00,\n...\n         8.64135992e+00, -1.45149265e+00,  4.21691607e+00,\n         5.26780466e+00, -1.20620822e+00,  7.79568661e+00,\n         1.02279786e+01,  3.35132225e+00,  3.82649315e+00,\n         5.71903515e+00,  1.19331417e+01,  2.98137620e+00,\n         8.86580029e+00,  4.61650021e+00, -3.26253213e+00,\n        -2.86060739e+00,  8.98514447e+00,  6.86662981e+00,\n         7.47430176e+00,  2.82966473e+00,  1.75683067e+01,\n         1.25736665e+01,  9.82982661e+00,  7.98384939e+00,\n         3.72872318e+00,  2.27260207e+00,  5.17551659e+00,\n         3.50553035e+00,  8.94545600e+00,  6.90899655e+00,\n         1.07055784e+01,  3.45131075e+00,  4.75685941e+00,\n         1.05857124e+01,  9.82982661e+00,  5.02954781e+00,\n         2.77374441e+00,  5.15937774e+00,  2.34452622e+00,\n         6.63358496e+00,  1.64939321e+01,  9.61976976e+00,\n         3.73566476e+00,  5.29939272e+00, -1.32845073e+00,\n         5.19428493e+00,  1.65098374e+00,  3.74297277e+00,\n         2.09029102e+00,  5.61562454e+00,  4.76084023e+00,\n         5.62488877e+00,  7.62386038e+00,  1.13856246e+00,\n         1.17969226e+01,  2.85866939e+00, -5.53955074e+00,\n         3.35132225e+00]])tau(sample)float641.637 0.8965 2.171 ... 5.192 0.8965array([ 1.63690948,  0.89648017,  2.17145492,  2.44003772,  6.4706307 ,\n        4.83927067,  5.14483804,  2.68377051,  6.34719621,  1.75520797,\n        7.55636936,  2.81122027,  2.55959157,  3.06376083, 11.96852286,\n        3.39182938,  1.63947794,  7.19673258,  3.52343724,  4.28912896,\n        3.40886567,  2.77572244,  7.842858  ,  2.57264714,  1.93799288,\n       14.30535665,  3.71375867, 17.08049567,  1.17639717,  1.37286522,\n        6.48754336,  6.58572909,  1.39584712, 16.70487579,  4.90218733,\n        4.05506327,  1.17639717,  5.55509696,  3.48861281,  0.92166181,\n        2.23034785,  7.51876593,  4.89777987,  7.87949747,  1.66320641,\n        6.39717291,  3.22451421,  2.71908424,  1.99161225,  0.89648017,\n        3.92732773,  2.08172367,  6.3095818 ,  3.77228077,  1.21468447,\n        2.1611091 ,  2.84612798,  1.25274357,  7.56497739,  6.35226645,\n        1.47248205,  5.00186415,  7.31810286,  3.29329991,  1.67610879,\n        4.6224406 ,  4.76789088,  3.38315881,  1.61688501,  3.83624559,\n        1.66299163,  1.50411265,  6.15519826,  1.05397997,  4.9768676 ,\n        6.89201517,  1.67610879,  2.31645586,  7.70962029,  1.56842995,\n        4.84972805,  1.38231078,  4.92067419,  5.96439896,  3.68292521,\n        9.09769868,  5.84738332,  1.63947794,  2.83651563,  4.3038062 ,\n        6.00521795,  1.41023636,  4.3395538 ,  5.53171606,  6.23327191,\n        6.63950394,  8.09275911,  4.32698195,  5.19168836,  0.89648017])log_tau(sample)float640.4928 -0.1093 ... 1.647 -0.1093array([ 0.49281   , -0.10927911,  0.77539741,  0.8920135 ,  1.86727358,\n        1.57676402,  1.63799389,  0.98722271,  1.84801317,  0.56258735,\n        2.02239083,  1.03361865,  0.9398477 ,  1.11964319,  2.48228011,\n        1.22136942,  0.49437786,  1.97362711,  1.259437  ,  1.45608367,\n        1.22637959,  1.02091105,  2.05960331,  0.94493538,  0.66165284,\n        2.66063406,  1.31204448,  2.83793721,  0.16245652,  0.31689996,\n        1.86988393,  1.88490505,  0.33350148,  2.81570064,  1.5896815 ,\n        1.39996629,  0.16245652,  1.71471588,  1.24950418, -0.08157692,\n        0.80215756,  2.01740202,  1.58878201,  2.06426413,  0.50874731,\n        1.85585616,  1.17078231,  1.00029515,  0.68894449, -0.10927911,\n        1.36795923,  0.73319624,  1.8420694 ,  1.3276798 ,  0.19448435,\n        0.77062156,  1.04595947,  0.22533601,  2.02352936,  1.84881167,\n        0.38694945,  1.60981067,  1.99035112,  1.19189007,  0.51647491,\n        1.53092283,  1.56190404,  1.21880983,  0.48050146,  1.34449418,\n        0.50861817,  0.40820312,  1.81729697,  0.05257344,  1.6048007 ,\n        1.93036352,  0.51647491,  0.84003837,  2.04246894,  0.45007509,\n        1.57892263,  0.32375658,  1.59344555,  1.78580829,  1.30370733,\n        2.20802149,  1.76599427,  0.49437786,  1.04257641,  1.45949979,\n        1.79262875,  0.34375732,  1.46777153,  1.71049809,  1.82990138,\n        1.89303725,  2.09096972,  1.46487029,  1.64705895, -0.10927911])Indexes: (2)schoolPandasIndexPandasIndex(Index(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', 'St. Paul's', 'Mt. Hermon'],\n      dtype='object', name='school'))samplechaindrawPandasMultiIndexPandasIndex(MultiIndex([(3,  80),\n            (1, 443),\n            (0, 428),\n            (2, 346),\n            (1, 215),\n            (0, 341),\n            (2,  24),\n            (1, 305),\n            (3, 477),\n            (1,  37),\n            (3, 335),\n            (1, 141),\n            (2, 496),\n            (2, 473),\n            (1, 349),\n            (1,   3),\n            (3, 197),\n            (1, 322),\n            (2,  73),\n            (0, 100),\n            (2, 220),\n            (1, 136),\n            (2,   7),\n            (3, 448),\n            (3, 405),\n            (2, 252),\n            (0, 360),\n            (2, 266),\n            (3, 125),\n            (2, 169),\n            (3, 372),\n            (1, 181),\n            (0, 290),\n            (0, 349),\n            (2, 427),\n            (0,  50),\n            (3, 103),\n            (3, 355),\n            (1, 289),\n            (3, 141),\n            (0, 304),\n            (0, 204),\n            (0, 340),\n            (2,  97),\n            (2, 284),\n            (2, 276),\n            (1, 252),\n            (0, 190),\n            (3, 450),\n            (1, 472),\n            (0, 486),\n            (2, 383),\n            (2, 278),\n            (1,  56),\n            (3,  76),\n            (2, 381),\n            (1, 381),\n            (3, 254),\n            (0, 495),\n            (0, 350),\n            (0, 337),\n            (0,  67),\n            (0, 363),\n            (3, 213),\n            (3,  43),\n            (3, 366),\n            (2, 212),\n            (3, 233),\n            (3, 295),\n            (2,  70),\n            (3, 147),\n            (2, 289),\n            (3, 492),\n            (2, 359),\n            (3, 347),\n            (1, 400),\n            (3,  44),\n            (2,  76),\n            (0, 137),\n            (0, 265),\n            (3, 175),\n            (0, 306),\n            (2, 313),\n            (3, 363),\n            (0,  87),\n            (2,   6),\n            (3, 332),\n            (3, 198),\n            (3, 465),\n            (1,  86),\n            (3, 171),\n            (0, 212),\n            (3, 176),\n            (2, 127),\n            (3, 409),\n            (0, 424),\n            (1, 169),\n            (0, 116),\n            (2, 493),\n            (1, 477)],\n           name='sample'))Attributes: (6)created_at :2022-10-13T14:37:37.315398arviz_version :0.13.0.dev0inference_library :pymcinference_library_version :4.2.2sampling_time :7.480114936828613tuning_steps :1000",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with DataTree</span>"
    ]
  },
  {
    "objectID": "Chapters/DataTree.html#advance-operations-with-inferencedata",
    "href": "Chapters/DataTree.html#advance-operations-with-inferencedata",
    "title": "2  Working with DataTree",
    "section": "2.4 Advance operations with InferenceData",
    "text": "2.4 Advance operations with InferenceData\nNow we delve into more advanced operations with InferenceData. While these operations are not essential to use ArviZ, they can be useful in some cases. Exploring these advanced functionalities will help you become more familiar with InferenceData and provide additional insights that may enhance your overall experience with ArviZ.\n\n2.4.1 Compute and store posterior pushforward quantities\nWe use “posterior push-forward quantities” to refer to quantities that are not variables in the posterior but deterministic computations using posterior variables.\nYou can use xarray for these push-forward operations and store them as a new variable in the posterior group. You’ll then be able to plot them with ArviZ functions, calculate stats and diagnostics on them (like mcse), or save and share the InferenceData object with the push forward quantities included.\nThe first thing we are going to do is to store the posterior group in a variable called post to make the code more readable. And to compute the log of \\(\\tau\\).\n\npost = dt.posterior\npost[\"log_tau\"] = np.log(post[\"tau\"])\n\nCompute the rolling mean of \\(\\log(\\tau)\\) with xarray.DataArray.rolling, storing the result in the posterior:\n\npost[\"mlogtau\"] = post[\"log_tau\"].rolling({\"draw\": 50}).mean()\n\nUsing xarray for push-forward calculations has all the advantages of working with xarray. It also inherits the disadvantages of working with xarray, but we believe those to be outweighed by the advantages, and we have already shown how to extract the data as NumPy arrays. Working with InferenceData is working mainly with xarray objects and this is what is shown in this guide.\nSome examples of these advantages are specifying operations with named dimensions instead of positional ones (as seen in some previous sections), automatic alignment and broadcasting of arrays (as we’ll see now), or integration with Dask (as shown in the dask_for_arviz guide).\nIn this cell, you will compute pairwise differences between schools on their mean effects (variable theta). To do so, subtract the variable theta after renaming the school dimension to the original variable. Xarray then aligns and broadcasts the two variables because they have different dimensions, and the result is a 4D variable with all the pointwise differences.\nEventually, store the result in the theta_school_diff variable. Notice that the theta_shool_diff variable in the posterior has kept the named dimensions and coordinates:\n\npost[\"theta_school_diff\"] = post.theta - post.theta.rename(school=\"school_bis\")\npost\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DatasetView&gt; Size: 1MB\nDimensions:            (chain: 4, draw: 500, school: 8, school_bis: 8)\nCoordinates:\n  * chain              (chain) int64 32B 0 1 2 3\n  * draw               (draw) int64 4kB 0 1 2 3 4 5 ... 494 495 496 497 498 499\n  * school             (school) &lt;U16 512B 'Choate' 'Deerfield' ... 'Mt. Hermon'\n  * school_bis         (school_bis) &lt;U16 512B 'Choate' ... 'Mt. Hermon'\nData variables:\n    mu                 (chain, draw) float64 16kB 7.872 3.385 ... 3.486 3.404\n    theta              (chain, draw, school) float64 128kB 12.32 9.905 ... 1.295\n    tau                (chain, draw) float64 16kB 4.726 3.909 ... 2.932 4.461\n    log_tau            (chain, draw) float64 16kB 1.553 1.363 ... 1.076 1.495\n    mlogtau            (chain, draw) float64 16kB nan nan nan ... 1.496 1.511\n    theta_school_diff  (chain, draw, school, school_bis) float64 1MB 0.0 ... 0.0\nAttributes: (6)xarray.DataTreeGroups: (0)Dimensions:chain: 4draw: 500school: 8school_bis: 8Coordinates: (4)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')school_bis(school_bis)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Inherited coordinates: (0)Data variables: (6)mu(chain, draw)float647.872 3.385 9.1 ... 3.486 3.404array([[7.871796, 3.384554, 9.100476, ..., 3.656804, 5.427279, 2.735883],\n       [4.315817, 4.597701, 6.213762, ..., 4.841024, 5.946105, 4.666438],\n       [4.527603, 5.862628, 5.910703, ..., 2.871067, 4.095536, 1.775718],\n       [7.179504, 6.539699, 7.48362 , ..., 1.766733, 3.486112, 3.404464]])theta(chain, draw, school)float6412.32 9.905 14.95 ... 6.762 1.295array([[[12.320686,  9.905367, ..., 13.198059, 15.061366],\n        [11.285623,  9.129324, ..., 10.055223,  6.176724],\n        ...,\n        [10.402501,  6.907407, ...,  5.391061,  6.381239],\n        [ 6.661308,  7.413767, ...,  7.56657 ,  9.987618]],\n\n       [[ 5.596639,  6.959605, ...,  7.148854,  3.7909  ],\n        [ 4.96541 ,  1.779358, ...,  3.289161,  5.937311],\n        ...,\n        [ 7.030604,  6.214465, ...,  4.778138,  4.357112],\n        [ 4.884931,  3.952247, ...,  5.687935,  6.777772]],\n\n       [[11.81417 , -0.040477, ...,  0.318087,  0.661304],\n        [ 4.743465, 10.175684, ..., 11.271711,  7.12114 ],\n        ...,\n        [ 5.677651,  1.679108, ...,  1.280615,  7.627658],\n        [ 1.625447,  2.503281, ...,  2.362756, -2.967994]],\n\n       [[ 8.344508,  5.390855, ..., 12.46814 , 12.607797],\n        [ 8.93115 ,  6.852969, ...,  7.013971,  5.136297],\n        ...,\n        [ 4.182751,  7.554251, ...,  1.096098,  8.452282],\n        [ 0.192956,  6.498428, ...,  6.762455,  1.295051]]])tau(chain, draw)float644.726 3.909 4.844 ... 2.932 4.461array([[4.72574 , 3.908994, 4.844025, ..., 1.893838, 5.920062, 4.325896],\n       [1.97083 , 2.049029, 2.123765, ..., 2.17459 , 1.327551, 1.211995],\n       [3.501277, 2.893243, 4.273286, ..., 4.08978 , 2.72017 , 1.917011],\n       [6.07326 , 3.771867, 3.170537, ..., 2.740607, 2.932379, 4.461246]])log_tau(chain, draw)float641.553 1.363 1.578 ... 1.076 1.495array([[1.55302418, 1.36327995, 1.57774603, ..., 0.6386056 , 1.778347  ,\n        1.46461921],\n       [0.67845483, 0.71736608, 0.75319044, ..., 0.77684015, 0.28333575,\n        0.19226749],\n       [1.25312773, 1.06237808, 1.45238307, ..., 1.40849125, 1.00069436,\n        0.65076731],\n       [1.8038955 , 1.32757011, 1.15390112, ..., 1.00817931, 1.07581413,\n        1.49542809]])mlogtau(chain, draw)float64nan nan nan ... 1.494 1.496 1.511array([[       nan,        nan,        nan, ..., 1.16476321, 1.19559572,\n        1.22597193],\n       [       nan,        nan,        nan, ..., 0.12348971, 0.131342  ,\n        0.13737294],\n       [       nan,        nan,        nan, ..., 1.22653292, 1.21500516,\n        1.20868681],\n       [       nan,        nan,        nan, ..., 1.4938526 , 1.49647017,\n        1.5112594 ]])theta_school_diff(chain, draw, school, school_bis)float640.0 2.415 -2.631 ... -5.467 0.0array([[[[ 0.00000000e+00,  2.41531869e+00, -2.63092992e+00, ...,\n          -4.58110972e+00, -8.77373755e-01, -2.74068026e+00],\n         [-2.41531869e+00,  0.00000000e+00, -5.04624860e+00, ...,\n          -6.99642840e+00, -3.29269244e+00, -5.15599894e+00],\n         [ 2.63092992e+00,  5.04624860e+00,  0.00000000e+00, ...,\n          -1.95017980e+00,  1.75355616e+00, -1.09750340e-01],\n         ...,\n         [ 4.58110972e+00,  6.99642840e+00,  1.95017980e+00, ...,\n           0.00000000e+00,  3.70373596e+00,  1.84042946e+00],\n         [ 8.77373755e-01,  3.29269244e+00, -1.75355616e+00, ...,\n          -3.70373596e+00,  0.00000000e+00, -1.86330650e+00],\n         [ 2.74068026e+00,  5.15599894e+00,  1.09750340e-01, ...,\n          -1.84042946e+00,  1.86330650e+00,  0.00000000e+00]],\n\n        [[ 0.00000000e+00,  2.15629954e+00,  8.14635996e+00, ...,\n           8.89253514e+00,  1.23040040e+00,  5.10889901e+00],\n         [-2.15629954e+00,  0.00000000e+00,  5.99006042e+00, ...,\n           6.73623560e+00, -9.25899137e-01,  2.95259947e+00],\n         [-8.14635996e+00, -5.99006042e+00,  0.00000000e+00, ...,\n           7.46175179e-01, -6.91595956e+00, -3.03746095e+00],\n...\n         [-2.65379240e+00, -6.02529245e+00, -2.92707633e+00, ...,\n           0.00000000e+00,  4.32859842e-01, -6.92332351e+00],\n         [-3.08665224e+00, -6.45815229e+00, -3.35993618e+00, ...,\n          -4.32859842e-01,  0.00000000e+00, -7.35618335e+00],\n         [ 4.26953111e+00,  8.98031057e-01,  3.99624717e+00, ...,\n           6.92332351e+00,  7.35618335e+00,  0.00000000e+00]],\n\n        [[ 0.00000000e+00, -6.30547202e+00,  1.08737981e+00, ...,\n          -7.74350435e+00, -6.56949881e+00, -1.10209482e+00],\n         [ 6.30547202e+00,  0.00000000e+00,  7.39285182e+00, ...,\n          -1.43803234e+00, -2.64026792e-01,  5.20337720e+00],\n         [-1.08737981e+00, -7.39285182e+00,  0.00000000e+00, ...,\n          -8.83088416e+00, -7.65687862e+00, -2.18947462e+00],\n         ...,\n         [ 7.74350435e+00,  1.43803234e+00,  8.83088416e+00, ...,\n           0.00000000e+00,  1.17400554e+00,  6.64140953e+00],\n         [ 6.56949881e+00,  2.64026792e-01,  7.65687862e+00, ...,\n          -1.17400554e+00,  0.00000000e+00,  5.46740399e+00],\n         [ 1.10209482e+00, -5.20337720e+00,  2.18947462e+00, ...,\n          -6.64140953e+00, -5.46740399e+00,  0.00000000e+00]]]])Attributes: (6)created_at :2022-10-13T14:37:37.315398arviz_version :0.13.0.dev0inference_library :pymcinference_library_version :4.2.2sampling_time :7.480114936828613tuning_steps :1000\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis same operation using NumPy would require manual alignment of the two arrays to make sure they broadcast correctly. The code could be something like:\ntheta_school_diff = theta[:, :, :, None] - theta[:, :, None, :]\n\n\n\n\n2.4.2 Advanced subsetting\nTo select the value corresponding to the difference between the Choate and Deerfield schools do:\n\npost[\"theta_school_diff\"].sel(school=\"Choate\", school_bis=\"Deerfield\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'theta_school_diff' (chain: 4, draw: 500)&gt; Size: 16kB\n2.415 2.156 -0.04943 1.228 3.384 9.662 ... -1.656 -0.4021 1.524 -3.372 -6.305\nCoordinates:\n  * chain       (chain) int64 32B 0 1 2 3\n  * draw        (draw) int64 4kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\n    school      &lt;U16 64B 'Choate'\n    school_bis  &lt;U16 64B 'Deerfield'xarray.DataArray'theta_school_diff'chain: 4draw: 5002.415 2.156 -0.04943 1.228 3.384 ... -0.4021 1.524 -3.372 -6.305array([[ 2.41531869,  2.15629954, -0.04942665, ..., -1.568983  ,\n         3.49509445, -0.75245938],\n       [-1.36296658,  3.18605183, -3.57582959, ..., -0.03288082,\n         0.81613882,  0.93268411],\n       [11.85464717, -5.43221857,  0.85662645, ..., -1.62004592,\n         3.99854315, -0.87783417],\n       [ 2.95365309,  2.07818191,  4.51853032, ...,  1.5236138 ,\n        -3.37150005, -6.30547202]])Coordinates: (4)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])school()&lt;U16'Choate'array('Choate', dtype='&lt;U16')school_bis()&lt;U16'Deerfield'array('Deerfield', dtype='&lt;U16')Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))Attributes: (0)\n\n\nFor more advanced subsetting (the equivalent to what is sometimes called “fancy indexing” in NumPy) you need to provide the indices as DataArray objects:\n\nschool_idx = xr.DataArray([\"Choate\", \"Hotchkiss\", \"Mt. Hermon\"], dims=[\"pairwise_school_diff\"])\nschool_bis_idx = xr.DataArray(\n    [\"Deerfield\", \"Choate\", \"Lawrenceville\"], dims=[\"pairwise_school_diff\"]\n)\npost[\"theta_school_diff\"].sel(school=school_idx, school_bis=school_bis_idx)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'theta_school_diff' (chain: 4, draw: 500,\n                                       pairwise_school_diff: 3)&gt; Size: 48kB\n2.415 -6.741 -1.84 2.156 -3.474 3.784 ... -2.619 6.923 -6.305 1.667 -6.641\nCoordinates:\n  * chain       (chain) int64 32B 0 1 2 3\n  * draw        (draw) int64 4kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\n    school      (pairwise_school_diff) &lt;U16 192B 'Choate' ... 'Mt. Hermon'\n    school_bis  (pairwise_school_diff) &lt;U16 192B 'Deerfield' ... 'Lawrenceville'\nDimensions without coordinates: pairwise_school_diffxarray.DataArray'theta_school_diff'chain: 4draw: 500pairwise_school_diff: 32.415 -6.741 -1.84 2.156 -3.474 ... -2.619 6.923 -6.305 1.667 -6.641array([[[  2.41531869,  -6.74108399,  -1.84042946],\n        [  2.15629954,  -3.47410767,   3.78363613],\n        [ -0.04942665,   4.28447846,   0.62431982],\n        ...,\n        [ -1.568983  ,   5.53829038,  -3.9072901 ],\n        [  3.49509445, -12.62680339,   9.21627977],\n        [ -0.75245938,  -7.16363884,  14.24249242]],\n\n       [[ -1.36296658,  -2.09672634,  -1.78220365],\n        [  3.18605183,   1.39310745,   1.30385219],\n        [ -3.57582959,  -3.73894295,   1.50957259],\n        ...,\n        [ -0.03288082,   0.22920766,   4.259231  ],\n        [  0.81613882,  -1.39409761,  -2.83464009],\n        [  0.93268411,   0.99243438,   1.52660383]],\n\n       [[ 11.85464717,  -4.83033352,  -2.06409623],\n        [ -5.43221857,  -2.45302911,  -0.29934021],\n        [  0.85662645,  -6.70463782,  -4.8911762 ],\n        ...,\n        [ -1.62004592,  -0.95078454,  -1.95522498],\n        [  3.99854315,  -0.85757228,   6.01710874],\n        [ -0.87783417,  -0.5224125 ,  -4.44552037]],\n\n       [[  2.95365309, -17.43005136,   4.73220433],\n        [  2.07818191,   3.61925306,   5.02518058],\n        [  4.51853032, -10.373311  ,  -2.59184142],\n        ...,\n        [  1.5236138 ,  -0.88482747,  -7.33031773],\n        [ -3.37150005,  -2.61944346,   6.92332351],\n        [ -6.30547202,   1.66679103,  -6.64140953]]])Coordinates: (4)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])school(pairwise_school_diff)&lt;U16'Choate' 'Hotchkiss' 'Mt. Hermon'array(['Choate', 'Hotchkiss', 'Mt. Hermon'], dtype='&lt;U16')school_bis(pairwise_school_diff)&lt;U16'Deerfield' ... 'Lawrenceville'array(['Deerfield', 'Choate', 'Lawrenceville'], dtype='&lt;U16')Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))Attributes: (0)\n\n\nUsing lists or NumPy arrays instead of DataArrays does column/row-based indexing. As you can see, the result has 9 values of theta_shool_diff instead of the 3 pairs of difference we selected in the previous cell:\n\npost[\"theta_school_diff\"].sel(\n    school=[\"Choate\", \"Hotchkiss\", \"Mt. Hermon\"],\n    school_bis=[\"Deerfield\", \"Choate\", \"Lawrenceville\"],\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'theta_school_diff' (chain: 4, draw: 500, school: 3,\n                                       school_bis: 3)&gt; Size: 144kB\n2.415 0.0 -4.581 -4.326 -6.741 -11.32 ... 1.667 -6.077 -5.203 1.102 -6.641\nCoordinates:\n  * chain       (chain) int64 32B 0 1 2 3\n  * draw        (draw) int64 4kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\n  * school      (school) &lt;U16 192B 'Choate' 'Hotchkiss' 'Mt. Hermon'\n  * school_bis  (school_bis) &lt;U16 192B 'Deerfield' 'Choate' 'Lawrenceville'xarray.DataArray'theta_school_diff'chain: 4draw: 500school: 3school_bis: 32.415 0.0 -4.581 -4.326 -6.741 ... 1.667 -6.077 -5.203 1.102 -6.641array([[[[  2.41531869,   0.        ,  -4.58110972],\n         [ -4.3257653 ,  -6.74108399, -11.3221937 ],\n         [  5.15599894,   2.74068026,  -1.84042946]],\n\n        [[  2.15629954,   0.        ,   8.89253514],\n         [ -1.31780813,  -3.47410767,   5.41842747],\n         [ -2.95259947,  -5.10889901,   3.78363613]],\n\n        [[ -0.04942665,   0.        ,  -2.43482132],\n         [  4.2350518 ,   4.28447846,   1.84965714],\n         [  3.00971448,   3.05914114,   0.62431982]],\n\n        ...,\n\n        [[ -1.568983  ,   0.        ,  -7.17971855],\n         [  3.96930738,   5.53829038,  -1.64142817],\n         [  1.70344545,   3.27242845,  -3.9072901 ]],\n\n        [[  3.49509445,   0.        ,  13.23754201],\n         [ -9.13170894, -12.62680339,   0.61073863],\n...\n         [  5.69743498,   3.61925306,  12.43928685],\n         [ -1.71667129,  -3.7948532 ,   5.02518058]],\n\n        [[  4.51853032,   0.        ,  -1.82804532],\n         [ -5.85478068, -10.373311  , -12.20135632],\n         [  3.75473422,  -0.7637961 ,  -2.59184142]],\n\n        ...,\n\n        [[  1.5236138 ,   0.        ,  -1.17473412],\n         [  0.63878633,  -0.88482747,  -2.05956159],\n         [ -4.63196981,  -6.15558361,  -7.33031773]],\n\n        [[ -3.37150005,   0.        ,   2.6537924 ],\n         [ -5.99094351,  -2.61944346,   0.03434894],\n         [  0.89803106,   4.26953111,   6.92332351]],\n\n        [[ -6.30547202,   0.        ,  -7.74350435],\n         [ -4.63868099,   1.66679103,  -6.07671332],\n         [ -5.2033772 ,   1.10209482,  -6.64140953]]]])Coordinates: (4)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])school(school)&lt;U16'Choate' 'Hotchkiss' 'Mt. Hermon'array(['Choate', 'Hotchkiss', 'Mt. Hermon'], dtype='&lt;U16')school_bis(school_bis)&lt;U16'Deerfield' ... 'Lawrenceville'array(['Deerfield', 'Choate', 'Lawrenceville'], dtype='&lt;U16')Indexes: (4)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))schoolPandasIndexPandasIndex(Index(['Choate', 'Hotchkiss', 'Mt. Hermon'], dtype='object', name='school'))school_bisPandasIndexPandasIndex(Index(['Deerfield', 'Choate', 'Lawrenceville'], dtype='object', name='school_bis'))Attributes: (0)\n\n\n\n\n2.4.3 Add new chains using concat\nAfter checking the mcse and realizing you need more samples, you rerun the model with two chains and obtain an dt_rerun object.\n\ndt_rerun = (\n    dt.posterior.sel(chain=[0, 1])\n    .copy()\n    .to_dataset()\n    .assign_coords(coords={\"chain\": [4, 5]})\n)\n\nYou can combine the two into a single InferenceData object using the concat function from ArviZ:\n\ndt_complete = xr.concat([dt.posterior.to_dataset(), dt_rerun], dim=\"chain\")\ndt_complete.dims\n\nFrozenMappingWarningOnValuesAccess({'chain': 6, 'draw': 500, 'school': 8, 'school_bis': 8})\n\n\n\n\n2.4.4 Add groups to InferenceData objects\nTo add new groups to InferenceData objects you can use the extend method if the new groups are already in an InferenceData object or the add_groups method if the new groups are dictionaries or xarray.Dataset objects.\n\nrng = np.random.default_rng(3)\nds = azb.dict_to_dataset(\n    {\"obs\": rng.normal(size=(4, 500, 2))},\n    dims={\"obs\": [\"new_school\"]},\n    coords={\"new_school\": [\"Essex College\", \"Moordale\"]},\n)\ndicto = {k:v for k,v in dt.items()}\ndicto[\"predictions\"] = ds\n\nnew_dt = xr.DataTree.from_dict(dicto)\nnew_dt\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DatasetView&gt; Size: 0B\nDimensions:  ()\nData variables:\n    *empty*xarray.DataTreeGroups: (9)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DatasetView&gt; Size: 1MB\nDimensions:            (chain: 4, draw: 500, school: 8, school_bis: 8)\nCoordinates:\n  * chain              (chain) int64 32B 0 1 2 3\n  * draw               (draw) int64 4kB 0 1 2 3 4 5 ... 494 495 496 497 498 499\n  * school             (school) &lt;U16 512B 'Choate' 'Deerfield' ... 'Mt. Hermon'\n  * school_bis         (school_bis) &lt;U16 512B 'Choate' ... 'Mt. Hermon'\nData variables:\n    mu                 (chain, draw) float64 16kB 7.872 3.385 ... 3.486 3.404\n    theta              (chain, draw, school) float64 128kB 12.32 9.905 ... 1.295\n    tau                (chain, draw) float64 16kB 4.726 3.909 ... 2.932 4.461\n    log_tau            (chain, draw) float64 16kB 1.553 1.363 ... 1.076 1.495\n    mlogtau            (chain, draw) float64 16kB nan nan nan ... 1.496 1.511\n    theta_school_diff  (chain, draw, school, school_bis) float64 1MB 0.0 ... 0.0\nAttributes: (6)posteriorGroups: (0)Dimensions:chain: 4draw: 500school: 8school_bis: 8Coordinates: (4)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')school_bis(school_bis)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (6)mu(chain, draw)float647.872 3.385 9.1 ... 3.486 3.404array([[7.871796, 3.384554, 9.100476, ..., 3.656804, 5.427279, 2.735883],\n       [4.315817, 4.597701, 6.213762, ..., 4.841024, 5.946105, 4.666438],\n       [4.527603, 5.862628, 5.910703, ..., 2.871067, 4.095536, 1.775718],\n       [7.179504, 6.539699, 7.48362 , ..., 1.766733, 3.486112, 3.404464]])theta(chain, draw, school)float6412.32 9.905 14.95 ... 6.762 1.295array([[[12.320686,  9.905367, ..., 13.198059, 15.061366],\n        [11.285623,  9.129324, ..., 10.055223,  6.176724],\n        ...,\n        [10.402501,  6.907407, ...,  5.391061,  6.381239],\n        [ 6.661308,  7.413767, ...,  7.56657 ,  9.987618]],\n\n       [[ 5.596639,  6.959605, ...,  7.148854,  3.7909  ],\n        [ 4.96541 ,  1.779358, ...,  3.289161,  5.937311],\n        ...,\n        [ 7.030604,  6.214465, ...,  4.778138,  4.357112],\n        [ 4.884931,  3.952247, ...,  5.687935,  6.777772]],\n\n       [[11.81417 , -0.040477, ...,  0.318087,  0.661304],\n        [ 4.743465, 10.175684, ..., 11.271711,  7.12114 ],\n        ...,\n        [ 5.677651,  1.679108, ...,  1.280615,  7.627658],\n        [ 1.625447,  2.503281, ...,  2.362756, -2.967994]],\n\n       [[ 8.344508,  5.390855, ..., 12.46814 , 12.607797],\n        [ 8.93115 ,  6.852969, ...,  7.013971,  5.136297],\n        ...,\n        [ 4.182751,  7.554251, ...,  1.096098,  8.452282],\n        [ 0.192956,  6.498428, ...,  6.762455,  1.295051]]])tau(chain, draw)float644.726 3.909 4.844 ... 2.932 4.461array([[4.72574 , 3.908994, 4.844025, ..., 1.893838, 5.920062, 4.325896],\n       [1.97083 , 2.049029, 2.123765, ..., 2.17459 , 1.327551, 1.211995],\n       [3.501277, 2.893243, 4.273286, ..., 4.08978 , 2.72017 , 1.917011],\n       [6.07326 , 3.771867, 3.170537, ..., 2.740607, 2.932379, 4.461246]])log_tau(chain, draw)float641.553 1.363 1.578 ... 1.076 1.495array([[1.55302418, 1.36327995, 1.57774603, ..., 0.6386056 , 1.778347  ,\n        1.46461921],\n       [0.67845483, 0.71736608, 0.75319044, ..., 0.77684015, 0.28333575,\n        0.19226749],\n       [1.25312773, 1.06237808, 1.45238307, ..., 1.40849125, 1.00069436,\n        0.65076731],\n       [1.8038955 , 1.32757011, 1.15390112, ..., 1.00817931, 1.07581413,\n        1.49542809]])mlogtau(chain, draw)float64nan nan nan ... 1.494 1.496 1.511array([[       nan,        nan,        nan, ..., 1.16476321, 1.19559572,\n        1.22597193],\n       [       nan,        nan,        nan, ..., 0.12348971, 0.131342  ,\n        0.13737294],\n       [       nan,        nan,        nan, ..., 1.22653292, 1.21500516,\n        1.20868681],\n       [       nan,        nan,        nan, ..., 1.4938526 , 1.49647017,\n        1.5112594 ]])theta_school_diff(chain, draw, school, school_bis)float640.0 2.415 -2.631 ... -5.467 0.0array([[[[ 0.00000000e+00,  2.41531869e+00, -2.63092992e+00, ...,\n          -4.58110972e+00, -8.77373755e-01, -2.74068026e+00],\n         [-2.41531869e+00,  0.00000000e+00, -5.04624860e+00, ...,\n          -6.99642840e+00, -3.29269244e+00, -5.15599894e+00],\n         [ 2.63092992e+00,  5.04624860e+00,  0.00000000e+00, ...,\n          -1.95017980e+00,  1.75355616e+00, -1.09750340e-01],\n         ...,\n         [ 4.58110972e+00,  6.99642840e+00,  1.95017980e+00, ...,\n           0.00000000e+00,  3.70373596e+00,  1.84042946e+00],\n         [ 8.77373755e-01,  3.29269244e+00, -1.75355616e+00, ...,\n          -3.70373596e+00,  0.00000000e+00, -1.86330650e+00],\n         [ 2.74068026e+00,  5.15599894e+00,  1.09750340e-01, ...,\n          -1.84042946e+00,  1.86330650e+00,  0.00000000e+00]],\n\n        [[ 0.00000000e+00,  2.15629954e+00,  8.14635996e+00, ...,\n           8.89253514e+00,  1.23040040e+00,  5.10889901e+00],\n         [-2.15629954e+00,  0.00000000e+00,  5.99006042e+00, ...,\n           6.73623560e+00, -9.25899137e-01,  2.95259947e+00],\n         [-8.14635996e+00, -5.99006042e+00,  0.00000000e+00, ...,\n           7.46175179e-01, -6.91595956e+00, -3.03746095e+00],\n...\n         [-2.65379240e+00, -6.02529245e+00, -2.92707633e+00, ...,\n           0.00000000e+00,  4.32859842e-01, -6.92332351e+00],\n         [-3.08665224e+00, -6.45815229e+00, -3.35993618e+00, ...,\n          -4.32859842e-01,  0.00000000e+00, -7.35618335e+00],\n         [ 4.26953111e+00,  8.98031057e-01,  3.99624717e+00, ...,\n           6.92332351e+00,  7.35618335e+00,  0.00000000e+00]],\n\n        [[ 0.00000000e+00, -6.30547202e+00,  1.08737981e+00, ...,\n          -7.74350435e+00, -6.56949881e+00, -1.10209482e+00],\n         [ 6.30547202e+00,  0.00000000e+00,  7.39285182e+00, ...,\n          -1.43803234e+00, -2.64026792e-01,  5.20337720e+00],\n         [-1.08737981e+00, -7.39285182e+00,  0.00000000e+00, ...,\n          -8.83088416e+00, -7.65687862e+00, -2.18947462e+00],\n         ...,\n         [ 7.74350435e+00,  1.43803234e+00,  8.83088416e+00, ...,\n           0.00000000e+00,  1.17400554e+00,  6.64140953e+00],\n         [ 6.56949881e+00,  2.64026792e-01,  7.65687862e+00, ...,\n          -1.17400554e+00,  0.00000000e+00,  5.46740399e+00],\n         [ 1.10209482e+00, -5.20337720e+00,  2.18947462e+00, ...,\n          -6.64140953e+00, -5.46740399e+00,  0.00000000e+00]]]])Attributes: (6)created_at :2022-10-13T14:37:37.315398arviz_version :0.13.0.dev0inference_library :pymcinference_library_version :4.2.2sampling_time :7.480114936828613tuning_steps :1000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DatasetView&gt; Size: 133kB\nDimensions:  (chain: 4, draw: 500, school: 8)\nCoordinates:\n  * chain    (chain) int64 32B 0 1 2 3\n  * draw     (draw) int64 4kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\n  * school   (school) &lt;U16 512B 'Choate' 'Deerfield' ... 'Mt. Hermon'\nData variables:\n    obs      (chain, draw, school) float64 128kB ...\nAttributes: (4)posterior_predictiveGroups: (0)Dimensions:chain: 4draw: 500school: 8Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (1)obs(chain, draw, school)float64...[16000 values with dtype=float64]Attributes: (4)arviz_version :0.13.0.dev0created_at :2022-10-13T14:37:41.460544inference_library :pymcinference_library_version :4.2.2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DatasetView&gt; Size: 133kB\nDimensions:  (chain: 4, draw: 500, school: 8)\nCoordinates:\n  * chain    (chain) int64 32B 0 1 2 3\n  * draw     (draw) int64 4kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\n  * school   (school) &lt;U16 512B 'Choate' 'Deerfield' ... 'Mt. Hermon'\nData variables:\n    obs      (chain, draw, school) float64 128kB -4.173 -3.24 ... -3.853 -3.986\nAttributes: (4)log_likelihoodGroups: (0)Dimensions:chain: 4draw: 500school: 8Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (1)obs(chain, draw, school)float64-4.173 -3.24 ... -3.853 -3.986array([[[-4.173302, -3.239676, ..., -3.336817, -3.823773],\n        [-4.247812, -3.2279  , ..., -3.537121, -3.861641],\n        ...,\n        [-4.315149, -3.227492, ..., -4.01645 , -3.85803 ],\n        [-4.638855, -3.223242, ..., -3.765806, -3.81556 ]],\n\n       [[-4.742346, -3.226936, ..., -3.81026 , -3.913306],\n        [-4.806083, -3.415006, ..., -4.303568, -3.866033],\n        ...,\n        [-4.604134, -3.237464, ..., -4.095612, -3.899455],\n        [-4.814336, -3.303445, ..., -3.979458, -3.851396]],\n\n       [[-4.209169, -3.54477 , ..., -4.784774, -4.007715],\n        [-4.828914, -3.245192, ..., -3.447873, -3.846044],\n        ...,\n        [-4.734294, -3.421292, ..., -4.619213, -3.838812],\n        [-5.172804, -3.372593, ..., -4.444141, -4.155052]],\n\n       [[-4.485518, -3.255562, ..., -3.374531, -3.80988 ],\n        [-4.435035, -3.228102, ..., -3.824988, -3.882012],\n        ...,\n        [-4.88757 , -3.222517, ..., -4.650233, -3.828734],\n        [-5.345281, -3.232797, ..., -3.852936, -3.986156]]])Attributes: (4)arviz_version :0.13.0.dev0created_at :2022-10-13T14:37:37.487399inference_library :pymcinference_library_version :4.2.2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DatasetView&gt; Size: 246kB\nDimensions:              (chain: 4, draw: 500)\nCoordinates:\n  * chain                (chain) int64 32B 0 1 2 3\n  * draw                 (draw) int64 4kB 0 1 2 3 4 5 ... 495 496 497 498 499\nData variables: (12/16)\n    max_energy_error     (chain, draw) float64 16kB ...\n    energy_error         (chain, draw) float64 16kB ...\n    lp                   (chain, draw) float64 16kB ...\n    index_in_trajectory  (chain, draw) int64 16kB ...\n    acceptance_rate      (chain, draw) float64 16kB ...\n    diverging            (chain, draw) bool 2kB ...\n    ...                   ...\n    smallest_eigval      (chain, draw) float64 16kB ...\n    step_size_bar        (chain, draw) float64 16kB ...\n    step_size            (chain, draw) float64 16kB ...\n    energy               (chain, draw) float64 16kB ...\n    tree_depth           (chain, draw) int64 16kB ...\n    perf_counter_diff    (chain, draw) float64 16kB ...\nAttributes: (6)sample_statsGroups: (0)Dimensions:chain: 4draw: 500Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])Data variables: (16)max_energy_error(chain, draw)float64...[2000 values with dtype=float64]energy_error(chain, draw)float64...[2000 values with dtype=float64]lp(chain, draw)float64...[2000 values with dtype=float64]index_in_trajectory(chain, draw)int64...[2000 values with dtype=int64]acceptance_rate(chain, draw)float64...[2000 values with dtype=float64]diverging(chain, draw)bool...[2000 values with dtype=bool]process_time_diff(chain, draw)float64...[2000 values with dtype=float64]n_steps(chain, draw)float64...[2000 values with dtype=float64]perf_counter_start(chain, draw)float64...[2000 values with dtype=float64]largest_eigval(chain, draw)float64...[2000 values with dtype=float64]smallest_eigval(chain, draw)float64...[2000 values with dtype=float64]step_size_bar(chain, draw)float64...[2000 values with dtype=float64]step_size(chain, draw)float64...[2000 values with dtype=float64]energy(chain, draw)float64...[2000 values with dtype=float64]tree_depth(chain, draw)int64...[2000 values with dtype=int64]perf_counter_diff(chain, draw)float64...[2000 values with dtype=float64]Attributes: (6)arviz_version :0.13.0.dev0created_at :2022-10-13T14:37:37.324929inference_library :pymcinference_library_version :4.2.2sampling_time :7.480114936828613tuning_steps :1000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DatasetView&gt; Size: 45kB\nDimensions:  (chain: 1, draw: 500, school: 8)\nCoordinates:\n  * chain    (chain) int64 8B 0\n  * draw     (draw) int64 4kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\n  * school   (school) &lt;U16 512B 'Choate' 'Deerfield' ... 'Mt. Hermon'\nData variables:\n    tau      (chain, draw) float64 4kB ...\n    theta    (chain, draw, school) float64 32kB ...\n    mu       (chain, draw) float64 4kB ...\nAttributes: (4)priorGroups: (0)Dimensions:chain: 1draw: 500school: 8Coordinates: (3)chain(chain)int640array([0])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (3)tau(chain, draw)float64...[500 values with dtype=float64]theta(chain, draw, school)float64...[4000 values with dtype=float64]mu(chain, draw)float64...[500 values with dtype=float64]Attributes: (4)arviz_version :0.13.0.dev0created_at :2022-10-13T14:37:26.602116inference_library :pymcinference_library_version :4.2.2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DatasetView&gt; Size: 37kB\nDimensions:  (chain: 1, draw: 500, school: 8)\nCoordinates:\n  * chain    (chain) int64 8B 0\n  * draw     (draw) int64 4kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\n  * school   (school) &lt;U16 512B 'Choate' 'Deerfield' ... 'Mt. Hermon'\nData variables:\n    obs      (chain, draw, school) float64 32kB ...\nAttributes: (4)prior_predictiveGroups: (0)Dimensions:chain: 1draw: 500school: 8Coordinates: (3)chain(chain)int640array([0])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (1)obs(chain, draw, school)float64...[4000 values with dtype=float64]Attributes: (4)arviz_version :0.13.0.dev0created_at :2022-10-13T14:37:26.604969inference_library :pymcinference_library_version :4.2.2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DatasetView&gt; Size: 576B\nDimensions:  (school: 8)\nCoordinates:\n  * school   (school) &lt;U16 512B 'Choate' 'Deerfield' ... 'Mt. Hermon'\nData variables:\n    obs      (school) float64 64B ...\nAttributes: (4)observed_dataGroups: (0)Dimensions:school: 8Coordinates: (1)school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (1)obs(school)float64...[8 values with dtype=float64]Attributes: (4)arviz_version :0.13.0.dev0created_at :2022-10-13T14:37:26.606375inference_library :pymcinference_library_version :4.2.2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DatasetView&gt; Size: 576B\nDimensions:  (school: 8)\nCoordinates:\n  * school   (school) &lt;U16 512B 'Choate' 'Deerfield' ... 'Mt. Hermon'\nData variables:\n    scores   (school) float64 64B ...\nAttributes: (4)constant_dataGroups: (0)Dimensions:school: 8Coordinates: (1)school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (1)scores(school)float64...[8 values with dtype=float64]Attributes: (4)arviz_version :0.13.0.dev0created_at :2022-10-13T14:37:26.607471inference_library :pymcinference_library_version :4.2.2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DatasetView&gt; Size: 36kB\nDimensions:     (chain: 4, draw: 500, new_school: 2)\nCoordinates:\n  * chain       (chain) int64 32B 0 1 2 3\n  * draw        (draw) int64 4kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\n  * new_school  (new_school) &lt;U13 104B 'Essex College' 'Moordale'\nData variables:\n    obs         (chain, draw, new_school) float64 32kB 2.041 -2.556 ... -0.2822\nAttributes: (4)predictionsGroups: (0)Dimensions:chain: 4draw: 500new_school: 2Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])new_school(new_school)&lt;U13'Essex College' 'Moordale'array(['Essex College', 'Moordale'], dtype='&lt;U13')Data variables: (1)obs(chain, draw, new_school)float642.041 -2.556 ... -1.015 -0.2822array([[[ 2.04091912, -2.55566503],\n        [ 0.41809885, -0.56776961],\n        [-0.45264929, -0.21559716],\n        ...,\n        [-0.80265585,  0.40858787],\n        [ 0.89066617,  0.91324226],\n        [ 0.30152948, -2.85103878]],\n\n       [[-1.02941822,  0.81504467],\n        [-0.86725243, -1.00340203],\n        [-2.30495532,  1.26656886],\n        ...,\n        [-1.40028095,  1.9391935 ],\n        [-0.37582993, -0.76872586],\n        [ 0.11466401, -0.89829659]],\n\n       [[ 0.02963037, -0.96028439],\n        [ 0.56533507,  0.05565896],\n        [-1.36828642,  1.0376982 ],\n        ...,\n        [ 0.23222422,  0.36513287],\n        [ 0.31840946, -0.56685801],\n        [ 2.39826354,  0.91078977]],\n\n       [[-1.42283401, -0.74058959],\n        [ 0.83390251,  0.53293412],\n        [ 0.13188271, -0.03434879],\n        ...,\n        [ 1.57846099,  0.24653314],\n        [ 0.64302486,  1.42710376],\n        [-1.01529472, -0.28215614]]])Attributes: (4)created_at :2024-12-19T00:02:36.108193+00:00creation_library :ArviZcreation_library_version :0.4.0.dev0creation_library_language :PythonDimensions:Coordinates: (0)Inherited coordinates: (0)Data variables: (0)Attributes: (0)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with DataTree</span>"
    ]
  },
  {
    "objectID": "Chapters/DataTree.html#final-remarks",
    "href": "Chapters/DataTree.html#final-remarks",
    "title": "2  Working with DataTree",
    "section": "2.5 Final remarks",
    "text": "2.5 Final remarks\nWe have discussed a few of the most common operations with DataTree objects. If you want to learn more about InferenceData, you can check the InferenceData API documentation.\nIf you have doubts about how to use InferenceData with ArviZ functions, you can ask questions at PyMC’s discourse\n\n\n\n\nHoyer, Stephan, and Joe Hamman. 2017. “Xarray: N-D Labeled Arrays and Datasets in Python.” Journal of Open Research Software 5 (1). https://doi.org/10.5334/jors.148.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with DataTree</span>"
    ]
  },
  {
    "objectID": "Chapters/Distributions.html",
    "href": "Chapters/Distributions.html",
    "title": "3  Random variables, distributions, and uncertainty",
    "section": "",
    "text": "3.1 Random variables\nFrom a Bayesian perspective probabilities represent a degree of (un)certainty about the occurrence of an event. It is a measure of the likelihood that a particular hypothesis or event is true, given the available data and prior knowledge. We assign the value 0 to something impossible and 1 to something certain. When we are unsure we assign a value in between. For example, we could say that the probability of rain tomorrow is 0.32. This means that we are 32% certain that it will rain tomorrow.\nIn practice we usually do not care about individual probabilities, instead we work with probability distributions. A probability distribution describes the probabilities associated with each possible outcome of an experiment. In statistics, the term “experiment” is used in a very wide sense. It could mean a well-planned experiment in a laboratory, but it could also mean the result of a poll, the observation of the weather tomorrow, or the number of people that will visit a website next week.\nLet’s consider the experiment of observing the weather tomorrow. The possible outcomes of this experiment include the following outcomes:\nNotice that we are omitting the possibility of snow, or hail. In other words, we are assigning 0 probability to those outcomes. It is usually the case that we do not ponder all the possible outcomes of an experiment, either because we deliberately assume them to be irrelevant, because we don’t know about them, or because is too complex/expensive/time-consuming/etc to take them all into account.\nAnother important thing to notice, from this example, is that these outcomes are words (or strings if you want). To work with them we need to assign a number to each outcome. For example, we could assign the numbers 0 to Rainy, 1 to Sunny, and 2 to Cloudy. This mapping from the outcomes to the numbers is called a random variable. This is a funny and potentially misleading name as its mathematical definition is not random (the mapping is deterministic) nor a variable (it is a function). The mapping is arbitrary, -1 to Rainy, 0 to Sunny, and 4 to Cloudy is also valid. But once we pick one mapping, we keep using it for the rest of the experiment or analysis. One common source of confusion is understanding where the randomness comes from if the mapping is deterministic. The randomness comes from the uncertainty about the outcome of the experiment, i.e. the weather tomorrow. We are not sure if it will be rainy tomorrow until tomorrow comes.\nRandom variables can be classified into two main types: discrete and continuous.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random variables, distributions, and uncertainty</span>"
    ]
  },
  {
    "objectID": "Chapters/Distributions.html#random-variables",
    "href": "Chapters/Distributions.html#random-variables",
    "title": "3  Random variables, distributions, and uncertainty",
    "section": "",
    "text": "Rainy\nSunny\nCloudy\n\n\n\n\n\nDiscrete Random Variables: They can take on a countable number of distinct values. We already saw an example of a discrete random variable, the weather tomorrow. It can take on three values: Rainy, Sunny, and Cloudy. No intermediate values are allowed in our experiment, even when it is true that it can be partially sunny and still rain. And it has to be at least partially cloudy to rain. But we are not considering those possibilities.\nContinuous Random Variables: They can take on any value within a certain range. For example, the temperature tomorrow is a continuous random variable. If we use a Celsius scale, then it can take on any value between -273.15 Celsius to \\(+ \\infty\\). Of course, in practice, the expected temperature is restricted to a much narrower range. The lowest recorded temperature on Earth is −89.2 °C and the highest is 56.7 °C, and that range will be even narrower if we consider a particular region of our planet.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random variables, distributions, and uncertainty</span>"
    ]
  },
  {
    "objectID": "Chapters/Distributions.html#probability-mass-and-density-functionsc",
    "href": "Chapters/Distributions.html#probability-mass-and-density-functionsc",
    "title": "3  Random variables, distributions, and uncertainty",
    "section": "3.2 Probability mass and density functionsc",
    "text": "3.2 Probability mass and density functionsc\nThe probability distribution of a discrete random variable is often described using a probability mass function (PMF), which gives the probability of each possible outcome. For instance, the following plot shows the probability mass function of a categorical distribution with three possible outcomes, like Rainy, Sunny, and Cloudy.\n\npz.Categorical([0.15, 0.6, 0.25]).plot_pdf();\n\n\n\n\n\n\n\nFigure 3.1: PMF of a Categorical disribution\n\n\n\n\n\nUsually, there is more than one probability distribution that we can use to represent the same set of probabilities, for instance, we could use a binomial distribution.\n\npz.Binomial(2, 0.6).plot_pdf();\n\n\n\n\n\n\n\nFigure 3.2: PMF of a Binomial distribution\n\n\n\n\n\nThe probability distribution of a continuous random variable is described using a probability density function (PDF), which specifies the likelihood of the random variable falling within a particular interval. For instance, we could use a normal distribution to describe the temperature tomorrow.\n\npz.Normal(30, 4).plot_pdf();\n\n\n\n\n\n\n\nFigure 3.3: PDF of a normal distribution\n\n\n\n\n\nor maybe a skew normal like this if we expect higher temperatures like during summer.\n\npz.SkewNormal(38, 5, -2).plot_pdf();\n\n\n\n\n\n\n\nFigure 3.4: PDF of a skew-normal distribution\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that we use plot_pdf() instead of plot_pmf(), this is because PreliZ uses the same method to plot both discrete and continuous distributions. In the case of discrete distributions, it will plot the probability mass function (PMF), and in the case of continuous distributions, it will plot the probability density function (PDF).\n\n\nOne issue when interpreting a PDF is that the y-axis is a density, not a probability. To get probability from a PDF we need to integrate the density over a given interval. This is something straightforward to do with a computer. But not that easy to do “visually”, human eyes/brains are not very good at that task. One way to alleviate this issue is to accompany a PDF with a point interval, like in the following plot.\n\npz.SkewNormal(38, 5, -2).plot_pdf(pointinterval=True);\n\n\n\n\n\n\n\nFigure 3.5: PDF of a SkewNormal distribution with a pointinterval\n\n\n\n\n\nThe point interval shows the quantiles of the distribution. Quantiles divide a dataset into equal probability intervals. For example, deciles divide a dataset into 10 equal-probability intervals, and quartiles divide a dataset into 4 equal-probability intervals. The most common quantile is the median (or 50th percentile), which divides a dataset into two equal-probability intervals where half of the data falls below the median and half of the data falls above the median.\nThe point interval in Figure 3.5 shows the 5th, 25th, 50th, 75th, and 95th percentiles. The point is the median. The tick line is the interquartile range (the central 50% of the distribution) and the thin line is the central 90% of the distribution.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random variables, distributions, and uncertainty</span>"
    ]
  },
  {
    "objectID": "Chapters/Distributions.html#cumulative-distribution-functions",
    "href": "Chapters/Distributions.html#cumulative-distribution-functions",
    "title": "3  Random variables, distributions, and uncertainty",
    "section": "3.3 Cumulative distribution functions",
    "text": "3.3 Cumulative distribution functions\nIn the previous section, we saw that we can use PMFs and PDFs to represent the probability distribution of a random variable. But there are other ways to represent a distribution. For example, we could use the cumulative distribution function (CDF).\nThe CDF is defined as the probability that the random variable takes a value less than or equal to \\(x\\). The CDF is defined for both discrete and continuous random variables. Figure 3.6 shows the CDF of a categorical distribution with three possible outcomes. Compare it with Figure 3.1\n\npz.Categorical([0.15, 0.6, 0.25]).plot_cdf();\n\n\n\n\n\n\n\nFigure 3.6: CDF of a Categorical distribution\n\n\n\n\n\nFigure 3.7 shows the CDF of a normal distribution (compare it with Figure 3.3).\n\npz.Normal(30, 4).plot_cdf();\n\n\n\n\n\n\n\nFigure 3.7: CDF of a normal distribution\n\n\n\n\n\nThe CDF is usually easier to read than the PDF, as we already saw y-axis for a PDF is a density that has no intrinsic meaning, and to get probability from a PDF we need to evaluate areas. Instead for a CDF the y-axis is a probability. For the PMF/PDF it is easier to get the mode (the highest value for the point/curve), and for the CDF it is easier to get the median (the value of \\(x\\) for which \\(y=0.5\\)), or other quantiles. From the CDF it is also easier to quickly get quantities like the probability of getting a temperature equal or lower than 35 degrees. It is the value of the CDF at 35. From Figure 3.7 we can see that it is roughly 0.9 or 90%, if you want more accuracy you could use a matplotlib/ArviZ style with a grid (like arviz-darkgrid) or use the cdf() function.\n\npz.Normal(30, 4).cdf(35)\n\n0.894350157794624\n\n\nFrom the CDF we can also easily get the probability of a range of values. For example, the probability of the temperature being between 25 and 35 degrees is the difference between the CDF at 35 and the CDF at 25. From Figure 3.7 we can get that it is roughly 0.9 or 90%. Again even when you can get a good estimate just by looking at the graph you can use the cdf() function to get a more accurate estimate. But the fact that you can get a good estimate by looking at the graph is a good feature.\n\nnp.diff(pz.Normal(30, 4).cdf([25, 35]))\n\narray([0.78870032])",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random variables, distributions, and uncertainty</span>"
    ]
  },
  {
    "objectID": "Chapters/Distributions.html#inverse-cumulative-distribution-functions",
    "href": "Chapters/Distributions.html#inverse-cumulative-distribution-functions",
    "title": "3  Random variables, distributions, and uncertainty",
    "section": "3.4 Inverse cumulative distribution functions",
    "text": "3.4 Inverse cumulative distribution functions\nSometimes we may want to use the inverse of the CDF. This is known as the quantile function or the percent point function (PPF). The PPF is also defined for both discrete and continuous random variables. For example, Figure 3.8 shows the PPF of a categorical distribution with three possible outcomes and Figure 3.9 shows the PPF of a normal distribution.\n\npz.Categorical([0.15, 0.6, 0.25]).plot_ppf();\n\n\n\n\n\n\n\nFigure 3.8: PPF of a Categorical distribution\n\n\n\n\n\n\npz.Normal(30, 4).plot_ppf();\n\n\n\n\n\n\n\nFigure 3.9: PPF of a normal distribution",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random variables, distributions, and uncertainty</span>"
    ]
  },
  {
    "objectID": "Chapters/Distributions.html#distributions-in-arviz",
    "href": "Chapters/Distributions.html#distributions-in-arviz",
    "title": "3  Random variables, distributions, and uncertainty",
    "section": "3.5 Distributions in ArviZ",
    "text": "3.5 Distributions in ArviZ\nThe PMF/PDF, CDF, and PPF are convenient ways to represent distributions for which we know the analytical form. But in practice, we often work with distributions that we don’t know their analytical form. Instead, we have a set of samples from the distribution. A clear example is a posterior distribution, computed using an MCMC method. For those cases, we still want useful visualization that we can use for ourselves or to show others. Some common methods are:\n\nHistograms\nKernel density estimation (KDE)\nEmpirical cumulative distribution function (ECDF)\nQuantile dot plots\n\nWe will discuss these methods in the next subsections with special emphasis on how they are implemented in ArviZ.\n\n3.5.1 Histograms\nHistograms are a very simple and effective way to represent a distribution. The basic idea is to divide the range of the data into a set of bins and count how many data points fall into each bin. Then we use as many bars as bins, with the height of the bars being proportional to the counts. The following video shows a step-by-step animation of a histogram being built.\nVideo\nHistograms can be used to represent both discrete and continuous random variables. Discrete variables are usually represented using integers.  Arguably the most important parameter of a histogram is the number of bins. Too few bins and we will miss details, too many and we will plot noise. You can pick the number of bins with a bit of trial and error, especially when you have good idea of what you want to show. However, there are many methods to compute the number of bins automatically from the data, like the Freedman–Diaconis rule or the Sturges’ rule. By default, ArviZ computes the number of bins using both rules and then picks the one that gives the largest number of bins. This is the same approach used by np.histogram(., bins=\"auto) and plt.hist(., bins=\"auto). Additionally, when the data is of type integers, ArviZ will preserve that structure and will associate bins to integers, instead of floats. If the number of unique integers is relatively small then, it will associate one bin to each integer. For example, in the following plot each bar is associated with an integer in the interval [0, 9].\n\nd_values = azb.convert_to_dataset(pz.Poisson(3).rvs((1, 500)))\nazp.plot_dist(d_values, kind=\"hist\");\n\n\n\n\n\n\n\nFigure 3.10: Histogram from a sample of integers. Each bin corresponds to a single integer.\n\n\n\n\n\nWhen the discrete values take higher values, like in Figure 3.11, bins are still associated with integers but many integers are binned together.\n\nd_values = azb.convert_to_dataset(pz.Poisson(100).rvs((1, 500)))\nazp.plot_dist(d_values, kind=\"hist\");\n\n\n\n\n\n\n\nFigure 3.11: Histogram from a sample of integers. Bins group together many integers.\n\n\n\n\n\nIf you don’t like the default binning criteria of ArviZ, you can change it by passing the bins argument using the hist_kwargs.\n\nd_values = azb.convert_to_dataset(pz.Poisson(100).rvs((1, 500)))\nazp.plot_dist(d_values, kind=\"hist\", stats_kwargs={\"density\":{\"bins\":20}})\n\n\n\n\n\n\n\nFigure 3.12: Histogram from a sample of integers, with bins automatically computed by Matplotlib, not ArviZ.\n\n\n\n\n\n\n\n3.5.2 KDE\nKernel density estimation (KDE) is a non-parametric way to estimate the probability density function from a sample. Intuitively you can think of it as the smooth version of a histogram. Conceptually you place a kernel function like a Gaussian on top of a data point, then you sum all the Gaussians, generally evaluated over a grid and not over the data points. Results are normalized so the total area under the curve is one. The following video shows a step-by-step animation of a KDE being built. You can see a version with border corrections and without them. Border corrections avoid adding a positive density outside the range of the data.\nVideo\nThe following block of code shows a very simple example of a KDE.\n\n_, ax = plt.subplots(figsize=(12, 4))\nbandwidth = 0.4\nnp.random.seed(19)\ndatapoints = 7\ny = np.random.normal(7, size=datapoints)\nx = np.linspace(y.min() - bandwidth * 3, y.max() + bandwidth * 3, 100)\nkernels = np.transpose([pz.Normal(i, bandwidth).pdf(x) for i in y])\nkernels *= 1/datapoints  # normalize the results\nax.plot(x, kernels, 'k--', alpha=0.5)\nax.plot(y, np.zeros(len(y)), 'C1o')\nax.plot(x, kernels.sum(1))\nax.set_xticks([])\nax.set_yticks([]);\n\n\n\n\n\n\n\n\nThe most important parameter of a KDE is the bandwidth which controls the degree of smoothness of the resulting curve. It is analogous to the number of bins for the histograms. ArviZ’s default method to compute the bandwidth works well for a wide range of distributions including multimodal ones. Compared to other KDEs in the Python ecosystem, the KDE implemented in ArviZ automatically handles the boundaries of a distribution. ArviZ will assign a density of zero to any point outside the range of the data.\nThe following example shows a KDE computed from a sample from a Gamma distribution. Notice that ArviZ computes a KDE instead of a histogram, and notice that there is no density for negative values.\n\nc_values = azb.convert_to_dataset(pz.Gamma(2, 3).rvs((1,1000)))\nazp.plot_dist(c_values);\n\n\n\n\n\n\n\nFigure 3.13: KDE from a sample of floats. By default, ArviZ computes a KDE instead of a histogram.\n\n\n\n\n\n\n\n3.5.3 ECDF\nBoth histograms and KDEs are ways to approximate the PMF/PDF of a distribution from a sample. But sometimes we may want to approximate the CDF instead. The empirical cumulative distribution function (ECDF) is a non-parametric way to estimate the CDF. It is a step function that jumps up by 1/N at each observed data point, where N is the total number of data points. The following video shows a step-by-step animation of an ECDF being built.\nVideo\nThe following block of code shows a very simple example of an ECDF.\n\nazp.plot_dist(c_values, kind=\"ecdf\");\n\n\n\n\n\n\n\nFigure 3.14: empirical cumulative distribution function\n\n\n\n\n\n\n\n3.5.4 Quantile dot plots\nA quantile dot plot displays the distribution of a sample in terms of its quantiles. Reading the median or other quantiles from quantile dot plots is generally easy, we just need to count the number of dots.\nThe following video shows a step-by-step animation of a quantile dot plot being built.\nVideo\nFrom Figure 3.15 we can easily see that 30% of the data is below 2. We do this by noticing that we have a total of 10 dots and 3 of them are below 2.\n\nd_values = azb.convert_to_dataset(pz.Poisson(3).rvs((1, 500)))\ntry:\n    azp.plot_dist(d_values, kind=\"dots\");\nexcept NotImplementedError:\n    pass\n\n\n\n\n\n\n\nFigure 3.15: Quantile dot plot\n\n\n\n\n\nThe number of quantiles (nquantiles) is something you will need to choose by yourself, usually, it is a good idea to keep this number relatively small and “round”, as the main feature of a quantile dot plot is that finding probability intervals reduces to counting dots. It is easier to count and compute proportion if you have 10, or 20 dots than if you have 11 or 57. But sometimes a larger number could be a good idea too, for instance, if you or your audience wants to focus on the tails of the distribution a larger number of dots will give you more resolution and you still will be counting only a rather small number dots so it will be easy to compute proportions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random variables, distributions, and uncertainty</span>"
    ]
  },
  {
    "objectID": "Chapters/MCMC_diagnostics.html",
    "href": "Chapters/MCMC_diagnostics.html",
    "title": "4  MCMC Diagnostics",
    "section": "",
    "text": "4.1 From the MCMC theory to practical diagnostics\nThe theory describes certain behaviors of MCMCs methods, many diagnoses are based on evaluating whether the theoretical results are empirically verified. For example, MCMC theory says that:\nWe are going to see that many diagnostics need multiple chains. Each chain is an independent MCMC run. The logic is that by comparing independent runs we can more easily sport issues than running a single instance. This multiple-chain approach also takes advantage of modern hardware. If you have a CPU with 4 cores you can get 4 independent chains in essentially the same time that one single chain.\nTo keep the focus on the diagnostics and not on any particular Bayesian model. We are going to first create 3 synthetic samples, we will use them to emulate samples from a posterior distribution.\nShow the code for more details\ngood_sample = pz.Gamma(2, 5).rvs((2, 2000), random_state=rng)\nbad_sample0 = pz.Normal(np.sort(good_sample, axis=None),\n                        0.05).rvs(4000, random_state=rng).reshape(2, -1)\n\nbad_sample1 = good_sample.copy()\nstuck = []\nfor i in pz.DiscreteUniform(0, 1900).rvs(3, random_state=rng):\n    stuck.append((i, i+100))\n    bad_sample1[i%2:,i:i+100] = pz.Beta(i, 150).rvs(100, random_state=rng)\n\nsample = azb.convert_to_dataset({\"good_sample\":good_sample,\n          \"bad_sample_0\":bad_sample0,\n          \"bad_sample_1\":bad_sample1})",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>MCMC Diagnostics</span>"
    ]
  },
  {
    "objectID": "Chapters/MCMC_diagnostics.html#from-the-mcmc-theory-to-practical-diagnostics",
    "href": "Chapters/MCMC_diagnostics.html#from-the-mcmc-theory-to-practical-diagnostics",
    "title": "4  MCMC Diagnostics",
    "section": "",
    "text": "The initial value is irrelevant, we must always arrive at the same result\nThe samples are not really independent, but the value of a point only depends on the previous point, there are no long-range correlations.\nIf we look at the sample as a sequence we should not be able to find any patterns.\n\nFor example, for a sufficiently long sample, the first portion must be indistinguishable from the last (and so should any other combination of regions).\n\nFor the same problem, each sample generated will be different from the others, but for practical purposes, the samples should be indistinguishable from each other.\n\n\n\n\ngood_sample: A random sample from a Gamma(2, 5). This is an example of a good sample because we are generating independent and identically distributed (iid) draws. This is the ideal scenario.\nbad_sample_0: We sorted good_sample, split it into two chains, and then added a small Gaussian error. This is a representation of a bad sample because values are not independent (we sorted the values!) and they do not come from the same distribution, because of the split the first half has values that are lower than the second. This represents a scenario where the sampler has very poor mixing.\nbad_sample_1: we start from good_chains, and turn into a poor sample by randomly introducing portions where consecutive samples are highly correlated to each other. This represents a common scenario, a sampler can resolve a region of the parameter space very well, but get stuck into one or more regions.\n\n\n\n\n\n\n\n\nNote\n\n\n\nAfter reading this chapter a good exercise is to come back here and modify these synthetic samples and run one or more diagnostics. If you want to make the exercise even more fun challenge yourself to predict what the diagnostics will be before running, or the other way around how you should change the samples to get a given result. This is a good test of your understanding and a good way to correct possible misunderstandings.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>MCMC Diagnostics</span>"
    ]
  },
  {
    "objectID": "Chapters/MCMC_diagnostics.html#trace-plots",
    "href": "Chapters/MCMC_diagnostics.html#trace-plots",
    "title": "4  MCMC Diagnostics",
    "section": "4.2 Trace plots",
    "text": "4.2 Trace plots\nA trace plot is created by drawing the sampled values at each iteration step. Ideally, we should expect to see a very noisy plot, some people call it a caterpillar. The reason is that draws should be uncorrelated from each other, the value of a draw should not provide any hint about the previous or next draw. Also, the draws from the first iterations should be indistinguishable from the ones coming from the last iterations the middle iterations, or any other region. The ideal scenario is the lack of any clear pattern as we can see at the right panel of Figure 4.1.\nIn ArviZ by calling the function az.plot_trace_dist(.) we get a trace plot on the right and on the left a KDE (for continuous variables) or a histogram (for discrete variables). Figure 4.1 is an example of this. The KDE/histogram can help to spot differences between chains, ideally, distributions should overlap.\n\nazp.plot_trace_dist(sample, var_names=\"good_sample\");\n\n\n\n\n\n\n\nFigure 4.1: Trace plot of a sample without issues\n\n\n\n\n\nThis represents a scenario where the sampler is visiting two different regions of the parameter space and is not able to jump from one to the other. Additionally, it is also moving very slowly within each region. Figure 4.3 shows two problems. On the one hand, each chain is visiting a different region of the parameter space. We can see this from the trace plot itself and the KDE. On the other hand, even within each region, the sampler is having trouble properly exploring the space, notice how it keeps moving up, instead of being stationary.\nIf you just want the trace without the density call az.plot_trace(.)\n\nazp.plot_trace(sample, var_names=\"good_sample\");\n\n\n\n\n\n\n\nFigure 4.2: Trace plot of a sample without issues\n\n\n\n\n\nLet’s see an example of two chains that did not converge.\n\n\nCode\nazp.plot_trace_dist(sample, var_names=\"bad_sample_0\");\n\n\n\n\n\n\n\n\nFigure 4.3: Trace plot of two chains that has not converged\n\n\n\n\n\nFinally from Figure 4.4, we can see another common scenario. This time we see that globally everything looks fine. But there are 3 regions where the sampler gets stuck, see the orange bands at the bottom of the traceplot.\n\n\nCode\npc = azp.plot_trace_dist(sample, var_names=\"bad_sample_1\")\nax = pc.viz[\"bad_sample_1\"][\"plot\"].sel(column=\"trace\").item()\nfor s in stuck:\n    ax.fill_between(s, 2.5, color=\"C2\", alpha=0.5);\n\n\n\n\n\n\n\n\nFigure 4.4: Trace plot showing a sampler being stuck in some regions (see orange lines)\n\n\n\n\n\nTrace plots are probably the first plots we make after inference and also probably the most popular plots in Bayesian literature. But there are more options, as we will see next.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>MCMC Diagnostics</span>"
    ]
  },
  {
    "objectID": "Chapters/MCMC_diagnostics.html#sec-rank-plots",
    "href": "Chapters/MCMC_diagnostics.html#sec-rank-plots",
    "title": "4  MCMC Diagnostics",
    "section": "4.3 Rank plots",
    "text": "4.3 Rank plots\nThe basic idea is the following. For a parameter we take all the chains and order the values from lowest to highest and assign them a rank, that is, to the lowest value assign 0, to the following 1, and so on until we reach the total number of samples, (number of chains multiplied by the number of draws per chain). Then we regroup the rankings according to the chains that gave rise to them and for each chain we make a histogram. If the chains were indistinguishable we would expect the histograms to be uniform. Since there is no reason for one chain to have more low (or medium or high) rankings than the rest.\nFigure 4.5 shows rank plots for good_sample, bad_sample_0 and bad_sample_1. Notice how good_sample looks pretty close to uniform, bad_sample_1 is harder to tell, because overall it looks very close to uniform, except for the penultimate bar. On the contrary bad_sample_0 really looks bad.\n\n\nCode\n_, ax = plt.subplots(3, 1, figsize=(10, 8))\naz.plot_rank(sample, ax=ax);\n\n\n\n\n\n\n\n\nFigure 4.5: Rank plot for good_sample, bad_sample_0 and bad_sample_1",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>MCMC Diagnostics</span>"
    ]
  },
  {
    "objectID": "Chapters/MCMC_diagnostics.html#hat-r-r-hat",
    "href": "Chapters/MCMC_diagnostics.html#hat-r-r-hat",
    "title": "4  MCMC Diagnostics",
    "section": "4.4 \\(\\hat R\\) (R-hat)",
    "text": "4.4 \\(\\hat R\\) (R-hat)\nPlots are often useful for discovering patterns, but sometimes we want numbers, for example when quickly evaluating many parameters it may be easier to look at numbers than plots. Number are also easier to plug into some automatic routine, that call for human attention only if some threshold is exceeded.\n\\(\\hat R\\) is a numerical diagnostic that answers the question (Vehtari et al. 2021). Did the chains mix properly? But I also like to think of it as the score assigned by a jury in a trace (or rank) plot contest.\nThe version implemented in ArviZ does several things under the hood, but the central idea is that it compares the variance between chains with the variance within each chain. Ideally, we should get \\(\\hat R = 1\\), in practice \\(\\hat R \\lessapprox 1.01\\) are considered safe and in the first modeling phases, even higher values like \\(\\hat R \\approx 1.1\\) may be fine.\nUsing ArviZ we can get the \\(\\hat R\\) usando az.rhat(⋅), az.summary(⋅) and az.plot_forest(⋅, r_hat=True)\n\naz.rhat(sample)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 24B\nDimensions:       ()\nData variables:\n    good_sample   float64 8B 1.0\n    bad_sample_0  float64 8B 2.635\n    bad_sample_1  float64 8B 1.034xarray.DatasetDimensions:Coordinates: (0)Data variables: (3)good_sample()float641.0array(1.00025509)bad_sample_0()float642.635array(2.63541676)bad_sample_1()float641.034array(1.0343114)Indexes: (0)Attributes: (0)\n\n\n\n4.4.1 Effective Sample Size (ESS)\nSince the samples of an MCMC are (auto)correlated, the amount of “useful” information is less than a sample of the same size but iid. Figure 4.6 can help us develop intuition. In this figure, we analyze the error incurred while computing an estimate (such as the mean) from samples of different size, considering varying degrees of autocorrelation. The results represent averages from 1000 repetitions.\nWe can see that the error goes down as the sample size increases and we can also see that the lower the autocorrelation the smaller the sample size to achieve an estimate with a given error. In other words the higher the autocorrelation the larger the number of sample we will need to achieve the a given precision.\n\n\nShow the code for more details\ndef generate_autocorrelated_sample(original, rho):\n    \"\"\"\n    Generates an autocorrelated sample from original.\n\n    Parameters:\n    ----------\n    sample: numpy array, \n        The original sample\n    rho: float,\n        Desired autocorrelation value\n\n    Returns:\n    --------\n    new_sample: numpy array, autocorrelated sample\n    \"\"\"\n    n = len(original)\n    y = np.copy(original)\n    mean = np.mean(original)\n    for i in range(1, n):\n        y[i] += rho * (y[i-1]-mean) + np.random.randn()\n\n    return y\n\nmean = 0\nlag = 30\nsize = 300\n\niid_samples = pz.Normal(mean, 1).rvs((1000, size))\n\nrhos = np.linspace(0, 0.90, 7)\nN = len(rhos)\n\nfig, ax = plt.subplots()\n\nfor k, rho in enumerate(rhos):\n    auto_samples = np.stack([generate_autocorrelated_sample(iid_sample, rho) for iid_sample in iid_samples])\n    auto_error = []\n    for i in range(1, size):\n        auto_error.append(np.mean(((np.mean(auto_samples[:,:i] - mean, 1)**2)**0.5)))\n\n    ax.plot(auto_error[lag:], color=plt.cm.viridis_r(k/N))\n\nsm = plt.cm.ScalarMappable(cmap=plt.cm.viridis_r)\ncbar = plt.colorbar(sm, ax=ax, label='Autocorrelation', ticks=[0, 1])\ncbar.ax.set_yticklabels(['Low', 'High'])\ncbar.ax.tick_params(length=0)\n\nax.set_yticks([])\nax.set_ylabel(\"Error\")\nax.set_xticks([])\nax.set_xlabel(\"Sample size\")\n\nax.set_ylim(bottom=0)\nax.set_xlim(-2)\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\nax.plot(1, 0, \"&gt;k\", transform=ax.get_yaxis_transform(), clip_on=False)\nax.plot(-2, 1, \"^k\", transform=ax.get_xaxis_transform(), clip_on=False)\n\n\n\n\n\n\n\n\nFigure 4.6: Average error as a function of the sample size for different autocorrelation values.\n\n\n\n\n\nAs for MCMC samples, the sample size can be misleading, we instead estimate the effective sample size (ESS) (Vehtari et al. 2021), that is, the size of a sample with the equivalent amount of information but without autocorrelation. Figure 4.7 shows how when the sample size increases the ESS tends to increase too, and more importantly, it shows that the slope is higher for lower autocorrelation values.\n\n\nShow the code for more details\nmean = 0\nsize = 300\n\niid_samples =  pz.Normal(mean, 1).rvs((500, size))\n\nrhos = np.linspace(0, 0.90, 7)\nN = len(rhos)\n\nfig, ax = plt.subplots()\n\nfor k, rho in enumerate(rhos):\n    auto_samples = np.stack([generate_autocorrelated_sample(iid_sample, rho) for iid_sample in iid_samples])\n    auto_error = []\n\n    for i in range(50, size, 10):\n        auto_error.append(az.stats.ess(auto_samples[:,:i])/500)\n\n    ax.plot(range(50, size, 10), auto_error, color=plt.cm.viridis_r(k/N))\n\nsm = plt.cm.ScalarMappable(cmap=plt.cm.viridis_r)\ncbar = plt.colorbar(sm, ax=ax, label='Autocorrelation', ticks=[0, 1])\ncbar.ax.set_yticklabels(['Low', 'High'])\ncbar.ax.tick_params(length=0)\n\nax.set_yticks([])\nax.set_ylabel(\"Effective sample size\")\nax.set_xticks([])\nax.set_xlabel(\"Sample size\")\n\nax.set_ylim(0)\nax.set_xlim(48)\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\nax.plot(1, 0, \"&gt;k\", transform=ax.get_yaxis_transform(), clip_on=False)\nax.plot(48, 1, \"^k\", transform=ax.get_xaxis_transform(), clip_on=False)\n\n\n\n\n\n\n\n\nFigure 4.7: Effective sample size as a function of the sample size for different autocorrelation values.\n\n\n\n\n\nWith ArviZ we can get az.ess(⋅), az.summary(⋅) and az.plot_forest(⋅, ess=True)\n\naz.ess(sample)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 24B\nDimensions:       ()\nData variables:\n    good_sample   float64 8B 4.171e+03\n    bad_sample_0  float64 8B 2.354\n    bad_sample_1  float64 8B 163.8xarray.DatasetDimensions:Coordinates: (0)Data variables: (3)good_sample()float644.171e+03array(4170.96051113)bad_sample_0()float642.354array(2.35401828)bad_sample_1()float64163.8array(163.83284461)Indexes: (0)Attributes: (0)\n\n\nOne way to use the ESS is as a minimum requirement for trustworthy MCMC samples. It is recommended the ESS to be greater than 100 per chain. That is, for 4 chains we want a minimum of 400 effective samples.\n\n\n\n\n\n\nNote\n\n\n\nThe ESS can also be used as a metric of the efficiency of MCMC sampling methods. For instance, we may want to measure the ESS per sample (ESS/n), a sampler that generates a ESS/n closer to 1 is more efficient than a sampler that generates values closer to 0. Other common metrics are the ESS per second, and the ESS per likelihood evaluation.\n\n\nWe see that az.summary(⋅) returns two ESS values, ess_bulk and ess_tail. This is because different regions of the parameter space may have different ESS values since not all regions are sampled with the same efficiency. Intuitively, one may think that when sampling a distribution like a Gaussian it is easier to obtain better sample quality around the mean than around the tails, simply because we have more samples from that region. For some models, it could be the other way around, but the take-home message remains, not all regions are necessarily sampled with the same efficiency\n\naz.summary(sample, kind=\"diagnostics\")\n\n\n\n\n\n\n\n\n\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\ngood_sample\n0.004\n0.003\n4171.0\n4047.0\n1.00\n\n\nbad_sample_0\n0.182\n0.148\n2.0\n11.0\n2.64\n\n\nbad_sample_1\n0.028\n0.020\n164.0\n3480.0\n1.03\n\n\n\n\n\n\n\n\nIf we are going to use the MCMC samples to calculate central values such as means or medians then we have to make sure that the ess_bulk is sufficiently large, however, if we want to calculate intervals such as an HDI 94% we have to make sure that ess_tail be appropriate.\nArviZ offers several functions linked to the ESS. For example, if we want to evaluate the performance of the sampler for several regions at the same time we can use az.plot_ess.\n\nazp.plot_ess(sample,\n            pc_kwargs={\"plot_grid_kws\": {\"figsize\": (10, 8)},\n                       \"col_wrap\": 1})\n\n\n\n\n\n\n\n\nA simple way to increase the ESS is to increase the number of samples, but it could be the case that the ESS grows very slowly with the number of samples, so even if we increased the number of samples 10 times we could still be very far from our target value. One way to estimate “how many more samples do we need” is to use az.plot_ess(⋅, kind=\"evolution\"). This graph shows us how the ESS changed with each iteration, which allows us to make predictions.\nFrom Figure 4.8 we can see that the ESS grows linearly with the number of samples for good_sample, and it does not grow at all for bad_sample_0. In the latter case, this is an indication that there is virtually no hope of improving the ESS simply by increasing the number of draws.\n\nazp.plot_ess_evolution(sample,\n            var_names=[\"good_sample\", \"bad_sample_0\"],\n            pc_kwargs={\"col_wrap\":1},\n            );\n\n\n\n\n\n\n\nFigure 4.8: ESS evolution plot for good_sample and bad_sample_0.\n\n\n\n\n\n\n\n4.4.2 Monte Carlo standard error (MCSE)\nAn advantage of the ESS is that it is scale-free, it does not matter if one parameter varies between 0.1 and 0.2 and another between -2000 and 0, an ESS of 400 has the same meaning for both parameters. In models with many parameters, we can quickly identify which parameters are most problematic. However, when reporting results it is not very informative to know whether the ESS was 1372 or 1501. Instead, we would like to know the order of the errors we are making when approximating the posterior. This information is given by the Monte Carlo standard error (MCSE). Like the ESS, the MCSE takes into account the autocorrelation of the samples. This error should be below the desired precision in our results. That is, if for a parameter the MCSE is 0.1, it does not make sense to report that the mean of that parameter is 3.15. Since the correct value could easily be between 3.4 and 2.8.\nWith ArviZ we can get the MCSE with az.mcse(⋅) or az.summary(⋅).\n\naz.mcse(sample)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 24B\nDimensions:       ()\nData variables:\n    good_sample   float64 8B 0.004354\n    bad_sample_0  float64 8B 0.1816\n    bad_sample_1  float64 8B 0.02821xarray.DatasetDimensions:Coordinates: (0)Data variables: (3)good_sample()float640.004354array(0.00435365)bad_sample_0()float640.1816array(0.1816053)bad_sample_1()float640.02821array(0.02820501)Indexes: (0)Attributes: (0)\n\n\n\n\n4.4.3 Thinning\nOne way to reduce autocorrelation in an MCMC chain is through thinning, where we retain only every \\(n\\)-th sample. While this method is straightforward, it has the drawback of discarding useful information. Research generally suggests that it’s better to keep all the samples when calculating estimates (MacEachern and Berliner 1994; Link and Eaton 2012). Provided the variance is finite, the central limit theorem applies even to correlated samples. Then if higher accuracy is needed, it’s more effective to increase the number of draws rather than to perform thinning. Still, there are situations where thinning might be useful, such as:\n\nReducing the size of stored data, which is especially important when dealing with a large number of models or when the postprocessing of the samples is expensive, for instance when we need to run expensive computations on every draw.\nAddressing bias in extreme ordered statistics, which may affect diagnostics like rank-plots (see Section 4.3) and uniformity tests typically done for posterior predictive checks, as shown in Figure 5.7 and Figure 6.2 or as part of Simulation Based Calibration (Talts et al. 2020).\n\nTo determine an appropriate thinning factor, we can use the effective sample size (ESS). For instance, if you have 2,000 samples and an ESS of 1,000, you would thin by a factor of 2, keeping every other sample. The higher the ESS, the lower the thinning factor required. A more refined approach is to calculate both ESS-tail and ESS-bulk, then use the smaller value, which better accounts for differences in sampling efficiency between the central 90% quantile and the 5% tail quantiles (Säilynoja, Bürkner, and Vehtari 2022).\nIn ArviZ we have the thin function, which allows us to perform thinning automatically.\n\n# Coming soon with ArviZ 1.0\n#azs.thin(sample)\n\nAdditionally, if needed, we can specify the thinning factor manually or we can pass a target_ess, this last option is useful when we want as much thinning as possible provided we still get an ESS around the specified value of target_ess.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>MCMC Diagnostics</span>"
    ]
  },
  {
    "objectID": "Chapters/MCMC_diagnostics.html#diagnosis-of-gradient-based-algorithms",
    "href": "Chapters/MCMC_diagnostics.html#diagnosis-of-gradient-based-algorithms",
    "title": "4  MCMC Diagnostics",
    "section": "4.5 Diagnosis of gradient-based algorithms",
    "text": "4.5 Diagnosis of gradient-based algorithms\nDue to its internal workings, algorithms like NUTS offer some specific tests that are not available to other methods. These tests are generally very sensitive.\nTo exemplify this we are going to load two InferenceData from pre-calculated models. The details of how these data were generated are not relevant at the moment. We will only say that they are two models that are mathematically equivalent but parameterized in different ways. In this case, the parameterization affects the efficiency of the sampler. The centered model is sampled more efficiently than the uncentered model.\n\nidata_cm = az.load_arviz_data(\"centered_eight\")\nidata_ncm = az.load_arviz_data(\"non_centered_eight\")\n\n\n4.5.1 Transition energy vs marginal energy\nWe can think of a Hamiltonian Monte Carlo as a two-step process\n\nDeterministic sampling (following the Hamiltonian)\nA random walk in momentum space\n\nIf the transition energy distribution is similar to the marginal energy distribution, then NUTS can generate samples of the marginal energy distribution that are almost independent between transitions. We can evaluate this visually or numerically, calculating the Bayesian Fraction of Missing Information (BFMI), as shown in the following figure.\n\n_, axes = plt.subplots(1, 2, sharey=True, sharex=True, figsize=(12, 4), constrained_layout=True)\n\nfor ax, idata, nombre in zip(axes.ravel(), (idata_cm, idata_ncm), (\"centered\", \"non-centered\")):\n    az.plot_energy(idata, ax=ax)\n    ax.set_title(nombre)\n\n\n\n\n\n\n\n\n\n\n4.5.2 Divergences\nOne advantage of NUTS is that it fails with style. This happens, for example, when trying to go from regions of low curvature to regions of high curvature. In these cases, the numerical trajectories may diverge. Essentially this happens because in these cases there is no single set of hyper-parameters that allows efficient sampling of both regions. So one region is sampled properly and when the sampler moves to the other region it fails. Divergent numerical trajectories are extremely sensitive identifiers of pathological neighborhoods.\nThe following example shows two things the non-centered model shows several divergences (turquoise circles) grouped in one region. In the centered model, which has no divergence, you can see that around that same region, there are samples for smaller values of tau. That is to say, the ‘uncentered’ model fails to sample a region, but at least it warns that it is having problems sampling that region!\n\n_, axes = plt.subplots(1, 2, sharey=True, sharex=True, figsize=(10, 5), constrained_layout=True)\n\n\nfor ax, idata, nombre in zip(axes.ravel(), (idata_cm, idata_ncm), (\"centered\", \"non-centered\")):\n    az.plot_pair(idata, var_names=['theta', 'tau'], coords={'school':\"Choate\"}, kind='scatter',\n                 divergences=True, divergences_kwargs={'color':'C1'},\n                 ax=ax)\n    ax.set_title(nombre)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>MCMC Diagnostics</span>"
    ]
  },
  {
    "objectID": "Chapters/MCMC_diagnostics.html#what-to-do-when-the-diagnoses-are-wrong",
    "href": "Chapters/MCMC_diagnostics.html#what-to-do-when-the-diagnoses-are-wrong",
    "title": "4  MCMC Diagnostics",
    "section": "4.6 What to do when the diagnoses are wrong?",
    "text": "4.6 What to do when the diagnoses are wrong?\n\nMore samples or more tuning steps. This is usually only useful when the problems are minor\nBurn-in. Modern software like PyMC uses several samples to tune the hyper-parameters of the sampling methods. By default, these samples are eliminated, so in general, it is not necessary to do Burn-in manually.\nChange sampling method!\nReparameterize the model\nImprove priors\n\nThe folk theorem of computational statistics: When you have computational problems, there is often a problem with your model. The recommendation is NOT to change the priors to improve sampling quality. The recommendation is that if the sampling is bad, perhaps the model is too. In that case, we can think about improving the model, one way to improve it is to use prior knowledge to improve the priors.\n\nSome models can be expressed in more than one way, all mathematically equivalent. In those cases, some parameterizations may be more efficient than others. For example, as we will see later with hierarchical linear models.\nIn the case of divergences, these are usually eliminated by increasing the acceptance rate, for instance in PyMC you can do pm.sample(..., target_accept=x) where x is 0.8 by default and the maximum value is 1. If you reach 0.99 you should probably do something else.\nModern probabilistic programming languages, usually provide useful warning messages and tips if they detect issues with sampling, paying attention to those messages can save you a lot of time.\n\n\n\n\n\nLink, William A., and Mitchell J. Eaton. 2012. “On Thinning of Chains in MCMC.” Methods in Ecology and Evolution 3 (1): 112–15. https://doi.org/https://doi.org/10.1111/j.2041-210X.2011.00131.x.\n\n\nMacEachern, Steven N., and L. Mark Berliner. 1994. “Subsampling the Gibbs Sampler.” The American Statistician 48 (3): 188–90. https://doi.org/10.2307/2684714.\n\n\nSäilynoja, Teemu, Paul-Christian Bürkner, and Aki Vehtari. 2022. “Graphical Test for Discrete Uniformity and Its Applications in Goodness-of-Fit Evaluation and Multiple Sample Comparison.” Statistics and Computing 32 (2): 32. https://doi.org/10.1007/s11222-022-10090-6.\n\n\nTalts, Sean, Michael Betancourt, Daniel Simpson, Aki Vehtari, and Andrew Gelman. 2020. “Validating Bayesian Inference Algorithms with Simulation-Based Calibration.” https://arxiv.org/abs/1804.06788.\n\n\nVehtari, Aki, Andrew Gelman, Daniel Simpson, Bob Carpenter, and Paul-Christian Bürkner. 2021. “Rank-Normalization, Folding, and Localization: An Improved \\(\\widehat{R}\\) for Assessing Convergence of MCMC (with Discussion).” Bayesian Analysis 16 (2): 667–718. https://doi.org/10.1214/20-BA1221.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>MCMC Diagnostics</span>"
    ]
  },
  {
    "objectID": "Chapters/Model_criticism.html",
    "href": "Chapters/Model_criticism.html",
    "title": "5  Model Criticism",
    "section": "",
    "text": "5.1 Prior predictive checks\nThe idea behind prior predictive checks is very general and simple: if a model is good it should be able generate data resembling our prior knowledge. We call these checks, prior predictive because we are generating synthetic data before we have seen the actual data.\nThe general algorithm for prior predictive checks is:\nNotice that in step 4 we use domain knowledge, NOT observed data!\nIn steps 1 and 2 what we are doing is approximating this integral: \\[\np(y^\\ast) = \\int_{\\Theta} p(y^\\ast \\mid \\theta) \\; p(\\theta) \\; d\\theta\n\\]\nwhere \\(y^\\ast\\) represents unobserved but potentially observable data. Notice that to compute \\(y^\\ast\\) we are evaluating the likelihood over all possible values ​​of the prior. Thus we are effectively marginalizing out the values of \\(\\theta\\), the parameters.\nTo exemplify a prior predictive check, let’s try with a super simple example. Let’s say we want to model the height of humans. We know that the heights are positive numbers, so we should use a distribution that assigns zero mass to negative values. But we also know that at least for adults using a normal distribution could be a good approximation. So we create the following model, without too much thought, and then draw 500 samples from the prior predictive distribution.\nIn the following plot, we can see samples from the prior predictive distribution (blue solid lines). If we aggregate all the individual samples into a single large sample we get the dashed cyan line. To help us interpret this plot we have added two reference values, the average length/height of a newborn and the average (male) adult height. These reference values, are values that are meaningful to the problem at hand that we obtain from domain-knowledge (not from the observed data). We used reference values to get a sense of the scale of the expected data. Expected or typical values could also be used.\nWe can see that our model is bananas, not only heights can be negative, but the bulk of the prior predictive distribution is outside of our reference values.\nax = az.plot_ppc(idata, group=\"prior\")\nax.axvline(50, color=\"0.5\", linestyle=\":\", label=\"newborn\")\nax.axvline(175, color=\"0.5\", linestyle=\"--\", label=\"adult\")\nplt.legend()\n\n\n\n\n\n\n\nFigure 5.1: The prior predictive check for the model of heights. We can see that the bulk of the samples are outside the reference values.\nWe can tighten up the priors. There is no general rule to do this. For most problems, it is usually a good idea to set priors using some broad domain knowledge in such a way that we get a prior predictive distribution that allocates most of its mass in a reasonable region. These priors are called weakly informative priors. While there isn’t a formal definition of what a weakly informative prior is, we can think of them as priors that generate a prior predictive distribution with none to very little mass in disallowed or unlikely regions. For instance, we can use a normal distribution with a mean of 175 and a standard deviation of 10. This distribution doesn’t exclude negative values, but it assigns very little mass to them. Also is broad enough to allow for a wide range of values.\nWe repeat the previous plot with the new prior predictive distribution. We can see that the bulk of the prior predictive distribution is within the reference values. The model predicts values above 200 cm and below 150 cm, which are indeed possible but are less likely. You are free to pick other priors and other reference values. Maybe you can use the historical record for the taller and shorter persons in the world as33 reference values.\nax = az.plot_ppc(idata, group=\"prior\")\nax.axvline(50, color=\"0.5\", linestyle=\":\", label=\"newborn\")\nax.axvline(175, color=\"0.5\", linestyle=\"--\", label=\"adult\")\nplt.legend()\n\n\n\n\n\n\n\nFigure 5.2: The prior predictive check for the model of heights with a more narrower prior than Figure 5.1. Predictions are closer to our domain knowledge about human heights.\nWeakly informative priors can have practical advantages over very vague priors because adding some prior information is usually better than adding none. By adding some information we reduce the chances of spurious results or nonsensical results. Additionally, weakly informative priors can provide computational advantages, like helping with faster sampling.\nWeakly informative priors can also have a practical advantage over informative priors. To be clear, if you have trustworthy informative priors you should use them, it doesn’t make sense to ignore information that you have. But in many setting informative priors can be hard to set, and they can be very time-consuming to get. That’s why in practice weakly informative priors can be a good compromise.\nax = az.plot_ppc(idata, group=\"prior\", kind=\"cumulative\")\nax.axvline(50, color=\"0.5\", linestyle=\":\", lw=3, label=\"newborn\")\nax.axvline(175, color=\"0.5\", linestyle=\"--\", lw=3, label=\"adult\")\nplt.legend()\n\n\n\n\n\n\n\nFigure 5.3: The prior predictive check for the model of heights. Same as Figure 5.2 but using empirical CDFs instead of KDEs.\nWhen plotting many distributions, where each one spans a narrow range of values compared to the range spanned but the collection of distributions, it is usually a good idea to plot the cumulative distribution, like in the previous figure. The cumulative distribution is easier to interpret in these cases compared to plots that approximate the densities, like KDEs, histograms, or quantile dot plots.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Criticism</span>"
    ]
  },
  {
    "objectID": "Chapters/Model_criticism.html#prior-predictive-checks",
    "href": "Chapters/Model_criticism.html#prior-predictive-checks",
    "title": "5  Model Criticism",
    "section": "",
    "text": "Draw \\(N\\) realizations from a prior distribution.\nFor each draw, simulate new data from the likelihood.\nPlot the results.\nUse domain knowledge to assess whether simulated values reflect prior knowledge.\nIf simulated values do not reflect prior knowledge, change the prior distribution, likelihood, or both and repeat the simulation from step 1.\nIf simulated values reflect prior knowledge, compute the posterior.\n\n\n\n\n\n\nPyMCPyStan\n\n\n\nwith pm.Model() as model: \n    # Priors for unknown model parameters\n    mu = pm.Normal('mu', mu=0, sigma=10)\n    sigma = pm.HalfNormal('sigma', sigma=10)\n    # Likelihood (sampling distribution) of observations\n    y_obs = pm.Normal('Y_obs', mu=mu, sigma=sigma, observed=y)\n    # draw 500 samples from the prior predictive\n    idata = pm.sample_prior_predictive(samples=500, random_seed=SEED)\n\nSampling: [Y_obs, mu, sigma]\n\n\n\n\n## coming soon\n\n\n\n\n\n\n\n\nPyMCPyStan\n\n\n\nwith pm.Model() as model: \n    # Priors for unknown model parameters\n    mu = pm.Normal('mu', mu=175, sigma=10)\n    sigma = pm.HalfNormal('sigma', sigma=10)\n    # Likelihood (sampling distribution) of observations\n    y_obs = pm.Normal('Y_obs', mu=mu, sigma=sigma, observed=y)\n    # draw 500 samples from the prior predictive\n    idata = pm.sample_prior_predictive(samples=500, random_seed=SEED)\n\nSampling: [Y_obs, mu, sigma]\n\n\n\n\n## coming soon\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nOne aspect that is often overlooked is that even if the priors have little effect on the actual posterior, by doing prior predictive checks and playing with the priors we can get a better understanding of our models and problems. If you want to learn more about prior elicitation you can check the PreliZ library.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Criticism</span>"
    ]
  },
  {
    "objectID": "Chapters/Model_criticism.html#posterior-predictive-checks",
    "href": "Chapters/Model_criticism.html#posterior-predictive-checks",
    "title": "5  Model Criticism",
    "section": "5.2 Posterior predictive checks",
    "text": "5.2 Posterior predictive checks\nThe idea behind posterior predictive checks is very general and simple: if a model is good it should be able generate data resembling the observed data. We call these checks, posterior predictive because we are generating synthetic data after seeing the data.\nThe general algorithm for posterior predictive checks is:\n\nDraw \\(N\\) realizations from the posterior distribution.\nFor each draw, simulate new data from the likelihood.\nPlot the results.\nUse observed data to assess whether simulated values agree with observed values.\nIf simulated values do not agree with observations, change the prior distribution, likelihood, or both and repeat the simulation from step 1.\nIf simulated values reflect prior knowledge, compute the posterior.\n\nNotice that in contrast with prior predictive checks, we use observations here. Of course, we can also include domain knowledge to assess whether the simulated values are reasonable, but because we are using observations we do more stringent evaluations.\nIn steps 1 and 2 what we are doing is approximating this integral: \\[\np(\\tilde y) = \\int_{\\Theta} p(\\tilde y \\mid \\theta) \\; p(\\theta \\mid y) \\; d\\theta\n\\]\nwhere \\(\\tilde y\\) represents new observations, according to our model. The data generated is predictive since it is the data that the model expects to see.\nNotice that what we are doing is marginalizing the likelihood by integrating all possible values ​​of the posterior. Therefore, from the perspective of our model, we are describing the marginal distribution of data, that is, regardless of the values of the parameters.\nContinuing with our height example, we can generate synthetic data from the posterior predictive distribution.\n\nPyMCPyStan\n\n\n\nwith model: \n    idata = pm.sample()\n    pm.sample_posterior_predictive(idata, random_seed=SEED, extend_inferencedata=True)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [mu, sigma]\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 1 seconds.\nWe recommend running at least 4 chains for robust computation of convergence diagnostics\nSampling: [Y_obs]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## coming soon\n\n\n\nAnd then we use ArviZ to plot the comparison. We can see that the model is doing a good job at predicting the data. The observed data (black line) is within the bulk of the posterior predictive distribution (blue lines).\nThe dashed orange line, labelled as “posterior predictive mean”, is the aggregated posterior predictive distribution. If you combine all the individual densities into a single density, that’s what you would get.\n\naz.plot_ppc(idata, num_pp_samples=200, colors=[\"C0\", \"k\", \"C2\"]);\n\n\n\n\n\n\n\nFigure 5.4: Posterior predictive check for the model of heights.\n\n\n\n\n\nOther common visualizations to compare observed and predictive values are empirical CDFs, histograms and less often quantile dotplots. Like with other types of visualizations, you may want to try different options, to be sure visualizations are not misleading and you may also want to adapt the visualization to your audience.\n\n5.2.1 Using summary statistics\nBesides directly comparing observations and predictions in terms of their densities, we can do comparisons in terms of summary statistics. Which ones, we decide to use may vary from one data-analysis problem to another, and ideally it should be informed by the data-analysis goals.\nThe following plot shows a comparison in terms of the mean, median and interquartile range (IQR). The dots at the bottom of each subplots corresponds to the summary statistics computed for the observed data and the KDE is for the model’s predictions.\n\n_, ax = plt.subplots(1, 3, figsize=(12, 3))\n\ndef iqr(x, a=-1):\n    \"\"\"interquartile range\"\"\"\n    return np.subtract(*np.percentile(x, [75, 25], axis=a))\n\naz.plot_bpv(idata, kind=\"t_stat\", t_stat=\"mean\", ax=ax[0])\naz.plot_bpv(idata, kind=\"t_stat\", t_stat=\"median\", ax=ax[1])\naz.plot_bpv(idata, kind=\"t_stat\", t_stat=iqr, ax=ax[2])\nax[0].set_title(\"mean\")\nax[1].set_title(\"median\")\nax[2].set_title(\"IQR\");\n\n\n\n\n\n\n\nFigure 5.5: Posterior predictive check for the model of heights using summary statistics.\n\n\n\n\n\nThe numerical values labeled as bpv correspond to the following probability:\n\\[\np(T_{\\text{sim}} \\le T_{\\text{obs}} \\mid \\tilde y)\n\\]\nWhere \\(T\\) is the summary statistic of our choice, computed for both the observed data \\(T_{\\text{obs}}\\) and the simulated data \\(T_{\\text{sim}}\\).\nThis probabilities are often called Bayesian p-values, and their ideal value is 0.5, meaning that half of the predictions are below the observed values and half above.\nThe following paragraph is for those of you that are familiar with the p-values as used in frequentist statistics for null hypothesis testing and computation of “statistical significance”. If that’s the case you should be aware that the Bayesian p-values are something different. While frequentist p-values measure the probability of obtaining results at least as extreme as the observed data under the assumption that the null hypothesis is true, Bayesian p-values assess the discrepancy between the observed data and simulated data based on the posterior predictive distribution. They are used as a diagnostic tool to evaluate the adequacy of a model rather than as a measure of “statistical significance” or evidence against a null hypothesis.\nBayesian p-values are not tied to a null hypothesis and are not subject to the same kind of binary decision-making framework (reject/accept). Instead, they provide insight into how well the model, given the data, predicts the summary statistics. Values close to 0.5 indicates a well-calibrated model. However, the interpretation is more nuanced, as the goal is model assessment and improvement rather than hypothesis testing.\nIf you want to use plot_bpv to do the plots, but you prefer to omit the Bayesian p-values pass bpv=False. plot_bpv supports to other types of plots kind=\"p_value\", that corresponds to computing\n\\[\np(\\tilde y \\le y_{\\text{obs}} \\mid y)\n\\]\nthat is, instead of using a summary statistics, as before, we directly compare observations and predictions. In the following plots the dashed black line represent the expected distribution.\n\naz.plot_bpv(idata, kind=\"p_value\");\n\n\n\n\n\n\n\nFigure 5.6: Posterior predictive check for the model of heights using Bayesian p-values.\n\n\n\n\n\nAnother possibility is to perform the comparison per observation.\n\\[\np(\\tilde y_i \\le y_i \\mid y)\n\\]\nThis is often called the marginal p-value and the ideal distribution is the standard uniform distribution.\nThe white line in the following figure represents the ideal scenario and the grey band the expected deviation given the size of the data. The x values corresponds to the quantiles of the original distribution, i.e. the central values represent the “bulk” of the distribution and the extreme values the “tails”.\n\naz.plot_bpv(idata);\n\n\n\n\n\n\n\nFigure 5.7: Posterior predictive check for the model of heights using marginal Bayesian p-values, also know as u-values.\n\n\n\n\n\nTo build intuition, let’s look at some synthetic data. The following plot shows four different scenarios, where the observed data follows a standard normal distribution (\\(\\mu=0, \\sigma^2=1\\)). In each case, we compare the observed data to predictions where:\n\nThe mean of the predictions is shifted to the right.\nThe mean of the predictions is shifted to the left.\nThe predictions have a wider spread (larger variance).\nThe predictions have a narrower spread (smaller variance).\n\n\nobserved = pz.Normal(0, 1).rvs(500)\n\n_, axes = plt.subplots(2, 2, sharex=True, sharey=True)\n\nfor ax, (mu, sigma) in zip(axes.ravel(), \n                           [(0.5, 1),   # shifted right\n                            (-0.5, 1),  # shifted left\n                            (0, 2),     # wider\n                            (0, 0.5)]): # narrower\n    \n    idata_i = az.from_dict(posterior_predictive={\"y\":pz.Normal(mu, sigma).rvs((50, 500))},\n                           observed_data={\"y\":observed})\n    \n    az.plot_bpv(idata_i, ax=ax);\n    ax.set_ylim(0, 2.5)\n\n\n\n\n\n\n\nFigure 5.8: Posterior predictive check for the model of heights using u-values and showing for alternative scenarios.\n\n\n\n\n\n\n\n5.2.2 Hypothetical Outcome Plots\nAnother strategy that can be useful for posterior predictive plots is to use animations. Rather than showing a continuous probability distribution, Hypothetical Outcome Plots (HOPs) visualize a set of draws from a distribution, where each draw is shown as a new plot in either a small multiples or animated form. HOPs enable a user to experience uncertainty in terms of countable events, just like we experience probability in our day to day lives.\naz.plot_ppc support animations using the option animation=True.\nYou can read more about HPOs here.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Criticism</span>"
    ]
  },
  {
    "objectID": "Chapters/Model_criticism.html#posterior-predictive-checks-for-discrete-data",
    "href": "Chapters/Model_criticism.html#posterior-predictive-checks-for-discrete-data",
    "title": "5  Model Criticism",
    "section": "5.3 Posterior predictive checks for discrete data",
    "text": "5.3 Posterior predictive checks for discrete data\nSo far we have show examples with continuous data. Many of the tools can still be used for discrete data, for instance az.plot_ppc will automatically use histograms instead of KDEs when the data is discrete. And the bins of the histograms will be centred at integers. Also cumulative plots can be used for discrete data. Still, there are some tools that are more specific for discrete data. In the next sections we discuss posterior predictive checks for count data and binary data.\n\n5.3.1 Posterior predictive checks for count data\nCount data is a type of discrete data that is very common in many fields. For instance, the number of iguanas per square meter in a rainforest, the number of bikes in a bike-sharing station, the number of calls to a call center, the number of emails received, etc. When assessing the fit of a model to count data we need to consider that the data is non-negative and typically skewed. Additionally we also care about the amount of (over/under-)dispersion.\nRootograms are a graphical tool to assess the fit of count data models (Tukey 1977; Kleiber and Zeileis 2016). This tool is handy for diagnosing overdispersion and/or excess zeros in count data models. The name originates from the fact that rootograms plot the square root of the observed counts against the square root of the expected counts.\nThere are a few variations of rootograms. Here we are going to discuss hanging rootograms and suspended rootograms. For the first one the predicted counts are plotted as dots with oblique lines, to represent uncertainty in the predictions. In the absect of uncertainty this lines will be flat (parallel to the x-axis). The observed counts are plotted as (oblique) bars hanging from the lines, see Figure 5.9.\nHow do we read hanging rootograms? For a count, if a bar goes below the dashed line at y=0 the model is underestimating that count. Instead, if the bar goes above the dashed line at y=0, the model is overestimating that count.\n\n# Coming soon with ArviZ 1.0\n\n\n\n\n\n\n\nFigure 5.9: Hanging rootograms, showing the uncertainty in the predictions\n\n\n\nA variant of the hanging rootogram is the suspended rootogram (see Figure 5.10). As for the hanging version a bar that goes below the dashed line means that the model is underestimating that count, and a bar that goes above the dashed line means that the model is overestimating that count. The difference is that for suspended rootograms we plot the residuals (predicted-observed counts) as departing from the dashed line at y=0, instead of “hanging” from the predictions. Ideally, for each count we would expect a small departure from the dashed line equally distributed above and below.\n\n# Coming soon with ArviZ 1.0\n\n\n\n\n\n\n\nFigure 5.10: Suspended rootograms, showing the uncertainty in the predictions\n\n\n\nArviZ currently does not support rootograms, but it will in the future.\nFor those curious about the data, we are using the horseshoe crab dataset (Brockmann 1996). The motivation was to predict the number of “satellite” male crabs based on the features of the female crabs. Satellite crabs are solitary males that gather around a couple of nesting crabs vying for the opportunity to fertilize the eggs. For this example we have two models for one we have used a Poisson likelihood and for the other a Hurdle NegativeBinomial distribution.\n\n\n5.3.2 Posterior predictive checks for binary data\nBinary data is a common form of discrete data, often used to represent outcomes like yes/no, success/failure, or 0/1. Modelling binary data poses a unique challenge for assessing model fit because these models generate predicted values on a probability scale (0-1), while the actual values of the response variable are dichotomous (either 0 or 1).\nOne solution to this challenge was presented by (Greenhill, Ward, and Sacks 2011) and is a know as separation plot. This graphical tool consists of a sequence of bars, where each bar represents a data point. Bars can have one of two colours, one for positive cases and one for negative cases. The bars are sorted by the predicted probabilities, so that the bars with the lowest predicted probabilities are on the left and the bars with the highest predicted probabilities are on the right. Usually the plot also includes a marker showing the expected number of total events. For and ideal fit all the bars with one color should be on one side of the marker and all the bars with the other color on the other side.\nThe following example show a separation plot for a logistic regression model.\n\nidata = az.load_arviz_data('classification10d')\n\naz.plot_separation(idata=idata,\n                   y='outcome',\n                   y_hat='outcome',\n                   expected_events=True, \n                   figsize=(10, 1))\n\n\n\n\n\n\n\nFigure 5.11: Separation plot for a dummy logistic regression model.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Criticism</span>"
    ]
  },
  {
    "objectID": "Chapters/Model_criticism.html#prior-sensitivity-checks",
    "href": "Chapters/Model_criticism.html#prior-sensitivity-checks",
    "title": "5  Model Criticism",
    "section": "5.4 Prior sensitivity checks",
    "text": "5.4 Prior sensitivity checks\nDetermining the sensitivity of the posterior to perturbations of the prior is an important part of building Bayesian models. By sensitivity of the posterior to the prior we mean how much the posterior distribution changes when the prior is changed. Usually this is done by comparing the results obtained using reference prior against the results obtained using alternative priors. The reference prior can be the default prior in packages like Bambi or some other “template” prior from literature or previous analysis of a similar dataset/problem. But it can also be the prior obtained after a more careful elicitation process. Once we have this prior we compute the posterior, diagnose samples and check everything is OK with this model. As usual is import to check that we can trust the model and samples. Then we come up with a set of “alternative” priors that for some reason we consider as relevant. We then fit and check model with these alternative priors. To asses how different posteriors are we can use a series of visual and numerical comparisons as we already explained in this the previous sections. For instance, when working with ArviZ, we can use az.plot_forest or az.plot_density, as both plots allows us to compare multiple posteriors in a single plot. Ideally, we should summarize and report the results of this analysis, so that others can also understand how robust the model is to different prior choices.\nThis process can be a very time-consuming, but it can help us understand how robust the model is to different prior choices. We also notice, that the same procedure can also be used to compare how different likelihoods affect the posterior. So we can discuss the sensitivity of the posterior to the likelihood, the prior or both. It the next section we are going to discuss a more automated way to do this kind of analysis.\n\n5.4.1 Prior and likelihood sensitivity with power-scaling\nThe method we are going to discuss is described in detail in (Kallioinen et al. 2023). The main idea of the method is that it is possible to fit a models once and then approximate the effect of making the prior more or less “informative” with respect to the likelihood, and viceversa. The method uses two ideas. Importance sampling, that we have already discussed in Chapter 6, and power-scaling the prior or likelihood. By power-scaling we mean raising a distribution to a power \\(\\alpha\\). As we can see from Figure 5.12 the effect of power-scaling is to “stretch” or “compress” the distribution. This transformation is very general and it will work for many distributions, one exception being the uniform distribution.\n\n\n\n\n\n\n\n\nFigure 5.12: The effect of power-scaling on Beta and Normal distributions\n\n\n\n\n\nIn the context of Bayes Theorem we can use power-scaling to modulate the relative weights of the likelihood and prior. For instance we could write:\n\\[\np(\\theta \\mid y)_{\\alpha} \\propto p(y \\mid \\theta)**\\alpha \\; p(\\theta)\n\\]\nSetting \\(\\alpha=1\\) is the same as omitting the power, and we recover the usual or “unperturbed” expresion for Bayes’ theorem posterior. Setting \\(\\alpha=0\\) is equivalent to ignoring the likelihood, thus \\(p(\\theta \\mid y)_{\\alpha}\\) will be equivalent to the prior. We can then conclude that any number between 0 and 1 will have the effect of “weakening” the likelihood with respect to the prior. By the same token any number greater than 1 will “strength” the likelihood with respect to the prior. Thus by adjusting the value of \\(\\alpha\\) we can modulate a stronger or weaker likelihood, if we use values close to 1, we emulate relative small perturbations. This very same idea can be apply to the prior. Figure 5.13 shows the effect on the posterior of power-scaling the prior for a fixed likelihood.\n\n\n\n\n\n\n\n\nFigure 5.13: The effect of power-scaling the prior on the posterior for a fixed likelihood.\n\n\n\n\n\nAs we usually work with MCMC samples, we can use importance sampling to approximate the effect of power-scaling the prior or likelihood. This allows us to approximate the sensitivity of the posterior to the prior and likelihood without the need to fit the model multiple times. (Kallioinen et al. 2023) proposed to use Pareto smoothed importance sampling (PSIS), the same method used to compute the ELPD discussed in Chapter 6.\n\n\n5.4.2 Example of prior and likelihood sensitivity analysis\n\n# Coming soon with ArviZ 1.0\n\n\n\n5.4.3 Final remarks about prior (and likelihood) sensitivity analysis\nA couple of final remarks. First, the presence of prior sensitivity (or the absence of likelihood sensitivity) is not an issue by itself. Context and modeling purpose should always be part of an analysis. Sometimes we may expect a certain degree of prior sensitivity, for instance when we are working with very small datasets and/or very informative priors, but in other cases we may not expect it. In any case, the presence of prior sensitivity should be reported and discussed. Second, sensitivity analysis should be performed thoughtfully, without repeatedly adjusting priors to eliminate discrepancis or diagnostic warnings. One way to avoid this is to keep the reference prior unchanged during the process and simply report the results of the sensitivity analysis. But there may be the case that the prior sensitivity uncover a problem with the model, in that case it may be more useful to address the problem and not just report the results of the sensitivity analysis. As usual in statistics and data analysis, the goal is to understand the data and the model, not to get a particular result.\n\n\n\n\nBrockmann, H. Jane. 1996. “Satellite Male Groups in Horseshoe Crabs, Limulus Polyphemus.” Ethology 102 (1): 1–21. https://doi.org/https://doi.org/10.1111/j.1439-0310.1996.tb01099.x.\n\n\nGreenhill, Brian, Michael D. Ward, and Audrey Sacks. 2011. “The Separation Plot: A New Visual Method for Evaluating the Fit of Binary Models.” American Journal of Political Science 55 (4): 991–1002. https://doi.org/https://doi.org/10.1111/j.1540-5907.2011.00525.x.\n\n\nKallioinen, Noa, Topi Paananen, Paul-Christian Bürkner, and Aki Vehtari. 2023. “Detecting and Diagnosing Prior and Likelihood Sensitivity with Power-Scaling.” Statistics and Computing 34 (1): 57. https://doi.org/10.1007/s11222-023-10366-5.\n\n\nKleiber, Christian, and Achim Zeileis. 2016. “Visualizing Count Data Regressions Using Rootograms.” The American Statistician 70 (3): 296–303. https://doi.org/10.1080/00031305.2016.1173590.\n\n\nTukey, John W. 1977. Exploratory Data Analysis. 1 edition. Pearson.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Criticism</span>"
    ]
  },
  {
    "objectID": "Chapters/Model_comparison.html",
    "href": "Chapters/Model_comparison.html",
    "title": "6  Model comparison",
    "section": "",
    "text": "6.0.1 The balance between simplicity and accuracy\nWhen choosing between alternative explanations, there is a principle known as Occam’s razor. In very general terms, this principle establishes that given two or more equivalent explanations for the same phenomenon, the simplest one is the preferred explanation. When the explanations are models, a common criterion for simplicity is the number of parameters in a model.\nAnother factor we generally need to consider when comparing models is their accuracy, that is, how good a model is at fitting the data. According to this criterion, if we have two (or more) models and one of them explains the data better than the other, then that is the preferred model.\nIntuitively, it seems that when comparing models, we tend to prefer those that fit the data best and those that are simpler. But what to do if these two principles conflict? Or more generally, is there a quantitative way to consider both contributions? The short answer is yes. In fact there is more than one way to do it.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Model comparison</span>"
    ]
  },
  {
    "objectID": "Chapters/Model_comparison.html#bayes-factors",
    "href": "Chapters/Model_comparison.html#bayes-factors",
    "title": "6  Model comparison",
    "section": "6.1 Bayes factors",
    "text": "6.1 Bayes factors\nAn alternative to cross-validation, approximate cross-validation with LOO and information criteria is Bayes factors. It is common for Bayes factors to show up in the literature as a Bayesian alternative to frequentist hypothesis testing.\nWe can compare \\(K\\) models by computing their marginal likelihood, \\(p(y \\mid M_k)\\), i.e., the probability of the observed data \\(Y\\) given the model \\(M_K\\). The marginal likelihood is the normalization constant of Bayes’ theorem. We can see this if we write Bayes’ theorem and make explicit the fact that all inferences depend on the model.\n\\[\np (\\theta \\mid Y, M_k ) = \\frac{p(Y \\mid \\theta, M_k) p(\\theta \\mid M_k)}{p(Y \\mid M_k)}\n\\]\nwhere, \\(Y\\) is the data, \\(\\theta\\) is the parameters, and \\(M_K\\) is a model out of \\(K\\) competing models.\nIf our main objective is to choose only one model, the best from a set of models, we can choose the one with the largest value of \\(p(y \\mid M_k)\\). This is fine if we assume that all models have the same prior probability. Otherwise, we must calculate:\n\\[\np(M_k \\mid y) \\propto p(y \\mid M_k) p(M_k)\n\\]\nIf, instead, our main objective is to compare models to determine which are more likely and to what extent, this can be achieved using the Bayes factors:\n\\[\nBF_{01} = \\frac{p(y \\mid M_0)}{p(y \\mid M_1)}\n\\]\nThat is the ratio between the marginal likelihood of two models. The higher the value of \\(BF_{01}\\), the better the model in the numerator (\\(M_0\\) in this example). To facilitate the interpretation of the Bayes factors, and to put numbers into words, Harold Jeffreys proposed a scale for their interpretation, with levels of support or strength, see the following table.\n\n\n\nBayes Factor\nSupport\n\n\n\n\n1–3\nAnecdotal\n\n\n3–10\nModerate\n\n\n10–30\nStrong\n\n\n30–100\nVery Strong\n\n\n&gt;100\nExtreme\n\n\n\nKeep in mind that if you get numbers below 1, then the support is for \\(M_1\\), i.e., the model in the denominator. Tables are also available for those cases, but notice that you can simply take the inverse of the obtained value.\nIt is very important to remember that these rules are just conventions – simple guides at best. Results should always be put in the context of our problems and should be accompanied by enough detail so that others can assess for themselves whether they agree with our conclusions. The proof necessary to ensure something in particle physics, or in court, or to decide to carry out an evacuation in the face of a looming natural catastrophe is not the same.\n\n6.1.1 Some observations\nWe will now briefly discuss some key facts about the marginal likelihood:\n\nThe good: Occam’s razor included. Models with lots of parameters have a higher penalty than models with few parameters. The intuitive reason is that the greater the number of parameters, the more the prior extends with respect to the likelihood. An example where it is easy to see this is with nested models: for example, a polynomial of order 2 “contains” the models polynomial of order 1 and polynomial of order 0.\nThe bad: For many problems, the marginal likelihood cannot be calculated analytically. Also, approximating it numerically is usually a difficult task that in the best of cases requires specialized methods and, in the worst case, the estimates are either impractical or unreliable. In fact, the popularity of the MCMC methods is that they allow obtaining the posterior distribution without the need to calculate the marginal likelihood.\nThe ugly: The marginal likelihood depends very sensitively on the prior distribution of the parameters in each model \\(p(\\theta_k \\mid M_k)\\).\n\nIt is important to note that the good and the ugly points are related. Using marginal likelihood to compare models is a good idea because it already includes a penalty for complex models (which helps us prevent overfitting), and at the same time, a change in the prior will affect the marginal likelihood calculations. At first, this sounds a bit silly; we already know that priors affect calculations (otherwise we could just avoid them). But we are talking about changes in the prior that would have a small effect in the posterior but a great impact on the value of the marginal likelihood.\nThe use of Bayes factors is often a watershed among Bayesians. The difficulty of its calculation and the sensitivity to the priors are some of the arguments against it. Another reason is that, like p-values and hypothesis testing in general, Bayes factors favor dichotomous thinking over the estimation of the “effect size.” In other words, instead of asking ourselves questions like: How many more years of life can a cancer treatment provide? We end up asking if the difference between treating and not treating a patient is “statistically significant.” Note that this last question can be useful in some contexts. The point is that in many other contexts, this type of question is not the question that interests us; we’re only interested in the one that we were taught to answer.\n\n\n6.1.2 Calculation of Bayes factors\nThe marginal likelihood (and the Bayes factors derived from it) is generally not available in closed form, except for a few models. For this reason, many numerical methods have been devised for its calculation. Some of these methods are so simple and naive that they work very poorly in practice. We are going to discuss only one way to compute them, once that can be applied under some particular cases.\n\n\n6.1.3 Savage–Dickey ratio\nThere are times when we want to compare a null hypothesis \\(H_0\\) (or null model) against an alternative \\(H_1\\) hypothesis. For example, to answer the question “Is this coin biased?”, we could compare the value \\(\\theta = 0.5\\) (representing no bias) with the output of a model in which we allow \\(\\theta\\) to vary. For this type of comparison, the null model is nested within the alternative, which means that the null is a particular value of the model we are building. In those cases, calculating the Bayes factor is very easy and does not require any special methods. We only need to compare the prior and posterior evaluated at the null value (for example, \\(\\theta = 0.5\\)) under the alternative model. We can see that this is true from the following expression:\n\\[\nBF_{01} = \\frac{p(y \\mid H_0)}{p(y \\mid H_1)} \\frac{p(\\theta=0.5 \\mid y, H_1)}{p(\\theta=0.5 \\mid H_1)}\n\\]\nThis is true only when \\(H_0\\) is a particular case of \\(H_1\\), see.\nLet’s do it. We only need to sample the prior and posterior for a model. Let’s try the BetaBinomial model with a Uniform prior:\n\nPyMCPyStan\n\n\n\ny = np.repeat([1, 0], [50, 50])  # 50 heads, 50 tails\nwith pm.Model() as model_uni:\n    a = pm.Beta(\"a\", 1, 1)\n    yl = pm.Bernoulli(\"yl\", a, observed=y)\n    idata_uni = pm.sample(2000, random_seed=42)\n    idata_uni.extend(pm.sample_prior_predictive(8000))\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [a]\nSampling 2 chains for 1_000 tune and 2_000 draw iterations (2_000 + 4_000 draws total) took 2 seconds.\nWe recommend running at least 4 chains for robust computation of convergence diagnostics\nSampling: [a, yl]\n\n\n\n\n\n\n\n\n\n\n## coming soon\n\n\n\nAnd now we call az.plot_bf\n\naz.plot_bf(idata_uni, var_name=\"a\", ref_val=0.5);\n\n\n\n\n\n\n\n\nIn the previous Figure we can see one KDE for the prior (black) and one for the posterior (gray). The two black dots show that we evaluated both distributions at the value 0.5. We can see that the Bayes factor in favor of the null hypothesis, BF_01, is \\(\\approx 8\\), which we can interpret as moderate evidence in favor of the null hypothesis.\nAs we have already discussed, the Bayes factors measure which model, as a whole, is better at explaining the data. This includes the prior, even for models that the prior has a relatively low impact on the computation of the posterior. We can also see this prior effect by comparing a second model to the null model.\nIf, instead, our model were a BetaBinomial with a prior Beta(30, 30), the BF_01 would be lower ( on the Jeffrey scale). This is because, according to this model, the value of \\(\\theta=0.5\\) is much more likely a priori than for a Uniform prior, and therefore the prior and posterior will be much more similar. That is, it is not very to see that the posterior is concentrated around 0.5 after collecting data. Don’t just believe me, let’s calculate it:\n\nPyMCPyStan\n\n\n\nwith pm.Model() as model_conc:\n    a = pm.Beta(\"a\", 30, 30)\n    yl = pm.Bernoulli(\"yl\", a, observed=y)\n    idata_conc = pm.sample(2000, random_seed=42)\n    idata_conc.extend(pm.sample_prior_predictive(8000))\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [a]\nSampling 2 chains for 1_000 tune and 2_000 draw iterations (2_000 + 4_000 draws total) took 2 seconds.\nWe recommend running at least 4 chains for robust computation of convergence diagnostics\nSampling: [a, yl]\n\n\n\n\n\n\n\n\n\n\n## coming soon\n\n\n\n\naz.plot_bf(idata_conc, var_name=\"a\", ref_val=0.5);\n\n\n\n\n\n\n\n\nWe can see that the BF_01 is \\(\\approx 1.6\\), which we can interpret as anecdotal evidence in favor of the null hypothesis (see the Jeffreys’ scale, discussed earlier).\n\n\n6.1.4 Bayes factors vs the alternatives\nWe could say that the Bayes factors measure which model, as a whole, is better for explaining the data. This includes the details of the prior, no matter how similar the model predictions are. In many scenarios, this is not what interests us when comparing models. For many real problems prior are not intended to be an accurate description of the True prior distribution of parameters, instead in many problems priors are choosen using some information and with the goal of providing some regulatization. In this and other cases we prefer to evaluate models in terms of how similar their predictions are. For those cases, we can use LOO.\n\n\n\n\nAkaike, H. 1974. “A New Look at the Statistical Model Identification.” IEEE Transactions on Automatic Control 19 (6): 716–23. https://doi.org/10.1109/TAC.1974.1100705.\n\n\nVehtari, Aki, Andrew Gelman, and Jonah Gabry. 2017. “Practical Bayesian Model Evaluation Using Leave-One-Out Cross-Validation and WAIC.” Statistics and Computing 27 (5): 1413–32. https://doi.org/10.1007/s11222-016-9696-4.\n\n\nWatanabe, Sumio. 2013. “A Widely Applicable Bayesian Information Criterion.” Journal of Machine Learning Research 14 (March): 867–97.\n\n\nYao, Yuling, Aki Vehtari, Daniel Simpson, and Andrew Gelman. 2018. “Using Stacking to Average Bayesian Predictive Distributions (with Discussion).” Bayesian Analysis 13 (3): 917–1007. https://doi.org/10.1214/17-BA1091.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Model comparison</span>"
    ]
  },
  {
    "objectID": "Chapters/Case_study_model_comparison.html",
    "href": "Chapters/Case_study_model_comparison.html",
    "title": "7  Model Comparison (case study)",
    "section": "",
    "text": "7.1 Information criteria for hierarchical and multi-likelihood models\nThere are many situations where one model can be used for several prediction tasks at the same time. Hierarchical models or models with multiple observations are examples of such cases. With two observations for example, the same model can be used to predict only the first observation, only the second or both observations at the same time.\nBefore estimating the predictive accuracy, there are two important questions to answer: what is the predictive task we are interested in and, whether or not the exchangeability criteria is met. This section will show several alternative ways to define the predictive task using the same model.\nWe are going to analyze data from the 2022-2023 season of Spain’s highest men’s football league. In this notebook, we will start from InferenceData files, but the model and data used is also described because they are key to understanding what is going on. If you are interested in the model itself in more depth, or the coding of the models themselves, refer to TODO.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Comparison (case study)</span>"
    ]
  },
  {
    "objectID": "Chapters/Case_study_model_comparison.html#the-data",
    "href": "Chapters/Case_study_model_comparison.html#the-data",
    "title": "7  Model Comparison (case study)",
    "section": "7.2 The data",
    "text": "7.2 The data\nThe data used to fit the models are the results of all matches from 2022-2023 and the budget of each team (for the 2nd model only). Our data therefore consists of two tables: one with one row per match, containing the home and away teams and the goals scored by each; another with one row per team, containing the team and its budget.\n\nimport arviz.data.datasets as azd\n\nazd.REMOTE_DATASETS.update({\n    \"laliga_base\": azd.RemoteFileMetadata(\n        name=\"laliga_base\",\n        filename=\"base_model.nc\",\n        url=\"http://figshare.com/ndownloader/files/44240747\",\n        checksum=\"6f39f2646bbef8259f0d8fa3c6d7f6304b9cf6d63f5df7cdae1e6b3a93094fda\",\n        description=\"\"\"Base model for the analysis of LaLiga 2022-2023 results.\"\"\"\n    ),\n    \"laliga_budget\": azd.RemoteFileMetadata(\n        name=\"laliga_budget\",\n        filename=\"budget_model.nc\",\n        url=\"http://figshare.com/ndownloader/files/44240762\",\n        checksum=\"5c77671b98bcf2ae30d45a7ab3fd47b855e285ffe15ce2cd176ca25b1fd369fd\",\n        description=\"\"\"Budget model for the analysis of LaLiga 2022-2023 results.\"\"\"\n    ),\n    \"laliga_nofield\": azd.RemoteFileMetadata(\n        name=\"laliga_nofield\",\n        filename=\"nofield_model.nc\",\n        url=\"http://figshare.com/ndownloader/files/44240807\",\n        checksum=\"3e4910690cb2d8428e6486ba9973921d1b195bb8f14e2136422a0fc5d953d8e8\",\n        description=\"\"\"No field effect model for the analysis of LaLiga 2022-2023 results.\"\"\"\n    )\n})",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Comparison (case study)</span>"
    ]
  },
  {
    "objectID": "Chapters/Case_study_model_comparison.html#base-model",
    "href": "Chapters/Case_study_model_comparison.html#base-model",
    "title": "7  Model Comparison (case study)",
    "section": "7.3 Base model",
    "text": "7.3 Base model\nThe model used is taken from this blog post which was added as an example notebook to PyMC docs.\nWe are trying to model a league in which all teams play against each other twice. We indicate the number of goals scored by the home and the away team in the \\(m\\)-th match of the season (\\(M\\) matches) as \\(y_{m,h}\\) and \\(y_{m,a}\\) respectively. The model assumes the goals scored by a team follow a Poisson distribution:\n\\[y_{m,f} | \\theta_{m,f} \\sim \\text{Poiss}(\\theta_{m,f})\\]\nwhere \\(f = {h, a}\\) indicates the field, representing either home or away team. We will therefore start with a model containing two observation vectors: \\(\\mathbf{y_h} = (y_{1,h}, y_{2,h}, \\dots, y_{M,h})\\) and \\(\\mathbf{y_a} = (y_{1,a}, \\dots, y_{M,a})\\). In order to take into account each team’s scoring and defensive power and also the advantage of playing home, we will use different formulas for \\(\\theta_{m,h}\\) and for \\(\\theta_{m,a}\\):\n\\[\n\\begin{align}\n\\theta_{m,h} &= \\alpha + home + atts_{home\\_team} + defs_{away\\_team}\\\\\n\\theta_{m,a} &= \\alpha + atts_{away\\_team} + defs_{home\\_team}\n\\end{align}\n\\]\nThe expected number of goals score by the home team \\(\\theta_{m,h}\\) depends on an intercept (\\(\\alpha\\)), \\(home\\) to quantify the home advantage, on the attacking power of the home team and on the defensive power of the away team. Similarly, the expected number of goals score by the away team \\(\\theta_{m,a}\\) also depends on the intercept but not on the home advantage, and now, consequently, we use the attacking power of the away team and the defensive power of the home team.\nSumming up and including the priors, our base model is the following one:\n\\[\n\\begin{align}\n\\alpha &\\sim \\text{Normal}(0,5) \\qquad &\\text{scalar} \\\\\nhome &\\sim \\text{Normal}(0,5) \\qquad &\\text{scalar} \\\\\nsd_{att} &\\sim \\text{HalfStudentT}(3,2.5) \\qquad &\\text{scalar} \\\\\nsd_{def} &\\sim \\text{HalfStudentT}(3,2.5) \\qquad &\\text{scalar} \\\\\natts_* &\\sim \\text{Normal}(0,sd_{att}) \\qquad &\\text{shape (T,)} \\\\\ndefs_* &\\sim \\text{Normal}(0,sd_{def}) \\qquad &\\text{shape (T,)} \\\\\natts &= atts_* - \\text{mean}(atts_*) \\qquad &\\text{shape (T,)} \\\\\ndefs &= defs_* - \\text{mean}(defs_*) \\qquad &\\text{shape (T,)} \\\\\n\\mathbf{y}_h &\\sim \\text{Poiss}(\\theta_h) \\qquad &\\text{shape (M,)} \\\\\n\\mathbf{y}_a &\\sim \\text{Poiss}(\\theta_a) \\qquad &\\text{shape (M,)}\n\\end{align}\n\\]\nwhere \\(\\theta_j\\) has been defined above.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Comparison (case study)</span>"
    ]
  },
  {
    "objectID": "Chapters/Case_study_model_comparison.html#budget-model",
    "href": "Chapters/Case_study_model_comparison.html#budget-model",
    "title": "7  Model Comparison (case study)",
    "section": "7.4 Budget model",
    "text": "7.4 Budget model\nThe budget model only represents a slight variation on the base model, adding two new parameters and modifying \\(atts\\) and \\(defs\\) variables:\n\\[\n\\begin{align}\nbudget_{att} &\\sim \\text{Normal}(0,5) \\qquad &\\text{scalar} \\\\\nbudget_{def} &\\sim \\text{Normal}(0,5) \\qquad &\\text{scalar} \\\\\natts &= atts_* - \\text{mean}(atts_*) + budget_{att} \\log{\\mathbf{b}} \\qquad &\\text{shape (T,)} \\\\\ndefs &= defs_* - \\text{mean}(defs_*) + budget_{def} \\log{\\mathbf{b}} \\qquad &\\text{shape (T,)} \\\\\n\\end{align}\n\\]\nwith \\(\\mathbf{b} = (b_1, b_2, \\dots, b_T)\\) the budgets of each team.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Comparison (case study)</span>"
    ]
  },
  {
    "objectID": "Chapters/Case_study_model_comparison.html#no-field-effect-model",
    "href": "Chapters/Case_study_model_comparison.html#no-field-effect-model",
    "title": "7  Model Comparison (case study)",
    "section": "7.5 No field effect model",
    "text": "7.5 No field effect model\nThis third model is another variation on the base model, where we remove the \\(home\\) variable. Thus, the \\(\\theta\\) variables become:\n\\[\n\\begin{align}\n\\theta_{m,h} &= \\alpha + atts_{home\\_team} + defs_{away\\_team}\\\\\n\\theta_{m,a} &= \\alpha + atts_{away\\_team} + defs_{home\\_team}\n\\end{align}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Comparison (case study)</span>"
    ]
  },
  {
    "objectID": "Chapters/Case_study_model_comparison.html#variable-and-index-glossary",
    "href": "Chapters/Case_study_model_comparison.html#variable-and-index-glossary",
    "title": "7  Model Comparison (case study)",
    "section": "7.6 Variable and index glossary",
    "text": "7.6 Variable and index glossary\n\nMatches. The total number of matches in the season, 380. \\(M\\) denotes the total, and we use \\(m\\) as the index going from \\(1\\) to \\(M\\).\nTeams. The number of teams in the league, 20. \\(T\\) denotes the total, and we use \\(t\\) as the index going from \\(1\\) to \\(T\\).\nField. The field identifier. Two teams play in each game, one being the home team, the other the away one. We use \\(f\\) as the index indicating the field, which can take only two values \\(h\\) or \\(a\\).\nArbitrary index. For theoretical concepts, we use \\(i\\) to indicate an arbitrary index.\n\n\n# load data\nbase_idata = az.load_arviz_data(\"laliga_base\")\nbudget_idata = az.load_arviz_data(\"laliga_budget\")\nnofield_idata = az.load_arviz_data(\"laliga_nofield\")\n\nmodel_dict = {\"base\": base_idata, \"budget\": budget_idata, \"nofield\": nofield_idata}",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Comparison (case study)</span>"
    ]
  },
  {
    "objectID": "Chapters/Case_study_model_comparison.html#information-criterion-calculation",
    "href": "Chapters/Case_study_model_comparison.html#information-criterion-calculation",
    "title": "7  Model Comparison (case study)",
    "section": "7.7 Information criterion calculation",
    "text": "7.7 Information criterion calculation\nDue to the presence of the two likelihoods in our model, we cannot call az.loo or az.waic straight away because the predictive task to evaluate is ambiguous. The calculation of information criteria requires pointwise likelihood values, \\(p(y_i|\\theta)\\) with \\(y_i\\) indicating observation \\(i\\)-th and \\(\\theta\\) representing all the parameters in the model. We need to define \\(y_i\\), what does one observation represent in our model.\nAs we were introducing above, this model alone can tackle several predictive tasks. These predictive tasks can be identified by the definition of one observation which at the same time defines how are pointwise likelihood values to be calculated. Here are some examples:\n\nWe could be a group of students supporting different teams with budget to travel only to one away match of our respective teams. We may want to travel to the match where our team will score the most goals (while being the away team and also independently of the winner of the match). We will therefore assess the predictive accuracy of our model using only \\(\\mathbf{y}_a\\).\nWe could also be football fans without any clear allegiance who love an intense match between two teams of similar strength. Based on previous experience, we may consider matches that end up 3-3 or 4-4 the ones that better fit our football taste. Now we need to assess the predictive accuracy using the result of the whole match.\nEven another alternative would be wanting to be present at the match where a single team scores the most goals. In this situation, we would have to put both home and away goals in the same bag and assess the predictive accuracy on the ability to predict values from this bag, we may call the observations in this hypothetical bag “number of goals scored per match and per team”.\n\nThere are even more examples of predictive tasks where this particular model can be of use. However, it is important to keep in mind that this model predicts the number of goals scored. Its results can be used to estimate probabilities of victory and other derived quantities, but calculating the likelihood of these derived quantities may not be straightforward. And as you can see above, there isn’t one unique predictive task: it all depends on the specific question you’re interested in. As often in statistics, the answer to these questions lies outside the model, you must tell the model what to do, not the other way around.\nEven though we know that the predictive task is ambiguous, we will start trying to calculate az.loo with idata_base and then work on the examples above and a couple more to show how would this kind of tasks be performed with ArviZ. But before that, let’s see what ArviZ says when you naively ask it for the LOO of a multi-likelihood model:\n\n\nCode\naz.loo(base_idata)\n\n\nTypeError: Found several log likelihood arrays ['home_goals', 'away_goals'], var_name cannot be None\n\n\nAs expected, ArviZ has no way of knowing what predictive task we have in mind so it raises an error.\n\n7.7.1 Predicting the goals scored by the away team\nIn this particular case, we are interested in predicting the goals scored by the away team. We will still use the goals scored by the home team, but won’t take them into account when assessing the predictive accuracy. Below there is an illustration of how would cross validation be performed to assess the predictive accuracy in this particular case:\n\nThis can also be seen from a mathematical point of view. We can write the pointwise log likelihood in the following way so it defines the predictive task at hand:\n\\[ p(y_i|\\theta) = p(y_{i,h}|\\theta_{i,h}) = \\text{Poiss}(y_{i,h}; \\theta_{i,h}) \\]\nwith \\(i\\) being both the match indicator (\\(m\\), which varies with \\(i\\)) and the field indicator (\\(f\\), here always fixed at \\(h\\)). These are precisely the values stored in the home_goals of the log_likelihood group of idata_base.\nWe can tell ArviZ to use these values using the argument var_name.\n\naz.loo(base_idata, var_name=\"home_goals\")\n\nComputed from 8000 posterior samples and 380 observations log-likelihood matrix.\n\n         Estimate       SE\nelpd_loo  -568.55    12.66\np_loo       15.59        -\n------\n\nPareto k diagnostic values:\n                         Count   Pct.\n(-Inf, 0.70]   (good)      380  100.0%\n   (0.70, 1]   (bad)         0    0.0%\n   (1, Inf)   (very bad)    0    0.0%\n\n\n\naz.compare(model_dict, var_name=\"home_goals\")\n\n\n\n\n\n\n\n\n\nrank\nelpd_loo\np_loo\nelpd_diff\nweight\nse\ndse\nwarning\nscale\n\n\n\n\nbudget\n0\n-567.057532\n10.716592\n0.000000\n0.715937\n13.055752\n0.000000\nFalse\nlog\n\n\nbase\n1\n-568.553182\n15.589902\n1.495650\n0.284063\n12.661187\n2.767823\nFalse\nlog\n\n\nnofield\n2\n-574.119733\n15.814319\n7.062202\n0.000000\n15.312384\n4.387979\nFalse\nlog\n\n\n\n\n\n\n\n\n\naz.compare(model_dict, var_name=\"away_goals\")\n\n\n\n\n\n\n\n\n\nrank\nelpd_loo\np_loo\nelpd_diff\nweight\nse\ndse\nwarning\nscale\n\n\n\n\nbudget\n0\n-480.386626\n6.888639\n0.000000\n1.000000e+00\n11.546581\n0.000000\nFalse\nlog\n\n\nbase\n1\n-482.936745\n10.241680\n2.550119\n2.781109e-14\n11.831405\n1.943906\nFalse\nlog\n\n\nnofield\n2\n-488.971732\n10.193199\n8.585106\n0.000000e+00\n9.456310\n3.420849\nFalse\nlog\n\n\n\n\n\n\n\n\n\n\n7.7.2 Predicting the outcome of a match\nAnother option is being interested in the outcome of the matches. In our current model, the outcome of a match is not who wins or the aggregate of scored goals by both teams, the outcome is the goals scored by the home team and by the away team, both quantities at the same time. Below there is an illustration on how would cross validation be used to assess the predictive accuracy in this situation:\n\nThe one observation in this situation is therefore a vector with two components: \\(y_i = (y_{i,h}, y_{i,a})\\). Like above, we also have \\(M\\) observations. The pointwise likelihood is therefore a product:\n\\[\np(y_i|\\theta) = p(y_{i,h}|\\theta_{i,h})p(y_{i,a}|\\theta_{i,a}) =\n\\text{Poiss}(y_{i,h}; \\theta_{i,h})\\text{Poiss}(y_{i,a}; \\theta_{i,a})\n\\]\nwith \\(i\\) being equal to the match indicator \\(m\\). Therefore, we have \\(M\\) observations like in the previous example, but each observation has two components.\nWe can calculate the product as a sum of logarithms and store the result in a new variable inside the log_likelihood group.\n\ndef match_lik(idata):\n    log_lik = idata.log_likelihood\n    log_lik[\"matches\"] = log_lik.home_goals + log_lik.away_goals\n    return idata\n\nbase_idata = match_lik(base_idata)\nbudget_idata = match_lik(budget_idata)\nnofield_idata = match_lik(nofield_idata)\n\naz.loo(base_idata, var_name=\"matches\")\n\nComputed from 8000 posterior samples and 380 observations log-likelihood matrix.\n\n         Estimate       SE\nelpd_loo -1051.48    16.93\np_loo       25.81        -\n------\n\nPareto k diagnostic values:\n                         Count   Pct.\n(-Inf, 0.70]   (good)      380  100.0%\n   (0.70, 1]   (bad)         0    0.0%\n   (1, Inf)   (very bad)    0    0.0%\n\n\n\naz.compare(model_dict, var_name=\"matches\")\n\n\n\n\n\n\n\n\n\nrank\nelpd_loo\np_loo\nelpd_diff\nweight\nse\ndse\nwarning\nscale\n\n\n\n\nbudget\n0\n-1047.441755\n17.606963\n0.000000\n0.84896\n17.072842\n0.000000\nFalse\nlog\n\n\nbase\n1\n-1051.475361\n25.812936\n4.033606\n0.15104\n16.927232\n3.399485\nFalse\nlog\n\n\nnofield\n2\n-1063.093483\n26.019985\n15.651728\n0.00000\n17.563750\n5.491706\nFalse\nlog\n\n\n\n\n\n\n\n\n\n\n7.7.3 Predicting the goals scored per match and per team\nAnother example described above is being interested in the scored goals per match and per team. In this situation, our observations are a scalar once again.\n\nThe expression of the likelihood is basically the same as the one in the first example (both cases are scalars), but the difference is in the index, but that does not make it less significant:\n\\[\np(y_i|\\theta) = p(y_{i}|\\theta_{i}) =\n\\text{Poiss}(y_{i}; \\theta_{i})\n\\]\nwith \\(i\\) being both the match indicator \\(m\\) and the field indicator \\(f\\), both varying with \\(i\\). Now, we will consider \\(i\\) as an index iterating over the values in\n\\[\\big\\{(1,h), (2,h), \\dots, (M-1,h), (M,h), (1,a), (2,a) \\dots (M-1,a), (M,a)\\big\\}\\]\nTherefore, unlike in previous cases, we have \\(2M\\) observations.\nWe can obtain the pointwise log likelihood corresponding to this case by concatenating the pointwise log likelihoods of home_goals and away_goals. Then, like in the previous case, store the result in a new variable inside the log_likelihood group.\n\ndef goals_lik(idata):\n    log_lik = idata.log_likelihood\n    log_lik[\"goals\"] = xr.concat((log_lik.home_goals, log_lik.away_goals), \"match\").rename({\"match\": \"goal\"})\n    return idata\n    \nbase_idata = goals_lik(base_idata)\nbudget_idata = goals_lik(budget_idata)\nnofield_idata = goals_lik(nofield_idata)\n\naz.loo(base_idata, var_name=\"goals\")\n\nComputed from 8000 posterior samples and 760 observations log-likelihood matrix.\n\n         Estimate       SE\nelpd_loo -1051.49    17.60\np_loo       25.83        -\n------\n\nPareto k diagnostic values:\n                         Count   Pct.\n(-Inf, 0.70]   (good)      760  100.0%\n   (0.70, 1]   (bad)         0    0.0%\n   (1, Inf)   (very bad)    0    0.0%\n\n\n\naz.compare(model_dict, var_name=\"goals\")\n\n\n\n\n\n\n\n\n\nrank\nelpd_loo\np_loo\nelpd_diff\nweight\nse\ndse\nwarning\nscale\n\n\n\n\nbudget\n0\n-1047.444158\n17.605232\n0.000000\n8.519505e-01\n17.710455\n0.000000\nFalse\nlog\n\n\nbase\n1\n-1051.489927\n25.831582\n4.045769\n1.480495e-01\n17.604908\n3.382467\nFalse\nlog\n\n\nnofield\n2\n-1063.091466\n26.007518\n15.647308\n9.992007e-16\n18.260083\n5.564137\nFalse\nlog\n\n\n\n\n\n\n\n\n\n\n7.7.4 Predicting team level performance\nThe last example covered here is estimating the predictive accuracy at group level. This can be useful to assess the accuracy of predicting the whole season of a new team. In addition, this can also be used to evaluate the hierarchical part of the model.\nAlthough theoretically possible, importance sampling tends to fail at the group level due to all the observations being too informative. See this post for more details.\nIn this situation, we could describe the cross validation as excluding a team. When we exclude a team, we will exclude all the matches played by the team, not only the goals scored by the team but the whole match. Here is the illustration:\n\nIn the first column, we are excluding “Levante U.D.” which in the rows shown only appears once. In the second one, we are excluding “Athletic Club” which appears two times. This goes on following the order of appearance in the away team column.\n\ndef team_lik(idata):\n    log_lik = idata.log_likelihood\n    const = idata.constant_data\n    groupby_sum_home = log_lik.groupby(const.home_team).sum().rename({\"home_team\": \"team\"})\n    groupby_sum_away = log_lik.groupby(const.away_team).sum().rename({\"away_team\": \"team\"})\n\n    log_lik[\"teams_match\"] = (\n        groupby_sum_home.home_goals + groupby_sum_home.away_goals +\n        groupby_sum_away.home_goals + groupby_sum_away.away_goals\n    )\n    return idata\n    \nbase_idata = team_lik(base_idata)\nbudget_idata = team_lik(budget_idata)\nnofield_idata = team_lik(nofield_idata)\n\n\naz.loo(base_idata, var_name=\"teams_match\")\n\n/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/arviz/stats/stats.py:792: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.70 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  warnings.warn(\n\n\nComputed from 8000 posterior samples and 20 observations log-likelihood matrix.\n\n         Estimate       SE\nelpd_loo -2109.44    19.91\np_loo       52.62        -\n\nThere has been a warning during the calculation. Please check the results.\n------\n\nPareto k diagnostic values:\n                         Count   Pct.\n(-Inf, 0.70]   (good)        3   15.0%\n   (0.70, 1]   (bad)        14   70.0%\n   (1, Inf)   (very bad)    3   15.0%\n\n\nTODO: it would probably be best to run reloo for the three models for this case and include that on figshare too.\n\naz.compare(model_dict, var_name=\"teams_match\")\n\n/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/arviz/stats/stats.py:792: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.70 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  warnings.warn(\n/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/arviz/stats/stats.py:792: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.70 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  warnings.warn(\n/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/arviz/stats/stats.py:792: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.70 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\nrank\nelpd_loo\np_loo\nelpd_diff\nweight\nse\ndse\nwarning\nscale\n\n\n\n\nbudget\n0\n-2098.768712\n36.970533\n0.000000\n8.019749e-01\n22.754832\n0.000000\nTrue\nlog\n\n\nbase\n1\n-2109.444812\n52.616593\n10.676101\n1.980251e-01\n19.908256\n6.769330\nTrue\nlog\n\n\nnofield\n2\n-2133.223518\n52.665911\n34.454806\n2.220446e-16\n20.286200\n9.252461\nTrue\nlog",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Comparison (case study)</span>"
    ]
  },
  {
    "objectID": "Chapters/Prior_elicitation.html",
    "href": "Chapters/Prior_elicitation.html",
    "title": "8  Prior Elicitation",
    "section": "",
    "text": "8.1 Priors and Bayesian Statistics\nIf you are reading this guide, you probably already know what a prior distribution is. But let’s do a quick recap. In Bayesian statistics, the prior distribution is the probability distribution that expresses information about the parameters of the model before observing the data. The prior distribution is combined with the likelihood to obtain the posterior distribution, which is the distribution of the parameters after observing the data.\nPriors are one way to convey domain-knowledge information into models. Other ways to include information in a model are the type of model or its overall structure, e.g. using a linear model, and the choice of likelihood.\nLet use a couple of examples to think about priors and it’s role in Bayesian statistics, Figure 8.2 shows two priors, one in blue (Beta(0.5, 0.5)) and one in red (Beta(10, 10)).\nWe are going to combine these priors with data using a Binomial likelihood. And we are going to update these priors sequentially, i.e. we will be adding some data and compute the posterior. And then add some more data and keep updating. So, at each step \\(i\\) we add data and the distribution we are computing are the posteriors, but these posteriors are also the priors for step \\(i+1\\). This sequential updating, where a posterior becomes the prior of the next analysis, is possible because the Beta distribution is conjugate with the Binomial. Conjugate prior are priors that when combined with a likelihood function result in a posterior distribution that is of the same form as the prior distribution. Usually, we don’t care too much about conjugate priors, but sometimes, like for this animation, they can be useful.\nWe have represented these sequential updating in an animation (see Figure 8.3). As the animation moves forward, i.e. we add more data, we will see that the posteriors gradually converge to the same distribution.\nAsymptotically, priors have no meaning. If we have infinite data, not matter the prior you will get exactly the same posterior. When there is a large amount of data, the likelihood function dominates the updating process, and the prior has little influence on the posterior. In this limit, different reasonable priors will tend to converge to the same posterior as the amount of data increases.\nThere is a catch in the previous statement, or maybe two. If we assign 0 prior probability to a value, no amount of data will turn that into a positive value. In other words, if a particular value or hypothesis is assigned zero prior probability, it will also have zero posterior probability, regardless of the data observed. Alternatively, if we assign a prior probability of 1 to a value (and zero to the rest), no amount of data will allow us to update that prior neither. This is known as Cromwell’s rule and states that the use of prior probabilities of 1 (“the event will definitely occur”) or 0 (“the event will definitely not occur”) should be avoided, except when applied to statements that are logically true or false, such as \\(2+2=4\\).\nOK, but if we take Cromwell’s advice and avoid these corner cases eventually the data will dominate the priors. That’s true, as well as that asymptotically, we are all dead. For real, finite data, we should expect priors to have some impact on the results. The actual impact will depend on the specific combinations of priors, likelihood and data. Section 8.3 shows a couple of combinations. In practice we often need to worry about our priors, but maybe less than we think.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Prior Elicitation</span>"
    ]
  },
  {
    "objectID": "Chapters/Prior_elicitation.html#priors-and-bayesian-statistics",
    "href": "Chapters/Prior_elicitation.html#priors-and-bayesian-statistics",
    "title": "8  Prior Elicitation",
    "section": "",
    "text": "Figure 8.2: Two different priors\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.3: Priors updating as we keep adding data\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.4: If we give 0 prior probability, then we will always get 0 posterior probability\n\n\n\n\n\n\n\n\n\n\nFigure 8.5: The posterior is an interplay of prior and likelihood",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Prior Elicitation</span>"
    ]
  },
  {
    "objectID": "Chapters/Prior_elicitation.html#types-of-priors",
    "href": "Chapters/Prior_elicitation.html#types-of-priors",
    "title": "8  Prior Elicitation",
    "section": "8.2 Types of Priors",
    "text": "8.2 Types of Priors\nUsually, priors are described as informative vs non-informative. Informative priors are priors that convey specific information about the parameters of the model, while non-informative priors do not convey specific information about the parameters of the model. Non-informative priors are often used when little or no domain knowledge is available. A simple, intuitive and old rule for specifying a non-informative prior is the principle of indifference, which assigns the same probability to all possible events. Non-informative priors are also called objective priors especially when the main motivation for using them is to avoid the need to specify a prior distribution.\nNon-informative priors can be detrimental and difficult to implement or use. Informative priors can also be problematic in practice, as the information needed to specify them may be absent or difficult to obtain. And even if the information is available specifying informative priors can be time-consuming. A middle ground is to use weakly informative priors, which are priors that convey some information about the parameters of the model but are not overly specific. Weakly informative priors can help to regularize inference and even have positive side effects like improving sampling efficiency.\n\n\n\n\n\n\nFigure 8.6: Priors are often defined in terms of how much information they convey\n\n\n\nIt is important to recognize that the amount of information a priors carry can vary continuously and that the categories we use to discuss priors are a matter of convenience and not a matter of principle. These categories are qualitative and not well-defined. Still, they can be helpful when talking about priors more intuitively.\nSo far we have discussed the amount of information. There are at least two issues that seem fishy about this discussion. First, the amount of information is a relative concept, against what are we evaluating if a prior is informative or not? Second, the amount of information does not necessarily mean the information is good or correct. For instance, it’s possible to have a very informative prior based on wrong assumptions. Thus when we say informative we don’t necessarily mean reliable or that the prior will bias the inference in the correct direction and amount.\nThere is one way to frame the discussion about priors that can help to address these issues. That is to think about priors in terms of the prior predictive distribution they induce. In other words, we think about the priors in terms of their predictions about unobserved, potentially observable data. This mental scaffold can be helpful in many ways:\n\nFirst, it naturally leads us to think about priors in relation to other priors and the likelihood, i.e. it reflects the fact that we cannot understand a prior without the context of the model (Gelman, Simpson, and Betancourt 2017).\nSecond, it gives us an operational definition of what we mean by vague, informative, or weakly informative prior. An informative prior is a prior that makes predictions that are about the same. A weakly informative prior is a prior that makes predictions that are somewhere in between. The distinctions are still qualitative and subjective, but we have a criteria that is context-dependent and we can evaluate during a Bayesian data analysis. Figure Figure 8.7 shows a very schematic representation of this idea.\nThird, it provides us with a way to evaluate the priors for consistency, because the priors we are setting should agree with the prior predictive distribution we imagine. For instance, if we are setting an informative prior that induces a prior predictive distribution that is narrower, shifted or very different in any other way from the one we imagine either the prior or our expectation of the prior predictive distribution is wrong. We have specified two conflicting pieces of information. Reconciling these two pieces of information does not guarantee that the prior or any other part of the model is correct, but it provides internal consistency, which is a good starting point for a model.\n\n\n\n\n\n\n\nFigure 8.7: Prior amount of information in terms of the prior predictive distribution induced by them\n\n\n\nUsing the prior predictive distribution to evaluate priors is inherently a global approach, as it assesses the combined impact of all priors and the likelihood. However, during prior elicitation, we may sometimes focus on making one or two priors more informative while keeping the others vague. In these cases, we can think of this as having a “local” mix of priors with varying levels of informativeness. In practice, we often balance this global perspective with the local approach, tailoring priors to the specific needs of the model.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Prior Elicitation</span>"
    ]
  },
  {
    "objectID": "Chapters/Prior_elicitation.html#sec-prior-elicitation-workflow",
    "href": "Chapters/Prior_elicitation.html#sec-prior-elicitation-workflow",
    "title": "8  Prior Elicitation",
    "section": "8.3 Bayesian Workflow for Prior Elicitation",
    "text": "8.3 Bayesian Workflow for Prior Elicitation\nIt is important to think of prior elicitation in the context of a flexible workflow adapting to the specific needs of the model and the data. The workflow should also be iterative, as prior elicitation may need to be revisited as the model is developed and the data are analysed.\nKnowing when to perform prior elicitation is central to a prior elicitation workflow. In some situations, default priors and models may be sufficient, especially for routine inference that applies the same, or very similar, model to similar new datasets. But even for new datasets, default priors can be a good starting point, adjusting them only after initial analysis reveals issues with the posterior or computational problems. As with other components of the Bayesian workflow, prior elicitation isn’t just a one-time task. It’s not even one that is always done at the beginning of the analysis.\nFor simple models with strong data, the prior may have minimal impact, and starting with default or weakly informed priors may be more appropriate and provide better results than attempting to generate very informative priors. The key is knowing when it’s worth investing resources in prior elicitation. Or more nuanced how much time and domain knowledge is needed in prior specification. Usually, getting rough estimates can be sufficient to improve inference. Thus, in practice, weakly informative priors are often enough. In a model with many parameters eliciting all of them one by one may be too time-consuming and not worth the effort. Refining just a few priors in a model can be sufficient to improve inference.\nThe prior elicitation process should also include a step to verify the usefulness of the information and assess how sensitive the results are to the choice of priors, including potential conflicts with the data. This process can help identify when more or less informative priors are needed and when the model may need to be adjusted.\nFinally, we want to highlight that prior elicitation isn’t just about choosing the right prior but also about understanding the model and the problem. So even if we end up with a prior that has little impact on the posterior, compared to a vague or default prior, performing prior elicitation could be useful for the modeller. Especially among newcomers setting priors can be seen as an anxiogenic task. Spending some time thinking about priors, with the help of proper tools, can help reduce this brain drain and save mental resources for other modelling tasks.\nNevertheless, usually, the selling point when discussing in favour of priors is that they allow the inclusion of domain information. But there are potentially other advantages of :\n\nSampling efficiency. Often a more informed priors results in better sampling. This does not mean we should tweak the prior distribution to solve sampling problems, instead incorporating some domain-knowledge information can help to avoid them.\nRegularization. More informed priors can help to regularize the model, reducing the risk of overfitting. We make a distinction between regularization and “conveying domain-knowledge information” because motivations and justifications can be different in each case.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Prior Elicitation</span>"
    ]
  },
  {
    "objectID": "Chapters/Prior_elicitation.html#priors-and-entropy",
    "href": "Chapters/Prior_elicitation.html#priors-and-entropy",
    "title": "8  Prior Elicitation",
    "section": "8.4 Priors and entropy",
    "text": "8.4 Priors and entropy\nThe entropy is a property of probability distributions the same way the mean or variance are, actually it’s the expected value of the negative log probability of the distribution. We can think of entropy as a measure of the information or uncertainty of a distribution has. Loosely speaking the entropy of a distribution is high when the distribution is spread out and low when the distribution is concentrated. In the context of prior elicitation maximum entropy can be a guiding principle to pick priors. According to this principle we should choose the prior that maximizes the entropy, subject to known constraints of the prior (Jaynes 2003). This is a way to choose a prior that is as vague as possible, given the information we have. Figure Figure 8.8 shows a distribution with support in [0, 1]. On the first panel we have the distribution with maximum entropy and no other restrictions. We can see that this is a uniform distribution. On the middle we have the distribution with maximum entropy and a given mean. This distribution looks similar to an exponential distribution. On the last panel we have the distribution with maximum entropy and 70% of its mass between 0.5 and 0.75.\n\n\n\n\n\n\nFigure 8.8: 3 maximum entropy distributions subject to different constrains\n\n\n\nFor some priors in a model, we may know or assume that most of the mass is within a certain interval. This information is useful for determining a suitable prior, but this information alone may not be enough to obtain a unique set of parameters. Figure 8.9 shows Beta distributions with 90% of the mass between 0.1 and 0.7. As you can see we can obtain very different distributions, conveying very different prior knowledge. The red distribution is the one with maximum entropy, given the constraints.\n\n\n\n\n\n\nFigure 8.9: Beta distributions with a 90% of it mass between 0.1 and 0.7, the red one is the one with maximum entropy",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Prior Elicitation</span>"
    ]
  },
  {
    "objectID": "Chapters/Prior_elicitation.html#preliz",
    "href": "Chapters/Prior_elicitation.html#preliz",
    "title": "8  Prior Elicitation",
    "section": "8.5 Preliz",
    "text": "8.5 Preliz\nPreliZ (Icazatti et al. 2023) is a Python package that helps practitioners choose prior distributions by offering a set of tools for the various facets of prior elicitation. It covers a range of methods, from unidimensional prior elicitation on the parameter space to predictive elicitation on the observed space. The goal is to be compatible with probabilistic programming languages (PPL) in the Python ecosystem like PyMC and PyStan, while remaining agnostic of any specific PPL.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Prior Elicitation</span>"
    ]
  },
  {
    "objectID": "Chapters/Prior_elicitation.html#maximum-entropy-distributions-with-maxent",
    "href": "Chapters/Prior_elicitation.html#maximum-entropy-distributions-with-maxent",
    "title": "8  Prior Elicitation",
    "section": "8.6 Maximum entropy distributions with maxent",
    "text": "8.6 Maximum entropy distributions with maxent\nIn PreliZ we can compute maximum entropy priors using the function maxent. It works for unidimensional distributions. The first argument is a PreliZ distribution. Then we specify an upper and lower bounds and then the probability inside the range defined by the bounds.\nLet’s say that we want to elicit a scale parameter and from domain knowledge we know that the parameter has a relatively high probability of being less than 3. In that case we could use a HalfNormal and then do:\n\npz.maxent(pz.HalfNormal(), 0, 3, 0.8);\n\n\n\n\n\n\n\n\nWhen we want to avoid values too close to zero, other distributions like Gamma or InverseGamma may be a better choice.\n\npz.maxent(pz.Gamma(), 0, 3, 0.8);\n\n\n\n\n\n\n\n\nFurthermore, we may have more information that just a plausible range. We could also has extra restrictions like knowledge about the mean or mode. Let’s say we think a mean of 2 is very likely, then we can take advantage of the fact that the Gamma can also be parametrized in terms of the mean and pass the distribution pz.Gamma(mu=2). If instead we think is the mode the one is likely to be 2, then maxent has a mode argument we can use.\n\ndist_mean = pz.Gamma(mu=2)\npz.maxent(dist_mean, 0, 3, 0.8)\n\ndist_mode = pz.Gamma()\npz.maxent(dist_mode, 0, 3, 0.8, mode=2);\n\n\n\n\n\n\n\n\nNotice that if you call maxent several times in the same cell, as we just did, we will get all the distributions in the same plot. This can be very useful to visually compare several alternatives.\nThe function maxent as others in PreliZ modify distribution in place, so a common workflow is to instantiate a distribution first, perform the elicitation, and then inspect its properties, plot it, or use it in some other way. For instance, we may want to check a summary of some of its properties:\n\ndist_mean.summary(), dist_mode.summary()\n\n/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/numba/np/ufunc/dufunc.py:287: RuntimeWarning: invalid value encountered in nb_logpdf\n  return super().__call__(*args, **kws)\n/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/numba/np/ufunc/dufunc.py:287: RuntimeWarning: invalid value encountered in nb_logpdf\n  return super().__call__(*args, **kws)\n\n\n(Gamma(mean=2.0, median=1.67, std=1.43, lower=0.26, upper=5.39),\n Gamma(mean=2.32, median=2.22, std=0.86, lower=0.99, upper=4.19))",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Prior Elicitation</span>"
    ]
  },
  {
    "objectID": "Chapters/Prior_elicitation.html#other-direct-elicitation-methods-from-preliz",
    "href": "Chapters/Prior_elicitation.html#other-direct-elicitation-methods-from-preliz",
    "title": "8  Prior Elicitation",
    "section": "8.7 Other direct elicitation methods from PreliZ",
    "text": "8.7 Other direct elicitation methods from PreliZ\nThere are many other method for direct elicitation of parameters. For instance the quartile functions identifies a distribution that matches specified quartiles, and Quartine_int provides an interactive approach to achieve the same, offering a more hands-on experience for refining distributions.\nOne method worth of special mention is the Roulette method allows which allows users to find a prior distribution by drawing it interactively (Morris, Oakley, and Crowe 2014). The name “roulette” comes from the analogy of placing a limited set of chips where one believes the mass of a distribution should be concentrated. In this method, a grid of m equally sized bins is provided, covering the range of x, and users allocate a total of n chips across the bins. Effectively, this creates a histogram,representing the user’s information about the distribution. The method then identifies the best-fitting distribution from a predefined pool of options, translating the drawn histogram into a suitable probabilistic model.\nAs this is an interactive method we can’t show it here, but you can run the following cell to see how it works.\n\n%matplotlib widget\nresult = pz.Roulette()\n\nAnd this gif should give you an idea on how to use it.\n\n\n\n\n\n\nFigure 8.10: To elicit a distribution, we can interactively draw a histogram, and Roulette will identify the distribution that best matches it.\n\n\n\nOnce we have elicited the distribution we can call .dist attribute to get the selected distribution. In this example, it will be result.dist.\nIf needed, we can combine results for many independent “roulette sessions” with the combine_roulette function. Combining information from different elicitation sessions can be useful to aggregate information from different domain experts. Or even from a single person unable to pick a single option. For instance if we run Roulette twice, and for the first one we get result0 and for the second result1. Then, we can combine both solutions into a single one using:\n\npz.combine_roulette([result0.inputs, result1.inputs], weights=[0.3, 0.7])\n\nIn this example, we assign a larger weight to the results from the second elicitation session, we can do this to reflect uneven degrees of trust. By default, all sessions are weighted equally.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Prior Elicitation</span>"
    ]
  },
  {
    "objectID": "Chapters/Prior_elicitation.html#predictive-elicitation",
    "href": "Chapters/Prior_elicitation.html#predictive-elicitation",
    "title": "8  Prior Elicitation",
    "section": "8.8 Predictive elicitation",
    "text": "8.8 Predictive elicitation\nThe simplest way to perform predictive elicitation is to generate a model, sample from its prior predictive distribution and then evaluate if the samples are consistent with the domain knowledge. If there is disagreement, we can refine the prior distribution and repeat the process. This is usually known as prior predictive check and we discussed them in ?sec-sec-model-criticism together with posterior predictive checks.\nTo assess the agreement between the domain knowledge and the prior predictive distribution we may be tempted to use the observed data, as in posterior predictive checks. But, this can be problematic in many ways. Instead, we recommend using “reference values”. We can obtain a reference value from domain knowledge, like previous studies, asking clients or experts, or educated guesses. They can be typical values, or usually “extreme” values. For instance, if we are studying the temperature of a city, we may use the historical record of world temperature and use -90 as the minimum, 60 as the maximum and 15 as the average. These are inclusive values. Hence this will lead us to very broad priors. If we want something tighter we should use historical records of areas more similar to the city we are studying or even the same city we are studying. These will lead to more informative priors.\n\n8.8.1 Predator vs prey example\nWe are interested in modelling the relationship between the masses of organisms that are prey and organisms that are predators, and since masses vary in orders of magnitude from a 1e-9 grams for a typical cell to a 1.3e8 grams for the blue whale, it is convenient to work on a logarithmic scale.\nLet’s load the data and define the reference values.\n\npp_mass = pd.read_csv(\"../data/pp_mass.csv\")\npp_mass[\"predator_log\"] = np.log(pp_mass[\"predator\"])\npp_mass[\"prey_log\"] = np.log(pp_mass[\"prey\"])\n\n\n# Reference values in log-scale\nrefs = {\"Blue whale\":np.log(1.3e8),\n       \"Typical cell\":np.log(1e-9)}\n\nSo a model might be something like:\n\\[\\begin{align}\n   \\mu =& Normal(\\cdots, \\cdots) \\\\\n   \\sigma =& HalfNormal(\\cdots) \\\\\n   log(mass) =& Normal(\\mu, \\sigma)\n\\end{align}\\]\nLet’s now define a model with some priors and see what these priors imply on the scale of the data. To sample from the predictive prior we use pm.sample_prior_predictive() instead of sample and we need to define dummy observations. This is necessary to indicate to PyMC which term is the likelihood and to control the size of each predicted distribution, but the actual values do not affect the prior predictive distributions.\n\nwith pm.Model() as model:\n   α = pm.Normal(\"α\", 0, 100)\n   β = pm.Normal(\"β\", 0, 100)\n   σ = pm.HalfNormal(\"σ\", 5)\n   pm.Normal(\"prey\", α + β * pp_mass[\"prey_log\"], σ, observed=pp_mass[\"predator_log\"])\n   idata = pm.sample_prior_predictive(samples=100)\n\nSampling: [prey, α, β, σ]\n\n\nNow we can plot the prior predictive distribution and compare it with the reference values.\n\nax = az.plot_ppc(idata, group=\"prior\", kind=\"cumulative\", mean=False, legend=False)\n\n\nfor key, val in refs.items():\n   ax.axvline(val, ls=\"--\", color=\"0.5\")\n   ax.text(val-7, 0.5-(len(key)/100), key, rotation=90)\n\n\n\n\n\n\n\n\nPriors are so vague that we can not even distinguish the reference values from each other. Let’s try refining our priors.\n\nwith pm.Model() as model:\n   α = pm.Normal(\"α\", 0, 1)\n   β = pm.Normal(\"β\", 0, 1)\n   σ = pm.HalfNormal(\"σ\", 5)\n   prey = pm.Normal(\"prey\", α + β * pp_mass[\"prey_log\"], σ, observed=pp_mass[\"predator_log\"])\n   idata = pm.sample_prior_predictive(samples=100)\n\nSampling: [prey, α, β, σ]\n\n\nWe can plot the prior predictive distribution and compare it with the reference values.\n\nax = az.plot_ppc(idata, group=\"prior\", kind=\"cumulative\", mean=False, legend=False)\n\n\nfor key, val in refs.items():\n   ax.axvline(val, ls=\"--\", color=\"0.5\")\n   ax.text(val-7, 0.5-(len(key)/100), key, rotation=90)\n\n\n\n\n\n\n\n\nThe new priors still generate some values that are too wide, but at least the bulk of the model predictions are in the right range. So, without too much effort and extra information, we were able to move from a very vague prior to a weakly informative prior. If we decided this prior is still very vague we can add more domain-knowledge.\n\n\n8.8.2 Interactive predictive elicitation\nThe process described in the previous section is straightforward: sample from the prior predictive –&gt; plot –&gt; refine –&gt; repeat. On the good side, this is a very flexible approach and can be a good way to understand the effect of individual parameters in the predictions of a model. But it can be time-consuming and it requires some understanding of the model so you know which parameters to tweak and in which direction.\nOne way to improve this workflow is by adding interactivity. We can do this with PreliZ’s function, predictive_explorer. Which we can not show here, in a full glory but you can see an static image in Figure 8.11, and you can try it for yourself by running the following block of code.\n\ndef pp_model(α_μ=0, α_σ=100, β_μ=0, β_σ=100, σ_σ=5):\n   α = pz.Normal(α_μ, α_σ).rvs()\n   β = pz.Normal(β_μ, β_σ).rvs()\n   σ = pz.HalfNormal(σ_σ).rvs()\n   prey = pz.Normal(α + β * pp_mass.predator_log, σ).rvs()\n   return prey\n\npz.predictive_explorer(pp_model, references=refs)\n\n\n\n\n\n\n\nFigure 8.11: We can use the boxes to specify different prior values and see how the prior predictive changes, here we have changed the initial values of α_σ and β_σ from 100 to 1",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Prior Elicitation</span>"
    ]
  },
  {
    "objectID": "Chapters/Prior_elicitation.html#projective-predictive-elicitation",
    "href": "Chapters/Prior_elicitation.html#projective-predictive-elicitation",
    "title": "8  Prior Elicitation",
    "section": "8.9 Projective predictive elicitation",
    "text": "8.9 Projective predictive elicitation\nProjective predictive elicitation is an experimental method to elicit priors by specifying an initial model and a prior predictive distribution. Then instead of elicitate the prior themselves, we elicit the prior predictive distribution, which we call the target distribution. Then we use a procedure that automatically find the parameters of the prior that induce a prior predictive distribution that is as close as possible to the target distribution. This method is particularly useful when we have a good idea of how the data should look like, but we are not sure how to translate this into a prior distribution.\nThis method has been implemented in PreliZ, let see one example first and then discuss some details. To keep things concrete and familiar let’s assume we are still interested in the predator-prey example. And let assume that on a log-scale we think that the the prior predictive distribution is well described as a Normal distribution with most of its mass between the weight of a typical cell and the weight of a blue whale. The Normal is easy to work with and with this information we could derive its parameter. But let do something even easier. Let use pz.maxent and translate “most of it mass” to \\(0.98\\).\n\ntarget = pz.Normal()\npz.maxent(target, refs[\"Typical cell\"], refs[\"Blue whale\"], 0.98) \n\n\n\n\n\n\n\n\nThis will be our target distribution. If for a particular problem you are unsure about what distribution to choose as a target, think that this should be the distribution that you expect to match in a prior predictive check as discussed in previous sections. And also think that usually the goal is to find a weakly informative prior. So, the target will usually be a very approximate distribution.\nNow that we have the target, we write a model as you would do before a prior predictive check, we can reuse the model from previous section. The we pass the model and target to ppe\n\nwith pm.Model() as model:\n   α = pm.Normal(\"α\", 0, 100)\n   β = pm.Normal(\"β\", 0, 100)\n   σ = pm.HalfNormal(\"σ\", 5)\n   prey = pm.Normal(\"prey\", α + β * pp_mass[\"prey_log\"], σ, observed=pp_mass[\"predator_log\"])\n\nprint(pz.ppe(model, target)) \n\n/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/preliz/predictive/ppe.py:59: UserWarning: This method is experimental and under development with no guarantees of correctness.\n                  Use with caution and triple-check the results.\n  warnings.warn(\n\n\nwith pm.Model() as model:\n    α = pm.Normal(\"α\", mu=-1.03, sigma=0.0889)\n    β = pm.Normal(\"β\", mu=-0.000941, sigma=0.00619)\n    σ = pm.HalfNormal(\"σ\", sigma=8.47)\n\n\n\nOK, ppe function returns a solution, because this is an experimental method, and because good data analysis always keep a good dose of scepticism of their tools, let’s check that the suggested prior is reasonable given the provided information. To do this we write the model with the new prior and sample from the prior predictive. Notice that we could have copied the priors verbatim, but instead we are rounding them. We do not care of the exact solution and a rounded number looks easier to read. Feel free to not follow suggestion from machine blindly or be prepared to end inside a lake.\n\nwith pm.Model() as model:\n   α = pm.Normal(\"α\", mu=-1, sigma=0.09)\n   β = pm.Normal(\"β\", mu=0, sigma=0.006)\n   σ = pm.HalfNormal(\"σ\", sigma=8.5)\n   prey = pm.Normal(\"prey\", α + β * pp_mass[\"prey_log\"], σ, observed=pp_mass[\"predator_log\"])\n   idata = pm.sample_prior_predictive()\n\nSampling: [prey, α, β, σ]\n\n\nNow we can plot the prior predictive distribution, the target distribution and compare it with the reference values.\n\nax = az.plot_ppc(idata, group=\"prior\", kind=\"cumulative\", mean=False, legend=False)\ntarget.plot_cdf(color=\"C1\")\n\nfor key, val in refs.items():\n   ax.axvline(val, ls=\"--\", color=\"0.5\")\n   ax.text(val-7, 0.5-(len(key)/100), key, rotation=90)\n\n\n\n\n\n\n\n\nWe see very good agreement. Let’s do one more plot. This time we hide the individual prior predictive samples and instead plot the aggregated prior predictive distribution, labelled with “mean” as it is the average distribution when we average over all the computed prior predictive distributions.\n\nax = az.plot_ppc(idata, group=\"prior\", kind=\"cumulative\", mean=True, legend=False, alpha=0.001)\ntarget.plot_cdf(color=\"black\")\n\nfor key, val in refs.items():\n   ax.axvline(val, ls=\"--\", color=\"0.5\")\n   ax.text(val-7, 0.5-(len(key)/100), key, rotation=90)\n\n\n\n\n\n\n\n\nAs before we point out that this prior is still relatively vague, and we could use more information to tighten up. But what we care at this point is that the prior we got is consistent with the target distribution, and thus a prior that is coherent the domain-knowledge information that we decide to use.\n\n8.9.1 OK, but what’s under the hood?\nThe main idea is that once we have a target distribution we can define the prior elicitation problem as finding the parameters of the model that induce a prior predictive distribution as close as possible to the target distribution. Stated this way, this is a typical inference problem that could be solved using standard (Bayesian) inference methods that instead of conditioning on observed data we condition on synthetic data, our target distribution. Conceptually that’s what ppe is doing, but we still have two plot twist ahead of us.\nThe procedure is as follows:\n\nGenerate a sample from the target distribution.\nMaximize the model’s likelihood wrt that sample (i.e. we find the parameters for a fixed “observation”).\nGenerate a new sample from the target distribution and find a new set of parameters.\nCollect the parameters, one per prior parameter in the original model.\nUse MLE to fit the optimized values to their corresponding families in the original model.\n\nInstead of using standard inference methods like MCMC in step 1-3 we are using projection inference. See ?sec-variable-selection for details. Essentially we are approximating a posterior using an optimization method. Yes, we say posterior, because from the inference point we are computing a posterior, once that we then will use as prior. On the last step, we use a second approximation, we backfit the projected posterior into standard distributions used by PPLs as building blocks. We need to do this so we can write the resulting priors in terms a PPLs like PyMC, PyStan could understand.\nThis procedure ignores the prior information in the model passed to ppe, because the optimized function is just the likelihood. In the last step we use the information about each prior families. But in principle, we could even ignore this information and fit the optimized values to many families and pick the best fit. This allows the procedure to suggest alternative families. For instance, it could be that we use a Normal for a given parameters but the optimization only found positive values so a Gamma or HalfNormal could be a better choice. Having said that, the prior can have an effect because they are used to initialize the optimization routine. But for that to happen the prior has to be very off with respect to the target. Internally ppe performs many optimizations each time for a different sample from the target, the result of one optimization is stored as one projected posterior “sample” and also used as the initial guess for the next one. For the very first optimization, the one initialized from the prior, the result is discarded and only used as the initial guess for the next step.Another piece of information that is ignored is the observed data, the procedure only takes into account the sample size, but not the actual values. So you can pass dummy values, changing the sample size can we used to obtain more narrow (larger sample size) or more vague (smaller sample size) priors. Whether we should always use the same sample size of the data we are going to actually use or not is something that needs further research and evaluation.\n\n\n\n\nGelman, Andrew, Daniel Simpson, and Michael Betancourt. 2017. “The Prior Can Often Only Be Understood in the Context of the Likelihood.” Entropy 19 (10): 555. https://doi.org/10.3390/e19100555.\n\n\nIcazatti, Alejandro, Oriol Abril-Pla, Arto Klami, and Osvaldo A Martin. 2023. “PreliZ: A tool-box for prior elicitation.” Journal of Open Source Software 8 (89): 5499. https://doi.org/10.21105/joss.05499.\n\n\nJaynes, E. T. 2003. Probability Theory: The Logic of Science. Edited by G. Larry Bretthorst. Cambridge, UK ; New York, NY: Cambridge University Press.\n\n\nMikkola, Petrus, Osvaldo A. Martin, Suyog Chandramouli, Marcelo Hartmann, Oriol Abril Pla, Owen Thomas, Henri Pesonen, et al. 2024. “Prior Knowledge Elicitation: The Past, Present, and Future.” Bayesian Analysis 19 (4): 1129–61. https://doi.org/10.1214/23-BA1381.\n\n\nMorris, David E., Jeremy E. Oakley, and John A. Crowe. 2014. “A Web-Based Tool for Eliciting Probability Distributions from Experts.” Environmental Modelling & Software 52: 1–4. https://doi.org/https://doi.org/10.1016/j.envsoft.2013.10.010.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Prior Elicitation</span>"
    ]
  },
  {
    "objectID": "Chapters/Presenting_results.html",
    "href": "Chapters/Presenting_results.html",
    "title": "9  Presentation of results",
    "section": "",
    "text": "Summarizing results\nMonte Carlo Standard error and accuracy",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Presentation of results</span>"
    ]
  },
  {
    "objectID": "Chapters/Bayesian_workflow.html",
    "href": "Chapters/Bayesian_workflow.html",
    "title": "10  Bayesian Workflow",
    "section": "",
    "text": "10.1 A picture of a Bayesian Workflow\nFigure 10.1 shows a simplified Bayesian workflow (Martin, Kumar, and Lao 2021), check the Bayesian Workflow paper for a more detailed representation (Gelman et al. 2020). As you see there are many steps. We need all these steps because models are just lucubrations of our mind with no guarantee of helping us understand the data. We need to first be able to build such a model and then check its usefulness, and if not useful enough keep working, or sometimes stop trying. You may also have noticed the “evaluate samples” step. We need this because we, usually, use computational methods to solve Bayesian models, and here again we have no guarantee these methods always return the correct result (see Chapter 4 for details).\nDesigning a suitable model for a given data analysis task usually requires a mix of statistical expertise, domain knowledge, understanding of computational tools, and perseverance. Rarely a modelling effort is a one-shot process, instead, typically we need to iteratively write, test, and refine models. If you are familiar with writing code, then you already know what we are talking about. Even very short programs require some trial and error. Usually, you need to test it, debug it, and refine it, and sometimes try alternative approaches. The same is true for statistical models, especially when we use code to write and solve them.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bayesian Workflow</span>"
    ]
  },
  {
    "objectID": "Chapters/Bayesian_workflow.html#a-picture-of-a-bayesian-workflow",
    "href": "Chapters/Bayesian_workflow.html#a-picture-of-a-bayesian-workflow",
    "title": "10  Bayesian Workflow",
    "section": "",
    "text": "Figure 10.1: A Bayesian workflow. Solid lines show a linear workflow starting at problem framing and ending in summarizing the results. The dotted lines indicate that workflows usually are non-linear as practitioners usually skip steps or go back to previous steps.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bayesian Workflow</span>"
    ]
  },
  {
    "objectID": "Chapters/Bayesian_workflow.html#a-blueprint-for-effective-bayesian-workflows",
    "href": "Chapters/Bayesian_workflow.html#a-blueprint-for-effective-bayesian-workflows",
    "title": "10  Bayesian Workflow",
    "section": "10.2 A Blueprint for Effective Bayesian Workflows",
    "text": "10.2 A Blueprint for Effective Bayesian Workflows\nOften, especially for newcomers, Bayesian analysis can be overwhelming. In this section, we have collected a series of tips and recommendations so you can get a quick reference. Here we write the recommendations linearly, but in practice, you may need to come back one or more steps and sometimes skip steps. Think of these notes, not as a partiture of a classical piece that a violinist aims at playing almost exactly, but as the musical score that a Jazz bassist follows, you are free to improvise, rearrange some parts, and omit others, and you can even add your notes!\n\n10.2.1 Summarize the problem\nSummarize the key points of your problem, and what you would like to learn from the data. Think also about others, what your boss, client, or colleague would like to find out or learn. This does not need to be super thorough, you can revisit goals later, but they can help you organize your modelling efforts and avoid excessive wandering.\nSometimes you will not have any clear idea of what to expect or what to do, your only expectation will be to get something useful from a dataset, and that’s fine. But other times you may even know what kind of model you want, perhaps your boss explicitly asked you to run this or that analysis. If you already know what kind of problem you want to solve, but are not very familiar with the approach, search for what methods, metrics or visualizations are common for that problem/data, and ask others for advice. This is more important the less familiar you are with that type of problem/data. If you are familiar, then you may already know which methods, visualizations, and summaries you want to use or obtain. Either way write an outline, a roadmap to help you keep focus, and later to track what you have already tried.\n\n\n10.2.2 Get familiar with the data\nIt is always a good idea to perform Exploratory Data Analysis on your data. Blindly modelling your data leads you to all sorts of problems. Taking a look first will save you time and may provide useful ideas. Sometimes it saves you from having to write a Bayesian model at all, perhaps the answer is a scatter plot! In the early stages, a quick dirty plot could be enough but try to be organized as you may need to refer to these plots later on during the modelling or presentation phases.\nWhen exploring the data we want to make sure, we get a really good understanding of it. How to achieve this can vary a lot from dataset to dataset and from analysis to analysis. But there are useful sanity checks that we usually do, like checking for missing values, or errors in the data. Are the data types correct? Are all the values that should be numbers, numbers (usually integers or floats) or they are strings? Which variables are categorical? Which ones are continuous? At this stage, you may need to do some cleaning of your data. This will save you time in the future.\nUsually, we would like to also do some plots, histograms, boxplots, scatter plots, etc. Numerical summaries are also useful, like the mean, and median, for all your data, or by grouping the data, etc.\n\n\n10.2.3 Tell a story for the data\nIt is often helpful to think about how the data could have been generated. This is usually called the data-generating process or data-generating mechanism. We don’t need to find out the True mechanism, many times we just need to think about plausible scenarios.\nMake drawings, and try to be very schematic, doodles and geometrical figures should be enough unless you are a good sketcher. This step can be tricky, so let us use an example. Let’s say you are studying the water levels of a lake, think about what makes the water increase; rain, rivers, etc, and what makes it decrease; evaporation, animals drinking water, energy production, etc. Try to think which elements may be relevant and which could be negligible. Use as much context as you have for your problem. If you feel you don’t have enough context, write down questions and find out who knows.\nTry to keep it simple but not simpler. For instance, a mechanism could be “Pigs’ weight increases the more corn they are fed”, that’s a good mechanism if all you need to predict are your earnings from selling pigs. But it will be an over-simplistic mechanism if you are studying intestine absorption at the cellular level.\nIf you can think of alternative stories and you don’t know how to decide which one is better. Don’t worry, list them all! Maybe we can use the data to decide!\n\n\n10.2.4 Write a model\nTry to translate the data-generating mechanism into a model. If you feel comfortable with math, use that. If you prefer a visual representation like a graphical model, use that. If you like code, then go for it. Incomplete models are fine as a first step. For instance, if you use code, feel free to use pseudo code or add comments to signal missing elements as you think about the model. You can refine it later. A common blocker is trying to do too much too soon.\nTry to start simple, don’t use hierarchies, keep prior 1D (instead of multivariate), skip interactions for linear models, etc. If for some reason you come first with a complex model, that’s ok, but you may want to save it for later use, and try with a simplified version.\nSometimes you may be able to use a standard textbook model or something you saw on a blog post or a talk. It is common that for certain problems people tend to use certain “default” models. That may be a good start, or your final model. Keep things simple, unless you need something else.\nThis is a good step to think about your priors (see Chapter 8 for details), not only which family are you going to use, but what specific parameters. If you don’t have a clue just use some vague prior. But if you have some information, use it. Try to encode very general information, like this parameter can not be negative, or this parameter is likely to be smaller than this, or within this range. Look for the low-hanging fruit, usually that will be enough. The exception will be when you have enough good quality information to define a very precise prior, but even then, that’s something you can add later.\n\n\n10.2.5 Implement the model\nWrite the model in a probabilistic programming language. If you used code in the previous example the line between this step and the previous one, may be diffuse, that’s fine. Try to keep the model simple at first, we can add more layers later as we keep iteration through this workflow. Starting simple usually saves you time in the long run. Simple models are easier to debug and debugging one issue at a time is generally less frustrating than having to fix several issues before our model even runs.\nOnce you have a model, check that the model compiles and/or runs without error. When debugging a model, especially at an earlier stage of the workflow, you may want to reduce the number of tuning and sampling steps, at the beginning a crude posterior approximation is usually enough. Sometimes, it may also be a good idea to reduce the size of the dataset. For large datasets setting aside 50 or 90% of the data could help iterate faster and catch errors earlier. A potential downside is that you may miss the necessary data to uncover some relevant pattern but it could be ok at the very beginning when most of the time is spent fixing simple mistakes or getting familiar with the problem.\n\n\n10.2.6 Evaluate prior predictive distribution\nIt is usually a good idea to generate data from the prior predictive distribution and compare that to your prior knowledge (mikkola_2023?). Is the bulk of the simulated distribution in a reasonable range? Are there any extreme values? Use reference values as a guide. Reference values are empirical data or historical observations, usually, they will be minimum, maximum or expected values. Avoid comparing with the observed data, as that can lead to issues if you are not careful enough (see Chapter 8 for details).\n\n\n10.2.7 Compute posterior\nThere are many ways to compute the posterior, in this document, we have assumed the use of MCMC methods as they are the most general and commonly used methods to estimate the posterior in modern Bayesian analysis.\n\n\n10.2.8 Evaluate samples\nWhen using MCMC methods, we need to check that the samples are good enough. For this, we need to compute diagnostics such as \\(\\hat R\\) (r-hat) and effective sample size (ESS). And evaluate plots such as trace plots and rank plots. We can be more tolerant with diagnostics at the early stages of the workflow, for instance, an \\(\\hat R\\) of 1.1 is acceptable. At the same time, very bad diagnostics could be a signal of a problem with our model(s). We discuss these steps in detail in Chapter 4.\n\n\n10.2.9 Validate the model\nThere are many ways to validate your model, like a posterior predictive check, Bayesian p-values, residual analysis, and recovery parameters from synthetic data (or the most costly simulated-based calibration). Or a combination of all of this. Sometimes you may be able to use a holdout set to evaluate the predictive performance of your model. The main goal here is to find if the model is good enough for your purpose and what limitations the model can have. All models will have limitations, but some limitations may be irrelevant in the context of your analysis, some may be worth removing by improving the models, and others are simply worth knowing they are there. We discuss these steps in detail in Chapter 4.\n\n\n10.2.10 Compare models\nIf you manage to get more than one model (usually a good idea), you may need to define which one you would like to keep (assuming you only need one). To compare models you can use cross-validation and/or information criteria. But you can also use the results from the previous step (model validation). Sometimes we compare models to keep a single model, model comparison can also help us to better understand a model, its strengths and its limitations, and it can also be a motivation to improve a model or try a new one. Model averaging, i.e. combining several models, is usually a simple and effective strategy to improve predictive performance. We discuss these steps in detail in Chapter 6.\n\n\n10.2.11 Summarize results\nSummarize results in a way that helps you reach your goals, did you manage to answer the key questions? Is this something that will convince your boss, your peers or the marketing department? Think of effective ways to show the results. If your audience is very technical do a technical summary, but if your audience only cares about maximizing profit focus on that. Try to use summaries that are easy to understand without hiding valuable details, you don’t want to mislead your audience.\n\n\n\n\nGelman, Andrew, Aki Vehtari, Daniel Simpson, Charles C. Margossian, Bob Carpenter, Yuling Yao, Lauren Kennedy, Jonah Gabry, Paul-Christian Bürkner, and Martin Modrák. 2020. “Bayesian Workflow.” https://arxiv.org/abs/2011.01808.\n\n\nMartin, Osvaldo A., Ravin Kumar, and Junpeng Lao. 2021. Bayesian Modeling and Computation in Python. 1st edition. Boca Raton London New York: Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bayesian Workflow</span>"
    ]
  },
  {
    "objectID": "Chapters/References.html",
    "href": "Chapters/References.html",
    "title": "References",
    "section": "",
    "text": "Akaike, H. 1974. “A New Look at the Statistical Model\nIdentification.” IEEE Transactions on Automatic Control\n19 (6): 716–23. https://doi.org/10.1109/TAC.1974.1100705.\n\n\nBessiere, Pierre, Emmanuel Mazer, Juan Manuel Ahuactzin, and Kamel\nMekhnacha. 2013. Bayesian Programming. 1 edition.\nBoca Raton: Chapman; Hall/CRC. https://www.crcpress.com/Bayesian-Programming/Bessiere-Mazer-Ahuactzin-Mekhnacha/p/book/9781439880326.\n\n\nBrockmann, H. Jane. 1996. “Satellite Male Groups in Horseshoe\nCrabs, Limulus Polyphemus.” Ethology 102 (1): 1–21.\nhttps://doi.org/https://doi.org/10.1111/j.1439-0310.1996.tb01099.x.\n\n\nCleveland, William S., and Robert McGill. 1984. “Graphical\nPerception: Theory, Experimentation, and Application to the Development\nof Graphical Methods.” Journal of the American Statistical\nAssociation 79 (387): 531–54. https://doi.org/10.1080/01621459.1984.10478080.\n\n\nDaniel Roy. 2015. Probabilistic Programming. http://probabilistic-programming.org.\n\n\nDiaconis, Persi. 2011. “Theories of Data\nAnalysis: From Magical\nThinking Through Classical\nStatistics.” In Exploring Data\nTables, Trends, and Shapes,\n1–36. John Wiley & Sons, Ltd. https://doi.org/10.1002/9781118150702.ch1.\n\n\nGelman, Andrew, Daniel Simpson, and Michael Betancourt. 2017. “The\nPrior Can Often Only\nBe Understood in the Context of\nthe Likelihood.” Entropy 19 (10): 555. https://doi.org/10.3390/e19100555.\n\n\nGelman, Andrew, Aki Vehtari, Daniel Simpson, Charles C. Margossian, Bob\nCarpenter, Yuling Yao, Lauren Kennedy, Jonah Gabry, Paul-Christian\nBürkner, and Martin Modrák. 2020. “Bayesian Workflow.” https://arxiv.org/abs/2011.01808.\n\n\nGhahramani, Zoubin. 2015. “Probabilistic Machine\nLearning and Artificial\nIntelligence.” Nature 521 (7553): 452–59.\nhttps://doi.org/10.1038/nature14541.\n\n\nGreenhill, Brian, Michael D. Ward, and Audrey Sacks. 2011. “The\nSeparation Plot: A New Visual Method for Evaluating the Fit of Binary\nModels.” American Journal of Political Science 55 (4):\n991–1002. https://doi.org/https://doi.org/10.1111/j.1540-5907.2011.00525.x.\n\n\nHeer, Jeffrey, and Michael Bostock. 2010. “Crowdsourcing Graphical\nPerception: Using Mechanical Turk to Assess Visualization\nDesign.” In Proceedings of the SIGCHI\nConference on Human Factors in\nComputing Systems, 203–12.\nCHI ’10. New York, NY, USA: Association for Computing\nMachinery. https://doi.org/10.1145/1753326.1753357.\n\n\nHoyer, Stephan, and Joe Hamman. 2017. “Xarray:\nN-D Labeled Arrays\nand Datasets in Python.” Journal of\nOpen Research Software 5 (1). https://doi.org/10.5334/jors.148.\n\n\nIcazatti, Alejandro, Oriol Abril-Pla, Arto Klami, and Osvaldo A Martin.\n2023. “PreliZ: A tool-box for prior\nelicitation.” Journal of Open Source Software 8\n(89): 5499. https://doi.org/10.21105/joss.05499.\n\n\nJaynes, E. T. 2003. Probability Theory:\nThe Logic of Science. Edited\nby G. Larry Bretthorst. Cambridge, UK ; New York, NY: Cambridge\nUniversity Press.\n\n\nKallioinen, Noa, Topi Paananen, Paul-Christian Bürkner, and Aki Vehtari.\n2023. “Detecting and Diagnosing Prior and Likelihood Sensitivity\nwith Power-Scaling.” Statistics and Computing 34 (1):\n57. https://doi.org/10.1007/s11222-023-10366-5.\n\n\nKleiber, Christian, and Achim Zeileis. 2016. “Visualizing Count\nData Regressions Using Rootograms.” The American\nStatistician 70 (3): 296–303. https://doi.org/10.1080/00031305.2016.1173590.\n\n\nLink, William A., and Mitchell J. Eaton. 2012. “On Thinning of\nChains in MCMC.” Methods in Ecology and Evolution 3 (1):\n112–15. https://doi.org/https://doi.org/10.1111/j.2041-210X.2011.00131.x.\n\n\nMacEachern, Steven N., and L. Mark Berliner. 1994. “Subsampling\nthe Gibbs Sampler.” The American\nStatistician 48 (3): 188–90. https://doi.org/10.2307/2684714.\n\n\nMartin, Osvaldo A., Ravin Kumar, and Junpeng Lao. 2021. Bayesian\nModeling and Computation in\nPython. 1st edition. Boca Raton London New York:\nChapman; Hall/CRC.\n\n\nMikkola, Petrus, Osvaldo A. Martin, Suyog Chandramouli, Marcelo\nHartmann, Oriol Abril Pla, Owen Thomas, Henri Pesonen, et al. 2024.\n“Prior Knowledge Elicitation: The Past,\nPresent, and Future.” Bayesian Analysis 19 (4):\n1129–61. https://doi.org/10.1214/23-BA1381.\n\n\nMorris, David E., Jeremy E. Oakley, and John A. Crowe. 2014. “A\nWeb-Based Tool for Eliciting Probability Distributions from\nExperts.” Environmental Modelling & Software 52:\n1–4. https://doi.org/https://doi.org/10.1016/j.envsoft.2013.10.010.\n\n\nSäilynoja, Teemu, Paul-Christian Bürkner, and Aki Vehtari. 2022.\n“Graphical Test for Discrete Uniformity and Its Applications in\nGoodness-of-Fit Evaluation and Multiple Sample Comparison.”\nStatistics and Computing 32 (2): 32. https://doi.org/10.1007/s11222-022-10090-6.\n\n\nTalts, Sean, Michael Betancourt, Daniel Simpson, Aki Vehtari, and Andrew\nGelman. 2020. “Validating Bayesian Inference Algorithms with\nSimulation-Based Calibration.” https://arxiv.org/abs/1804.06788.\n\n\nTukey, John W. 1977. Exploratory Data\nAnalysis. 1 edition. Pearson.\n\n\nVehtari, Aki, Andrew Gelman, and Jonah Gabry. 2017. “Practical\nBayesian Model Evaluation Using Leave-One-Out Cross-Validation and\nWAIC.” Statistics and Computing 27 (5): 1413–32. https://doi.org/10.1007/s11222-016-9696-4.\n\n\nVehtari, Aki, Andrew Gelman, Daniel Simpson, Bob Carpenter, and\nPaul-Christian Bürkner. 2021. “Rank-Normalization, Folding, and Localization: An\nImproved R̂ for Assessing\nConvergence of MCMC (with Discussion).” Bayesian\nAnalysis 16 (2): 667–718. https://doi.org/10.1214/20-BA1221.\n\n\nWatanabe, Sumio. 2013. “A Widely\nApplicable Bayesian Information\nCriterion.” Journal of Machine Learning\nResearch 14 (March): 867–97.\n\n\nYao, Yuling, Aki Vehtari, Daniel Simpson, and Andrew Gelman. 2018.\n“Using Stacking to Average Bayesian\nPredictive Distributions (with Discussion).” Bayesian\nAnalysis 13 (3): 917–1007. https://doi.org/10.1214/17-BA1091.",
    "crumbs": [
      "References"
    ]
  }
]