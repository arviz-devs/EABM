[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploratory Analysis of Bayesian Models",
    "section": "",
    "text": "‎\nWhile conceptually simple, Bayesian methods can be mathematically and numerically challenging. Probabilistic programming languages (PPLs) implement functions to easily build Bayesian models together with efficient automatic inference methods. This helps separate the model building from the inference, allowing practitioners to focus on their specific problems and leaving the PPLs to handle the computational details for them (Bessiere et al. 2013; Daniel Roy 2015; Ghahramani 2015). The inference process generates a posterior distribution - which has a central role in Bayesian statistics - together with other distributions like the posterior predictive distribution and the prior predictive distribution. The correct visualization, analysis, and interpretation of these distributions is key to properly answer the questions that motivated the inference process.\nWhen working with Bayesian models there are a series of related tasks that need to be addressed besides inference itself:\n\nDiagnoses of the quality of the inference (as this is generally done using numerical approximation methods)\nModel criticism, including evaluations of both model assumptions and model predictions\nComparison of models, including model selection or model averaging\nPreparation of the results for a particular audience\n\nWe collectivelly call all these tasks Exploratory analysis of Bayesian model, as we take many ideas from Exploratory data analysis and apply them to analyze Bayesian modesl.\nIn the words of Persi Diaconis (Diaconis 2011):\n\n“Exploratory data analysis seeks to reveal structure, or simple descriptions in data. We look at numbers or graphs and try to find patterns. We pursue leads suggested by background information, imagination, patterns perceived, and experience with other data analyses”.\n\nIn this book we will discuss how to use both numerical and visual summaries to help successfully performing such tasks that are central to the iterative and interactive modeling process. To do so, we first discuss some general principles of data visualization and uncertainty representation that are not exclusibe of Bayesian statistics.\n\n\n\n\nBessiere, Pierre, Emmanuel Mazer, Juan Manuel Ahuactzin, and Kamel Mekhnacha. 2013. Bayesian Programming. 1 edition. Boca Raton: Chapman; Hall/CRC. https://www.crcpress.com/Bayesian-Programming/Bessiere-Mazer-Ahuactzin-Mekhnacha/p/book/9781439880326.\n\n\nDaniel Roy. 2015. Probabilistic Programming. http://probabilistic-programming.org.\n\n\nDiaconis, Persi. 2011. “Theories of Data Analysis: From Magical Thinking Through Classical Statistics.” In Exploring Data Tables, Trends, and Shapes, 1–36. John Wiley & Sons, Ltd. https://doi.org/10.1002/9781118150702.ch1.\n\n\nGhahramani, Zoubin. 2015. “Probabilistic Machine Learning and Artificial Intelligence.” Nature 521 (7553): 452–59. https://doi.org/10.1038/nature14541."
  },
  {
    "objectID": "elements_of_visualization.html#coordinate-systems-and-axes",
    "href": "elements_of_visualization.html#coordinate-systems-and-axes",
    "title": "1  Elements of visualization",
    "section": "1.1 Coordinate systems and axes",
    "text": "1.1 Coordinate systems and axes\nData visualization requires defining position scales to determine where different data values are located in a graphic. In 2D visualizations, two numbers are required to uniquely specify a point. Thus, we need two position scales. The arrangement of these scales is known as a coordinate system. The most common coordinate system is the 2D Cartesian system, using x and y values with orthogonal axes. Conventionally with the x-axis running horizontally and the y-axis vertically. Figure 1.1 shows a Cartesian coordinate system.\n\n\n\nFigure 1.1: Cartesian coordinate system\n\n\nIn practice, we typically shift the axes so that they do not necessarily pass through the origin (0,0), and instead their location is determined by the data. We do this because it is usually more convenient and easier to read to have the axes to the left and bottom of the figure than in the middle. For instance Figure 1.2 plots the exact same points shown in Figure 1.1 but with the axes placed automatically by matplotlib.\n\n\n\nFigure 1.2: Cartesian coordinate system with axes automatically placed by matplotlib based on the data\n\n\nUsually, data has units, such as degrees Celsius for temperature, centimeters for length, or kilograms for weight. In case we are plotting variables of different types (and hence different units) we can adjust the aspect ratio of the axes as we wish. We can make a figure short and wide if it fits better on a page or screen. But we can also change the aspect ratio to highlight important differences, for example, if we want to emphasize changes along the y-axis we can make the figure tall and narrow. When both the x and y axes use the same units, it’s important to maintain an equal ratio to ensure that the relationship between data points on the graph accurately reflects their quantitative values.\nAfter the cartesian coordinate system, the most common coordinate system is the polar coordinate system. In this system, the position of a point is determined by the distance from the origin and the angle with respect to a reference axis. Polar coordinates are useful for representing periodic data, such as days of the week, or data that is naturally represented in a circular shape, such as wind direction. Figure Figure 1.3 shows a polar coordinate system.\n\n\n\nFigure 1.3: Polar coordinate system"
  },
  {
    "objectID": "elements_of_visualization.html#plot-elements",
    "href": "elements_of_visualization.html#plot-elements",
    "title": "1  Elements of visualization",
    "section": "1.2 Plot elements",
    "text": "1.2 Plot elements\nTo convey visual information we generally use shapes, including lines, circles, squares, etc. These elements have properties associated with them like, position, shape, and color. In addition, we can add text to the plot to provide additional information.\nArviZ uses both matplotlib and bokeh as plotting backends. While for basic use of ArviZ is not necessary to know about these libraries, being familiar with them is useful to better understand some of the arguments in ArviZ’s plots and/or to tweak the default plots generated with ArviZ. If you need to learn more about these libraries we recommend the official tutorials for matplotlib and bokeh."
  },
  {
    "objectID": "elements_of_visualization.html#good-practices-and-sources-of-error",
    "href": "elements_of_visualization.html#good-practices-and-sources-of-error",
    "title": "1  Elements of visualization",
    "section": "1.3 Good practices and sources of error",
    "text": "1.3 Good practices and sources of error\nUsing visualization to deceive third parties should not be the goal of an intellectually honest person, and you must also be careful not to deceive yourself. For example, it has been known for decades that a bar chart is more effective for comparing values than a pie chart. The reason is that our perceptual apparatus is quite good at evaluating lengths, but not very good at evaluating areas. Figure 1.4 shows different visual elements ordered according to the precision with which the human brain can detect differences and make comparisons between them (Cleveland and McGill 1984; Heer and Bostock 2010).\n\n\n\nFigure 1.4: Scale of elementary perceptual tasks, taken from The Truthful Art\n\n\n\n1.3.1 General principles for using colors\nHuman eyes work by essentially perceiving 3 wavelengths, this feature is used in technological devices such as screens to generate all colors from combinations of 3 components, Red, Green, and Blue. This is known as the RGB color model. But this is not the only possible system. A very common alternative is the CYMK color model, Cyan, Yellow, Magenta, and Black.\nTo analyze the perceptual attributes of color, it is better to think in terms of Hue, Saturation, and Lightness, HSL is an alternative representation of the RGB color model.\nThe hue is what we colloquially call “different colors”. Green, red, etc. Saturation is how colorful or washed out we perceive a given color. Two colors with different hues will look more different when they have more saturation. The lightness corresponds to the amount of light emitted (active screens) or reflected (impressions), ranging from black to white:\nVarying the tone is useful to easily distinguish categories as shown in Figure 1.5.\n\n\n\nFigure 1.5: Tone varitaions can be help to distinguish categories.\n\n\nIn principle, most humans are capable of distinguishing millions of tones, but if we want to associate categories with colors, the effectiveness of distinguishing them decreases drastically as the number of categories increases. This happens not only because the tones will be increasingly closer to each other, but also because we have a limited working memory. Associating a few colors (say 4) with categories (countries, temperature ranges, etc.) is usually easy. But unless there are pre-existing associations, remembering many categories becomes challenging and this exacerbates when colors are close to each other. This requires us to continually alternate between the graphic and the legend or text where the color-category association is indicated. Adding other elements besides color such as shapes can help, but in general, it will be more useful to try to keep the number of categories relatively low. In addition, it is important to take into account the presentation context, if we want to show a figure during a presentation where we only have a few seconds to dedicate to that figure, it is advisable to keep the figure as simple as possible. This may involve removing items and displaying only a subset of the data. If the figure is part of a text, where the reader will have the time to analyze for a longer period, perhaps the complexity can be somewhat greater.\nAlthough we mentioned before that human eyes are capable of distinguishing three main colors (red, green, and blue), the ability to distinguish these 3 colors varies between people, to the point that many individuals have difficulty distinguishing some colors. The most common case occurs with red and green. This is why it is important to avoid using those colors. An easy way to avoid this problem is to use colorblind-friendly palettes. We’ll see later that this is an easy thing to do when using ArviZ.\nVarying the lightness as in Figure 1.6 is useful when we want to represent a continuous scale. With the hue-based palette (left), it’s quite difficult to determine that our data shows two “spikes”, whereas this is easier to see with the lightness-modifying palette (right). Varying the lightness helps to see the structure of the data since changes in lightness are more intuitively processed as quantitative changes.\n\n\n\nFigure 1.6: Hue-based palette (left) vs lightness-modifying palette (right)\n\n\nOne detail that we should note is that the graph on the right of Figure 1.6 does not change only the lightness, it is not a map in gray or blue scales. That palette also changes the hue but in a very subtle way. This makes it aesthetically more pleasing and the subtle variation in hue contributes to increasing the perceptual distance between two values and therefore the ability to distinguish small differences.\nWhen using colors to represent numerical variables it is important to use uniformly perceptual maps like those offered by matplotlib or colorcet. These are maps where the colors vary in such a way that they adequately reflect changes in the data. Not all colormaps are perceptually uniform. Obtaining them is not trivial. Figure 1.7 shows the same image using different colormaps. We can see that widely used maps such as jet (also called rainbow) generate distortions in the image. In contrast viridis, a perceptually uniform color map does not generate such distortions.\n\n\n\nFigure 1.7: non-uniformly perceptual maps like jet can be very missleading\n\n\nA common criticism of perceptually smooth maps is that they appear more “flat” or “boring” at first glance. And instead maps like Jet, show greater contrast. But that is precisely one of the problems with maps like Jet, the magnitude of these contrasts does not correlate with changes in the data, so even extremes can occur, such as showing contrasts that are not there and hiding differences that are truly there."
  },
  {
    "objectID": "elements_of_visualization.html#style-sheets",
    "href": "elements_of_visualization.html#style-sheets",
    "title": "1  Elements of visualization",
    "section": "1.4 Style sheets",
    "text": "1.4 Style sheets\nMatplotlib allows users to easily switch between plotting styles by defining style sheets. ArviZ is delivered with a few additional styles that can be applied globally by writing az.style.use(name_of_style) or inside a with statement.\n\naz.style.use('arviz-white')\nx = np.linspace(0, 1, 100)\ndist = pz.Beta(2, 5).pdf(x)\n\nfig = plt.figure()\nfor i in range(10):\n    plt.plot(x, dist - i, f'C{i}', label=f'C{i}')\nplt.xlabel('x')\nplt.ylabel('f(x)', rotation=0, labelpad=15);\n\n\n\n\nFigure 1.8: arviz-white style use a color-blind friendly palette\n\n\n\n\naz.style is just an alias of matplotlib.pyplot.style, so everything you can do with one of them you can do with the other.\nThe styles arviz-colors, arviz-white, arviz-darkgrid, and arviz-whitegrid use the same color-blind friendly palette. This palette was designed using colorcyclepicker. arviz-doc, the style used in ArviZ documentation, uses another color-blind friendly and it looks gorgeous on-screen and in printed documents, although it can be tricky for projections if the room is not dark enough.\nIf you need to do plots in grey-scale we recommend restricting yourself to the first 3 colors of the ArviZ default palette (“C0”, “C1” and “C2”), otherwise, you may need to use different line styles or different markers. Another useful palette when the number of colors is restricted is the grayscale which assigns greys to the first four colors (“C0”, “C1”, “C2” and “C3”) and the ArviZ’s blue to “C5”. Check the style gallery to see all the available ArviZ’s styles.\n\n\n\n\nCleveland, William S., and Robert McGill. 1984. “Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods.” Journal of the American Statistical Association 79 (387): 531–54. https://doi.org/10.1080/01621459.1984.10478080.\n\n\nHeer, Jeffrey, and Michael Bostock. 2010. “Crowdsourcing Graphical Perception: Using Mechanical Turk to Assess Visualization Design.” In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 203–12. CHI ’10. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/1753326.1753357."
  },
  {
    "objectID": "InferenceData.html#inferencedata",
    "href": "InferenceData.html#inferencedata",
    "title": "2  Working with InferenceData",
    "section": "2.1 InferenceData",
    "text": "2.1 InferenceData\nDuring a modern Bayesian analysis we usually generate many sets of data including posterior samples and posterior predictive samples. But we also have observed data, and statistics generated by the sampling method, samples from the prior and/or prior predictive distribution, etc. All this data can be stored in an InferenceData object, to keep them tidy and avoid confusion each one of these sets it is assigned to a different group. For instance, the posterior samples are stored in the posterior group. The observed data is stored in the observed_data group and so on.\nArviZ comes equipped with a few InferenceData objects so we can start playing with them even without the need to fit a model. Let’s start by loading the centered_eight InferenceData. For the moment we do not need to know about the details of the model.\n\nidata = az.load_arviz_data(\"centered_eight\")\nidata\n\n\n\n            \n              \n                arviz.InferenceData\n              \n              \n              \n            \n                  \n                  posterior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (chain: 4, draw: 500, school: 8)\nCoordinates:\n  * chain    (chain) int64 0 1 2 3\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 ... 492 493 494 495 496 497 498 499\n  * school   (school) &lt;U16 'Choate' 'Deerfield' ... \"St. Paul's\" 'Mt. Hermon'\nData variables:\n    mu       (chain, draw) float64 ...\n    theta    (chain, draw, school) float64 ...\n    tau      (chain, draw) float64 ...\nAttributes: (6)xarray.DatasetDimensions:chain: 4draw: 500school: 8Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (3)mu(chain, draw)float64...[2000 values with dtype=float64]theta(chain, draw, school)float64...[16000 values with dtype=float64]tau(chain, draw)float64...[2000 values with dtype=float64]Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))schoolPandasIndexPandasIndex(Index(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', 'St. Paul's', 'Mt. Hermon'],\n      dtype='object', name='school'))Attributes: (6)created_at :2022-10-13T14:37:37.315398arviz_version :0.13.0.dev0inference_library :pymcinference_library_version :4.2.2sampling_time :7.480114936828613tuning_steps :1000\n                      \n                  \n            \n            \n            \n                  \n                  posterior_predictive\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (chain: 4, draw: 500, school: 8)\nCoordinates:\n  * chain    (chain) int64 0 1 2 3\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 ... 492 493 494 495 496 497 498 499\n  * school   (school) &lt;U16 'Choate' 'Deerfield' ... \"St. Paul's\" 'Mt. Hermon'\nData variables:\n    obs      (chain, draw, school) float64 ...\nAttributes: (4)xarray.DatasetDimensions:chain: 4draw: 500school: 8Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (1)obs(chain, draw, school)float64...[16000 values with dtype=float64]Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))schoolPandasIndexPandasIndex(Index(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', 'St. Paul's', 'Mt. Hermon'],\n      dtype='object', name='school'))Attributes: (4)arviz_version :0.13.0.dev0created_at :2022-10-13T14:37:41.460544inference_library :pymcinference_library_version :4.2.2\n                      \n                  \n            \n            \n            \n                  \n                  log_likelihood\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (chain: 4, draw: 500, school: 8)\nCoordinates:\n  * chain    (chain) int64 0 1 2 3\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 ... 492 493 494 495 496 497 498 499\n  * school   (school) &lt;U16 'Choate' 'Deerfield' ... \"St. Paul's\" 'Mt. Hermon'\nData variables:\n    obs      (chain, draw, school) float64 ...\nAttributes: (4)xarray.DatasetDimensions:chain: 4draw: 500school: 8Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (1)obs(chain, draw, school)float64...[16000 values with dtype=float64]Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))schoolPandasIndexPandasIndex(Index(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', 'St. Paul's', 'Mt. Hermon'],\n      dtype='object', name='school'))Attributes: (4)arviz_version :0.13.0.dev0created_at :2022-10-13T14:37:37.487399inference_library :pymcinference_library_version :4.2.2\n                      \n                  \n            \n            \n            \n                  \n                  sample_stats\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:              (chain: 4, draw: 500)\nCoordinates:\n  * chain                (chain) int64 0 1 2 3\n  * draw                 (draw) int64 0 1 2 3 4 5 6 ... 494 495 496 497 498 499\nData variables: (12/16)\n    max_energy_error     (chain, draw) float64 ...\n    energy_error         (chain, draw) float64 ...\n    lp                   (chain, draw) float64 ...\n    index_in_trajectory  (chain, draw) int64 ...\n    acceptance_rate      (chain, draw) float64 ...\n    diverging            (chain, draw) bool ...\n    ...                   ...\n    smallest_eigval      (chain, draw) float64 ...\n    step_size_bar        (chain, draw) float64 ...\n    step_size            (chain, draw) float64 ...\n    energy               (chain, draw) float64 ...\n    tree_depth           (chain, draw) int64 ...\n    perf_counter_diff    (chain, draw) float64 ...\nAttributes: (6)xarray.DatasetDimensions:chain: 4draw: 500Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])Data variables: (16)max_energy_error(chain, draw)float64...[2000 values with dtype=float64]energy_error(chain, draw)float64...[2000 values with dtype=float64]lp(chain, draw)float64...[2000 values with dtype=float64]index_in_trajectory(chain, draw)int64...[2000 values with dtype=int64]acceptance_rate(chain, draw)float64...[2000 values with dtype=float64]diverging(chain, draw)bool...[2000 values with dtype=bool]process_time_diff(chain, draw)float64...[2000 values with dtype=float64]n_steps(chain, draw)float64...[2000 values with dtype=float64]perf_counter_start(chain, draw)float64...[2000 values with dtype=float64]largest_eigval(chain, draw)float64...[2000 values with dtype=float64]smallest_eigval(chain, draw)float64...[2000 values with dtype=float64]step_size_bar(chain, draw)float64...[2000 values with dtype=float64]step_size(chain, draw)float64...[2000 values with dtype=float64]energy(chain, draw)float64...[2000 values with dtype=float64]tree_depth(chain, draw)int64...[2000 values with dtype=int64]perf_counter_diff(chain, draw)float64...[2000 values with dtype=float64]Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))Attributes: (6)arviz_version :0.13.0.dev0created_at :2022-10-13T14:37:37.324929inference_library :pymcinference_library_version :4.2.2sampling_time :7.480114936828613tuning_steps :1000\n                      \n                  \n            \n            \n            \n                  \n                  prior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (chain: 1, draw: 500, school: 8)\nCoordinates:\n  * chain    (chain) int64 0\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 ... 492 493 494 495 496 497 498 499\n  * school   (school) &lt;U16 'Choate' 'Deerfield' ... \"St. Paul's\" 'Mt. Hermon'\nData variables:\n    tau      (chain, draw) float64 ...\n    theta    (chain, draw, school) float64 ...\n    mu       (chain, draw) float64 ...\nAttributes: (4)xarray.DatasetDimensions:chain: 1draw: 500school: 8Coordinates: (3)chain(chain)int640array([0])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (3)tau(chain, draw)float64...[500 values with dtype=float64]theta(chain, draw, school)float64...[4000 values with dtype=float64]mu(chain, draw)float64...[500 values with dtype=float64]Indexes: (3)chainPandasIndexPandasIndex(Index([0], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))schoolPandasIndexPandasIndex(Index(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', 'St. Paul's', 'Mt. Hermon'],\n      dtype='object', name='school'))Attributes: (4)arviz_version :0.13.0.dev0created_at :2022-10-13T14:37:26.602116inference_library :pymcinference_library_version :4.2.2\n                      \n                  \n            \n            \n            \n                  \n                  prior_predictive\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (chain: 1, draw: 500, school: 8)\nCoordinates:\n  * chain    (chain) int64 0\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 ... 492 493 494 495 496 497 498 499\n  * school   (school) &lt;U16 'Choate' 'Deerfield' ... \"St. Paul's\" 'Mt. Hermon'\nData variables:\n    obs      (chain, draw, school) float64 ...\nAttributes: (4)xarray.DatasetDimensions:chain: 1draw: 500school: 8Coordinates: (3)chain(chain)int640array([0])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (1)obs(chain, draw, school)float64...[4000 values with dtype=float64]Indexes: (3)chainPandasIndexPandasIndex(Index([0], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))schoolPandasIndexPandasIndex(Index(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', 'St. Paul's', 'Mt. Hermon'],\n      dtype='object', name='school'))Attributes: (4)arviz_version :0.13.0.dev0created_at :2022-10-13T14:37:26.604969inference_library :pymcinference_library_version :4.2.2\n                      \n                  \n            \n            \n            \n                  \n                  observed_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (school: 8)\nCoordinates:\n  * school   (school) &lt;U16 'Choate' 'Deerfield' ... \"St. Paul's\" 'Mt. Hermon'\nData variables:\n    obs      (school) float64 ...\nAttributes: (4)xarray.DatasetDimensions:school: 8Coordinates: (1)school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (1)obs(school)float64...[8 values with dtype=float64]Indexes: (1)schoolPandasIndexPandasIndex(Index(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', 'St. Paul's', 'Mt. Hermon'],\n      dtype='object', name='school'))Attributes: (4)arviz_version :0.13.0.dev0created_at :2022-10-13T14:37:26.606375inference_library :pymcinference_library_version :4.2.2\n                      \n                  \n            \n            \n            \n                  \n                  constant_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (school: 8)\nCoordinates:\n  * school   (school) &lt;U16 'Choate' 'Deerfield' ... \"St. Paul's\" 'Mt. Hermon'\nData variables:\n    scores   (school) float64 ...\nAttributes: (4)xarray.DatasetDimensions:school: 8Coordinates: (1)school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (1)scores(school)float64...[8 values with dtype=float64]Indexes: (1)schoolPandasIndexPandasIndex(Index(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', 'St. Paul's', 'Mt. Hermon'],\n      dtype='object', name='school'))Attributes: (4)arviz_version :0.13.0.dev0created_at :2022-10-13T14:37:26.607471inference_library :pymcinference_library_version :4.2.2\n                      \n                  \n            \n            \n              \n            \n            \n\n\nWe can immediately see that we have many groups. This is an HTML representation of the InferenceData, so if you are reading this from a browser you should be able to interact with it. If you click on the posterior group you will see that we have the coordinates chain, draw, and school, and that they have dimensions 4, 500, and 8 respectively. This means that the posterior samples were generated by running an MCMC sampler with 4 chains, each one of 500 draws. The dimension school refers to the eight schools analyzed in the model. Furthermore, if you click on the  symbol by the school coordinate, you will be able to see the names of each school.\n\n2.1.1 Get the dataset corresponding to a single group\nYou can access each group using the dot notation, groups are attributes of the InferenceData object. For instance, to access the posterior group you can write:\n\nidata.posterior\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (chain: 4, draw: 500, school: 8)\nCoordinates:\n  * chain    (chain) int64 0 1 2 3\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 ... 492 493 494 495 496 497 498 499\n  * school   (school) &lt;U16 'Choate' 'Deerfield' ... \"St. Paul's\" 'Mt. Hermon'\nData variables:\n    mu       (chain, draw) float64 ...\n    theta    (chain, draw, school) float64 ...\n    tau      (chain, draw) float64 ...\nAttributes: (6)xarray.DatasetDimensions:chain: 4draw: 500school: 8Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (3)mu(chain, draw)float64...[2000 values with dtype=float64]theta(chain, draw, school)float64...[16000 values with dtype=float64]tau(chain, draw)float64...[2000 values with dtype=float64]Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))schoolPandasIndexPandasIndex(Index(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', 'St. Paul's', 'Mt. Hermon'],\n      dtype='object', name='school'))Attributes: (6)created_at :2022-10-13T14:37:37.315398arviz_version :0.13.0.dev0inference_library :pymcinference_library_version :4.2.2sampling_time :7.480114936828613tuning_steps :1000\n\n\nAlternativelly, we can use a dictionary-like notation:\n\nidata[\"posterior\"];\n\nIn both cases the result is an xarray dataset . If you are not familiar with xarray imagine NumPy multidimensional arrays but with labels! This makes many operations easier as you don’t have to remember the order of the dimensions.\n\n\n2.1.2 Get coordinate values\nAs we have seen, we have 8 schools in this InferenceData with their names. If we want to programmatically access the names we can do\n\nidata.posterior.school.values\n\narray(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'],\n      dtype='&lt;U16')\n\n\nWhich returns a NumPy array with the names of the schools. To obtain the number of schools we can write:\n\nlen(idata.observed_data.school)\n\n8\n\n\nNotice that we do not need to first obtain the NumPy array and then compute the length.\n\n\n2.1.3 Get a subset of chains\nBecause we have labels for the names of the schools we can use them to access their associated information. Labels are usually much easier to remember than numerical indices. For instance, to access the posterior samples of the school Choate we can write:\n\nidata.posterior.sel(school=\"Choate\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (chain: 4, draw: 500)\nCoordinates:\n  * chain    (chain) int64 0 1 2 3\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 ... 492 493 494 495 496 497 498 499\n    school   &lt;U16 'Choate'\nData variables:\n    mu       (chain, draw) float64 ...\n    theta    (chain, draw) float64 ...\n    tau      (chain, draw) float64 ...\nAttributes: (6)xarray.DatasetDimensions:chain: 4draw: 500Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])school()&lt;U16'Choate'array('Choate', dtype='&lt;U16')Data variables: (3)mu(chain, draw)float64...[2000 values with dtype=float64]theta(chain, draw)float64...[2000 values with dtype=float64]tau(chain, draw)float64...[2000 values with dtype=float64]Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))Attributes: (6)created_at :2022-10-13T14:37:37.315398arviz_version :0.13.0.dev0inference_library :pymcinference_library_version :4.2.2sampling_time :7.480114936828613tuning_steps :1000\n\n\nThe draw and chain coordinates are indexed using numbers, the following code will return the last draw from chain 1 and chain 2:\n\nidata.posterior.sel(draw=499, chain=[1, 2])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (chain: 2, school: 8)\nCoordinates:\n  * chain    (chain) int64 1 2\n    draw     int64 499\n  * school   (school) &lt;U16 'Choate' 'Deerfield' ... \"St. Paul's\" 'Mt. Hermon'\nData variables:\n    mu       (chain) float64 ...\n    theta    (chain, school) float64 ...\n    tau      (chain) float64 ...\nAttributes: (6)xarray.DatasetDimensions:chain: 2school: 8Coordinates: (3)chain(chain)int641 2array([1, 2])draw()int64499array(499)school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (3)mu(chain)float64...[2 values with dtype=float64]theta(chain, school)float64...[16 values with dtype=float64]tau(chain)float64...[2 values with dtype=float64]Indexes: (2)chainPandasIndexPandasIndex(Index([1, 2], dtype='int64', name='chain'))schoolPandasIndexPandasIndex(Index(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', 'St. Paul's', 'Mt. Hermon'],\n      dtype='object', name='school'))Attributes: (6)created_at :2022-10-13T14:37:37.315398arviz_version :0.13.0.dev0inference_library :pymcinference_library_version :4.2.2sampling_time :7.480114936828613tuning_steps :1000\n\n\nUsually we don’t access individual draws, a more common operation is to select a range. For that purpose, we can use Python’s slice function. For example, the following returns the first 200 draws from all chains:\n\nidata.posterior.sel(draw=slice(0, 200))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (chain: 4, draw: 201, school: 8)\nCoordinates:\n  * chain    (chain) int64 0 1 2 3\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 ... 193 194 195 196 197 198 199 200\n  * school   (school) &lt;U16 'Choate' 'Deerfield' ... \"St. Paul's\" 'Mt. Hermon'\nData variables:\n    mu       (chain, draw) float64 ...\n    theta    (chain, draw, school) float64 ...\n    tau      (chain, draw) float64 ...\nAttributes: (6)xarray.DatasetDimensions:chain: 4draw: 201school: 8Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 196 197 198 199 200array([  0,   1,   2, ..., 198, 199, 200])school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (3)mu(chain, draw)float64...[804 values with dtype=float64]theta(chain, draw, school)float64...[6432 values with dtype=float64]tau(chain, draw)float64...[804 values with dtype=float64]Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       191, 192, 193, 194, 195, 196, 197, 198, 199, 200],\n      dtype='int64', name='draw', length=201))schoolPandasIndexPandasIndex(Index(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', 'St. Paul's', 'Mt. Hermon'],\n      dtype='object', name='school'))Attributes: (6)created_at :2022-10-13T14:37:37.315398arviz_version :0.13.0.dev0inference_library :pymcinference_library_version :4.2.2sampling_time :7.480114936828613tuning_steps :1000\n\n\nUsing the slice function we can also remove the first 100 samples.\n\nidata.posterior.sel(draw=slice(100, None))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (chain: 4, draw: 400, school: 8)\nCoordinates:\n  * chain    (chain) int64 0 1 2 3\n  * draw     (draw) int64 100 101 102 103 104 105 ... 494 495 496 497 498 499\n  * school   (school) &lt;U16 'Choate' 'Deerfield' ... \"St. Paul's\" 'Mt. Hermon'\nData variables:\n    mu       (chain, draw) float64 ...\n    theta    (chain, draw, school) float64 ...\n    tau      (chain, draw) float64 ...\nAttributes: (6)xarray.DatasetDimensions:chain: 4draw: 400school: 8Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int64100 101 102 103 ... 496 497 498 499array([100, 101, 102, ..., 497, 498, 499])school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (3)mu(chain, draw)float64...[1600 values with dtype=float64]theta(chain, draw, school)float64...[12800 values with dtype=float64]tau(chain, draw)float64...[1600 values with dtype=float64]Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=400))schoolPandasIndexPandasIndex(Index(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', 'St. Paul's', 'Mt. Hermon'],\n      dtype='object', name='school'))Attributes: (6)created_at :2022-10-13T14:37:37.315398arviz_version :0.13.0.dev0inference_library :pymcinference_library_version :4.2.2sampling_time :7.480114936828613tuning_steps :1000\n\n\nWe can apply the same operations to the entire InferenceData object. Can you anticipate the result?\n\nidata.sel(draw=slice(100, None))\n\n\n\n            \n              \n                arviz.InferenceData\n              \n              \n              \n            \n                  \n                  posterior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (chain: 4, draw: 400, school: 8)\nCoordinates:\n  * chain    (chain) int64 0 1 2 3\n  * draw     (draw) int64 100 101 102 103 104 105 ... 494 495 496 497 498 499\n  * school   (school) &lt;U16 'Choate' 'Deerfield' ... \"St. Paul's\" 'Mt. Hermon'\nData variables:\n    mu       (chain, draw) float64 ...\n    theta    (chain, draw, school) float64 ...\n    tau      (chain, draw) float64 ...\nAttributes: (6)xarray.DatasetDimensions:chain: 4draw: 400school: 8Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int64100 101 102 103 ... 496 497 498 499array([100, 101, 102, ..., 497, 498, 499])school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (3)mu(chain, draw)float64...[1600 values with dtype=float64]theta(chain, draw, school)float64...[12800 values with dtype=float64]tau(chain, draw)float64...[1600 values with dtype=float64]Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=400))schoolPandasIndexPandasIndex(Index(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', 'St. Paul's', 'Mt. Hermon'],\n      dtype='object', name='school'))Attributes: (6)created_at :2022-10-13T14:37:37.315398arviz_version :0.13.0.dev0inference_library :pymcinference_library_version :4.2.2sampling_time :7.480114936828613tuning_steps :1000\n                      \n                  \n            \n            \n            \n                  \n                  posterior_predictive\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (chain: 4, draw: 400, school: 8)\nCoordinates:\n  * chain    (chain) int64 0 1 2 3\n  * draw     (draw) int64 100 101 102 103 104 105 ... 494 495 496 497 498 499\n  * school   (school) &lt;U16 'Choate' 'Deerfield' ... \"St. Paul's\" 'Mt. Hermon'\nData variables:\n    obs      (chain, draw, school) float64 ...\nAttributes: (4)xarray.DatasetDimensions:chain: 4draw: 400school: 8Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int64100 101 102 103 ... 496 497 498 499array([100, 101, 102, ..., 497, 498, 499])school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (1)obs(chain, draw, school)float64...[12800 values with dtype=float64]Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=400))schoolPandasIndexPandasIndex(Index(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', 'St. Paul's', 'Mt. Hermon'],\n      dtype='object', name='school'))Attributes: (4)arviz_version :0.13.0.dev0created_at :2022-10-13T14:37:41.460544inference_library :pymcinference_library_version :4.2.2\n                      \n                  \n            \n            \n            \n                  \n                  log_likelihood\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (chain: 4, draw: 400, school: 8)\nCoordinates:\n  * chain    (chain) int64 0 1 2 3\n  * draw     (draw) int64 100 101 102 103 104 105 ... 494 495 496 497 498 499\n  * school   (school) &lt;U16 'Choate' 'Deerfield' ... \"St. Paul's\" 'Mt. Hermon'\nData variables:\n    obs      (chain, draw, school) float64 ...\nAttributes: (4)xarray.DatasetDimensions:chain: 4draw: 400school: 8Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int64100 101 102 103 ... 496 497 498 499array([100, 101, 102, ..., 497, 498, 499])school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (1)obs(chain, draw, school)float64...[12800 values with dtype=float64]Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=400))schoolPandasIndexPandasIndex(Index(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', 'St. Paul's', 'Mt. Hermon'],\n      dtype='object', name='school'))Attributes: (4)arviz_version :0.13.0.dev0created_at :2022-10-13T14:37:37.487399inference_library :pymcinference_library_version :4.2.2\n                      \n                  \n            \n            \n            \n                  \n                  sample_stats\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:              (chain: 4, draw: 400)\nCoordinates:\n  * chain                (chain) int64 0 1 2 3\n  * draw                 (draw) int64 100 101 102 103 104 ... 496 497 498 499\nData variables: (12/16)\n    max_energy_error     (chain, draw) float64 ...\n    energy_error         (chain, draw) float64 ...\n    lp                   (chain, draw) float64 ...\n    index_in_trajectory  (chain, draw) int64 ...\n    acceptance_rate      (chain, draw) float64 ...\n    diverging            (chain, draw) bool ...\n    ...                   ...\n    smallest_eigval      (chain, draw) float64 ...\n    step_size_bar        (chain, draw) float64 ...\n    step_size            (chain, draw) float64 ...\n    energy               (chain, draw) float64 ...\n    tree_depth           (chain, draw) int64 ...\n    perf_counter_diff    (chain, draw) float64 ...\nAttributes: (6)xarray.DatasetDimensions:chain: 4draw: 400Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int64100 101 102 103 ... 496 497 498 499array([100, 101, 102, ..., 497, 498, 499])Data variables: (16)max_energy_error(chain, draw)float64...[1600 values with dtype=float64]energy_error(chain, draw)float64...[1600 values with dtype=float64]lp(chain, draw)float64...[1600 values with dtype=float64]index_in_trajectory(chain, draw)int64...[1600 values with dtype=int64]acceptance_rate(chain, draw)float64...[1600 values with dtype=float64]diverging(chain, draw)bool...[1600 values with dtype=bool]process_time_diff(chain, draw)float64...[1600 values with dtype=float64]n_steps(chain, draw)float64...[1600 values with dtype=float64]perf_counter_start(chain, draw)float64...[1600 values with dtype=float64]largest_eigval(chain, draw)float64...[1600 values with dtype=float64]smallest_eigval(chain, draw)float64...[1600 values with dtype=float64]step_size_bar(chain, draw)float64...[1600 values with dtype=float64]step_size(chain, draw)float64...[1600 values with dtype=float64]energy(chain, draw)float64...[1600 values with dtype=float64]tree_depth(chain, draw)int64...[1600 values with dtype=int64]perf_counter_diff(chain, draw)float64...[1600 values with dtype=float64]Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=400))Attributes: (6)arviz_version :0.13.0.dev0created_at :2022-10-13T14:37:37.324929inference_library :pymcinference_library_version :4.2.2sampling_time :7.480114936828613tuning_steps :1000\n                      \n                  \n            \n            \n            \n                  \n                  prior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (chain: 1, draw: 400, school: 8)\nCoordinates:\n  * chain    (chain) int64 0\n  * draw     (draw) int64 100 101 102 103 104 105 ... 494 495 496 497 498 499\n  * school   (school) &lt;U16 'Choate' 'Deerfield' ... \"St. Paul's\" 'Mt. Hermon'\nData variables:\n    tau      (chain, draw) float64 ...\n    theta    (chain, draw, school) float64 ...\n    mu       (chain, draw) float64 ...\nAttributes: (4)xarray.DatasetDimensions:chain: 1draw: 400school: 8Coordinates: (3)chain(chain)int640array([0])draw(draw)int64100 101 102 103 ... 496 497 498 499array([100, 101, 102, ..., 497, 498, 499])school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (3)tau(chain, draw)float64...[400 values with dtype=float64]theta(chain, draw, school)float64...[3200 values with dtype=float64]mu(chain, draw)float64...[400 values with dtype=float64]Indexes: (3)chainPandasIndexPandasIndex(Index([0], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=400))schoolPandasIndexPandasIndex(Index(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', 'St. Paul's', 'Mt. Hermon'],\n      dtype='object', name='school'))Attributes: (4)arviz_version :0.13.0.dev0created_at :2022-10-13T14:37:26.602116inference_library :pymcinference_library_version :4.2.2\n                      \n                  \n            \n            \n            \n                  \n                  prior_predictive\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (chain: 1, draw: 400, school: 8)\nCoordinates:\n  * chain    (chain) int64 0\n  * draw     (draw) int64 100 101 102 103 104 105 ... 494 495 496 497 498 499\n  * school   (school) &lt;U16 'Choate' 'Deerfield' ... \"St. Paul's\" 'Mt. Hermon'\nData variables:\n    obs      (chain, draw, school) float64 ...\nAttributes: (4)xarray.DatasetDimensions:chain: 1draw: 400school: 8Coordinates: (3)chain(chain)int640array([0])draw(draw)int64100 101 102 103 ... 496 497 498 499array([100, 101, 102, ..., 497, 498, 499])school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (1)obs(chain, draw, school)float64...[3200 values with dtype=float64]Indexes: (3)chainPandasIndexPandasIndex(Index([0], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=400))schoolPandasIndexPandasIndex(Index(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', 'St. Paul's', 'Mt. Hermon'],\n      dtype='object', name='school'))Attributes: (4)arviz_version :0.13.0.dev0created_at :2022-10-13T14:37:26.604969inference_library :pymcinference_library_version :4.2.2\n                      \n                  \n            \n            \n            \n                  \n                  observed_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (school: 8)\nCoordinates:\n  * school   (school) &lt;U16 'Choate' 'Deerfield' ... \"St. Paul's\" 'Mt. Hermon'\nData variables:\n    obs      (school) float64 ...\nAttributes: (4)xarray.DatasetDimensions:school: 8Coordinates: (1)school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (1)obs(school)float64...[8 values with dtype=float64]Indexes: (1)schoolPandasIndexPandasIndex(Index(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', 'St. Paul's', 'Mt. Hermon'],\n      dtype='object', name='school'))Attributes: (4)arviz_version :0.13.0.dev0created_at :2022-10-13T14:37:26.606375inference_library :pymcinference_library_version :4.2.2\n                      \n                  \n            \n            \n            \n                  \n                  constant_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (school: 8)\nCoordinates:\n  * school   (school) &lt;U16 'Choate' 'Deerfield' ... \"St. Paul's\" 'Mt. Hermon'\nData variables:\n    scores   (school) float64 ...\nAttributes: (4)xarray.DatasetDimensions:school: 8Coordinates: (1)school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (1)scores(school)float64...[8 values with dtype=float64]Indexes: (1)schoolPandasIndexPandasIndex(Index(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', 'St. Paul's', 'Mt. Hermon'],\n      dtype='object', name='school'))Attributes: (4)arviz_version :0.13.0.dev0created_at :2022-10-13T14:37:26.607471inference_library :pymcinference_library_version :4.2.2\n                      \n                  \n            \n            \n              \n            \n            \n\n\nIf you check the object you will see that the groups posterior, posterior_predictive, log_likelihood, sample_stats, prior, and prior_predictive have 400 draws compared to the original 500. The group observed_data has not been affected because it does not have the draw dimension. Alternatively, you can specify which group or groups you want to change.\nidata.sel(draw=slice(100, None), groups=\"posterior\")\n\n\n2.1.4 Compute posterior mean\nWe can perform operations on the InferenceData object. For instance, to compute the mean of the first 200 draws we can write:\n\nidata.posterior.sel(draw=slice(0, 200)).mean()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  ()\nData variables:\n    mu       float64 4.542\n    theta    float64 5.005\n    tau      float64 4.196xarray.DatasetDimensions:Coordinates: (0)Data variables: (3)mu()float644.542array(4.54161222)theta()float645.005array(5.00504117)tau()float644.196array(4.19590805)Indexes: (0)Attributes: (0)\n\n\nIn NumPy, it is common to perform operations like this along a given axis. With InferecenData/xarray we can do the same by specifying the dimension along which we want to operate. For instance, to compute the mean along the draw dimension we can write:\n\nidata.posterior.mean(\"draw\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (chain: 4, school: 8)\nCoordinates:\n  * chain    (chain) int64 0 1 2 3\n  * school   (school) &lt;U16 'Choate' 'Deerfield' ... \"St. Paul's\" 'Mt. Hermon'\nData variables:\n    mu       (chain) float64 4.246 4.184 4.659 4.855\n    theta    (chain, school) float64 5.793 4.648 3.742 4.591 ... 4.14 6.74 5.275\n    tau      (chain) float64 3.682 4.247 4.656 3.912xarray.DatasetDimensions:chain: 4school: 8Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (3)mu(chain)float644.246 4.184 4.659 4.855array([4.24630224, 4.18354806, 4.65892852, 4.8549536 ])theta(chain, school)float645.793 4.648 3.742 ... 6.74 5.275array([[5.79273861, 4.64846555, 3.74178447, 4.59127846, 3.55348781,\n        4.02845245, 5.96228791, 4.73727946],\n       [6.33445224, 4.67680497, 3.65081796, 4.43324764, 3.3964076 ,\n        3.60127667, 6.48283092, 4.55211921],\n       [6.83321937, 5.51281457, 3.92680194, 5.28074754, 3.98373065,\n        4.1289177 , 7.13866968, 4.52550204],\n       [6.87984671, 5.27213322, 4.43271831, 5.18117578, 3.73373858,\n        4.14010165, 6.7399058 , 5.27474343]])tau(chain)float643.682 4.247 4.656 3.912array([3.6818728 , 4.24683679, 4.65603863, 3.91214293])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))schoolPandasIndexPandasIndex(Index(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', 'St. Paul's', 'Mt. Hermon'],\n      dtype='object', name='school'))Attributes: (0)\n\n\nThis returns the mean for each chain and school. Can you anticipate how different this would be if the dimension was chain instead of draw? And what bout if we use school?\nWe can also specify multiple dimensions. For instance, to compute the mean along the draw and chain dimensions we can write:\n\nidata.posterior.mean([\"chain\", \"draw\"])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (school: 8)\nCoordinates:\n  * school   (school) &lt;U16 'Choate' 'Deerfield' ... \"St. Paul's\" 'Mt. Hermon'\nData variables:\n    mu       float64 4.486\n    theta    (school) float64 6.46 5.028 3.938 4.872 3.667 3.975 6.581 4.772\n    tau      float64 4.124xarray.DatasetDimensions:school: 8Coordinates: (1)school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (3)mu()float644.486array(4.4859331)theta(school)float646.46 5.028 3.938 ... 6.581 4.772array([6.46006423, 5.02755458, 3.93803067, 4.87161236, 3.66684116,\n       3.97468712, 6.58092358, 4.77241104])tau()float644.124array(4.12422279)Indexes: (1)schoolPandasIndexPandasIndex(Index(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', 'St. Paul's', 'Mt. Hermon'],\n      dtype='object', name='school'))Attributes: (0)\n\n\n\n\n2.1.5 Combine chains and draws\nOur primary goal is usually to obtain posterior samples and thus we aren’t concerned with chains and draws. In those cases, we can use the az.extract function. This combines the chain and draw into a sample coordinate which can make further operations easier. By default, az.extract works on the posterior, but you can specify other groups, if you want, with the group argument.\n\naz.extract(idata)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (sample: 2000, school: 8)\nCoordinates:\n  * school   (school) &lt;U16 'Choate' 'Deerfield' ... \"St. Paul's\" 'Mt. Hermon'\n  * sample   (sample) object MultiIndex\n  * chain    (sample) int64 0 0 0 0 0 0 0 0 0 0 0 0 ... 3 3 3 3 3 3 3 3 3 3 3 3\n  * draw     (sample) int64 0 1 2 3 4 5 6 7 ... 492 493 494 495 496 497 498 499\nData variables:\n    mu       (sample) float64 7.872 3.385 9.1 7.304 ... 1.859 1.767 3.486 3.404\n    theta    (school, sample) float64 12.32 11.29 5.709 ... -2.623 8.452 1.295\n    tau      (sample) float64 4.726 3.909 4.844 1.857 ... 2.741 2.932 4.461\nAttributes: (6)xarray.DatasetDimensions:sample: 2000school: 8Coordinates: (4)school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')sample(sample)objectMultiIndexarray([(0, 0), (0, 1), (0, 2), ..., (3, 497), (3, 498), (3, 499)], dtype=object)chain(sample)int640 0 0 0 0 0 0 0 ... 3 3 3 3 3 3 3 3array([0, 0, 0, ..., 3, 3, 3])draw(sample)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])Data variables: (3)mu(sample)float647.872 3.385 9.1 ... 3.486 3.404array([7.87179637, 3.38455431, 9.10047569, ..., 1.76673325, 3.48611194,\n       3.40446391])theta(school, sample)float6412.32 11.29 5.709 ... 8.452 1.295array([[12.32068558, 11.28562322,  5.70850575, ...,  3.53251469,\n         4.1827505 ,  0.19295578],\n       [ 9.90536689,  9.12932369,  5.7579324 , ...,  2.00890089,\n         7.55425055,  6.4984278 ],\n       [14.9516155 ,  3.13926327, 10.94458539, ...,  0.51080602,\n         4.45603444, -0.89442402],\n       ...,\n       [16.90179529,  2.39308809,  8.14332707, ...,  4.70724881,\n         1.5289581 ,  7.93646013],\n       [13.19805933, 10.05522282,  7.60475341, ...,  3.07331446,\n         1.09609826,  6.76245459],\n       [15.06136584,  6.17672421,  8.76764689, ..., -2.62306892,\n         8.45228161,  1.2950506 ]])tau(sample)float644.726 3.909 4.844 ... 2.932 4.461array([4.72574006, 3.90899361, 4.8440252 , ..., 2.74060666, 2.93237926,\n       4.46124596])Indexes: (2)schoolPandasIndexPandasIndex(Index(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', 'St. Paul's', 'Mt. Hermon'],\n      dtype='object', name='school'))samplechaindrawPandasMultiIndexPandasIndex(MultiIndex([(0,   0),\n            (0,   1),\n            (0,   2),\n            (0,   3),\n            (0,   4),\n            (0,   5),\n            (0,   6),\n            (0,   7),\n            (0,   8),\n            (0,   9),\n            ...\n            (3, 490),\n            (3, 491),\n            (3, 492),\n            (3, 493),\n            (3, 494),\n            (3, 495),\n            (3, 496),\n            (3, 497),\n            (3, 498),\n            (3, 499)],\n           name='sample', length=2000))Attributes: (6)created_at :2022-10-13T14:37:37.315398arviz_version :0.13.0.dev0inference_library :pymcinference_library_version :4.2.2sampling_time :7.480114936828613tuning_steps :1000\n\n\nYou can achieve the same result using idata.posterior.stack(sample=(\"chain\", \"draw\")). But extract can be more flexible because it takes care of the most common subsetting operations with MCMC samples. It can:\n\nCombine chains and draws\nReturn a subset of variables (with optional filtering with regular expressions or string matching)\nReturn a subset of samples. Moreover, by default, it returns a random subset to prevent getting non-representative samples due to bad mixing.\nAccess any group\n\nTo get a subsample we can specify the number of samples we want with the num_samples argument. For instance, to get 100 samples we can write:\n\naz.extract(idata, num_samples=100);\n\nIf you need to extract subsets from multiple groups, you should use a random seed. This will ensure that subsamples match. For example, if you do\n\nposterior = az.extract(idata, num_samples=100, rng=124)\nll = az.extract(idata, group=\"log_likelihood\", num_samples=100, rng=124)\n\nYou can inspect the samples in the posterior and ll variables and see that they match."
  },
  {
    "objectID": "InferenceData.html#add-a-new-variable",
    "href": "InferenceData.html#add-a-new-variable",
    "title": "2  Working with InferenceData",
    "section": "2.2 Add a new variable",
    "text": "2.2 Add a new variable\nWe can add variables to existing groups. For instance, we can add the log likelihood to the posterior group by writing:\n\nposterior[\"log_tau\"] = np.log(posterior[\"tau\"])\nposterior\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (sample: 100, school: 8)\nCoordinates:\n  * school   (school) &lt;U16 'Choate' 'Deerfield' ... \"St. Paul's\" 'Mt. Hermon'\n  * sample   (sample) object MultiIndex\n  * chain    (sample) int64 0 0 0 0 0 1 3 3 2 2 1 2 ... 2 1 1 2 1 0 3 1 3 2 2 1\n  * draw     (sample) int64 385 210 398 410 366 7 289 ... 248 470 145 415 455 67\nData variables:\n    mu       (sample) float64 -0.5899 3.603 3.139 4.188 ... 10.06 1.695 1.798\n    theta    (school, sample) float64 0.1299 -0.3171 1.546 ... -0.161 4.141\n    tau      (sample) float64 2.567 3.013 1.535 4.472 ... 10.63 6.453 4.329\n    log_tau  (sample) float64 0.9426 1.103 0.4285 1.498 ... 2.364 1.865 1.465\nAttributes: (6)xarray.DatasetDimensions:sample: 100school: 8Coordinates: (4)school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')sample(sample)objectMultiIndexarray([(0, 385), (0, 210), (0, 398), (0, 410), (0, 366), (1, 7), (3, 289),\n       (3, 380), (2, 446), (2, 305), (1, 342), (2, 262), (3, 411), (3, 327),\n       (3, 97), (0, 104), (0, 236), (3, 397), (0, 474), (2, 376), (2, 374),\n       (1, 281), (1, 427), (2, 184), (3, 146), (0, 217), (3, 404), (1, 270),\n       (3, 10), (3, 128), (2, 56), (3, 3), (2, 477), (2, 105), (0, 369),\n       (0, 186), (0, 290), (3, 16), (2, 67), (2, 57), (1, 96), (3, 1),\n       (0, 423), (1, 335), (0, 22), (2, 243), (2, 238), (0, 418), (1, 181),\n       (1, 331), (3, 378), (0, 118), (2, 162), (1, 436), (3, 329), (2, 25),\n       (3, 69), (2, 96), (0, 309), (2, 64), (0, 4), (0, 359), (0, 425),\n       (1, 191), (2, 268), (2, 19), (2, 301), (2, 255), (3, 280), (0, 404),\n       (3, 489), (3, 148), (2, 148), (1, 313), (0, 250), (1, 302), (2, 213),\n       (0, 21), (0, 436), (0, 39), (0, 470), (3, 255), (0, 0), (3, 290),\n       (1, 123), (3, 201), (1, 262), (3, 459), (2, 29), (1, 271), (1, 153),\n       (2, 190), (1, 127), (0, 130), (3, 248), (1, 470), (3, 145), (2, 415),\n       (2, 455), (1, 67)], dtype=object)chain(sample)int640 0 0 0 0 1 3 3 ... 1 0 3 1 3 2 2 1array([0, 0, 0, 0, 0, 1, 3, 3, 2, 2, 1, 2, 3, 3, 3, 0, 0, 3, 0, 2, 2, 1, 1, 2,\n       3, 0, 3, 1, 3, 3, 2, 3, 2, 2, 0, 0, 0, 3, 2, 2, 1, 3, 0, 1, 0, 2, 2, 0,\n       1, 1, 3, 0, 2, 1, 3, 2, 3, 2, 0, 2, 0, 0, 0, 1, 2, 2, 2, 2, 3, 0, 3, 3,\n       2, 1, 0, 1, 2, 0, 0, 0, 0, 3, 0, 3, 1, 3, 1, 3, 2, 1, 1, 2, 1, 0, 3, 1,\n       3, 2, 2, 1])draw(sample)int64385 210 398 410 ... 145 415 455 67array([385, 210, 398, 410, 366,   7, 289, 380, 446, 305, 342, 262, 411, 327,\n        97, 104, 236, 397, 474, 376, 374, 281, 427, 184, 146, 217, 404, 270,\n        10, 128,  56,   3, 477, 105, 369, 186, 290,  16,  67,  57,  96,   1,\n       423, 335,  22, 243, 238, 418, 181, 331, 378, 118, 162, 436, 329,  25,\n        69,  96, 309,  64,   4, 359, 425, 191, 268,  19, 301, 255, 280, 404,\n       489, 148, 148, 313, 250, 302, 213,  21, 436,  39, 470, 255,   0, 290,\n       123, 201, 262, 459,  29, 271, 153, 190, 127, 130, 248, 470, 145, 415,\n       455,  67])Data variables: (4)mu(sample)float64-0.5899 3.603 3.139 ... 1.695 1.798array([-0.58992182,  3.60271776,  3.13932356,  4.1875749 , 10.61369427,\n        7.66415117,  7.33881887, -0.05964838, 11.06850793,  2.35546341,\n        6.41031848,  8.68300709, 10.51004057,  7.80297005,  6.28471683,\n       -5.30644384,  0.15539645,  3.76413293,  3.60582372,  8.2046313 ,\n       10.78844313,  7.97486392,  2.48827453,  8.59831314,  5.7062514 ,\n        4.33288636,  3.65301302,  3.8764257 ,  9.39397335,  6.35300178,\n        7.39448521,  8.15193678,  4.48579435,  0.05291535,  6.42014633,\n        7.97294671,  0.33876008,  5.38096062, -0.2829402 ,  5.82861061,\n        5.56950581,  6.53969896,  2.98324651,  9.96636702,  5.39582442,\n       -6.24476747, -0.61701286,  6.85791289, -1.10251994,  2.87933614,\n        4.05400849,  2.79848565, 11.28114377,  2.23212811,  9.68969606,\n        6.42274543, 12.37254565, -2.22316504,  6.7823585 , -0.5325727 ,\n        9.87967529,  0.56326736, -0.83741744,  6.68852046,  1.97129047,\n        2.18820541,  7.18146879,  6.58351668,  6.6902741 ,  9.64214637,\n        4.19161706,  4.11395734,  3.98776455,  5.36298585,  5.42342531,\n        8.9472939 , -0.45500484,  2.93840197,  2.13952108,  4.22743188,\n        6.43616276, -1.12193332,  7.87179637,  6.95624746,  5.59018293,\n        5.80188688,  5.42862405, -0.76433588,  9.69811215,  3.0939095 ,\n        8.35361477,  5.15342131,  4.98391597,  4.07677779, -0.02423642,\n        2.82139573,  6.15558039, 10.05781797,  1.6952418 ,  1.79836666])theta(school, sample)float640.1299 -0.3171 ... -0.161 4.141array([[ 1.29864107e-01, -3.17105694e-01,  1.54551184e+00,\n         7.67156766e+00,  1.73999686e+01,  6.30961900e+00,\n         8.53632677e+00, -5.89347515e-02,  1.18389563e+01,\n         9.50244016e+00,  1.08965295e+01,  9.13284697e+00,\n         7.74520291e+00,  1.62668706e+01,  6.19166018e+00,\n        -5.78698087e+00, -3.14091079e+00,  5.34035782e+00,\n         6.32443017e+00,  1.00995821e+01,  1.33507741e+01,\n         7.72679763e+00,  1.67363399e+00,  9.12787329e+00,\n         9.05214496e+00,  4.68173181e+00,  1.15404595e+00,\n         3.87151190e+00,  7.15000517e+00,  7.88942950e+00,\n         4.50441948e+00,  1.22880450e+01,  3.93302243e+00,\n         1.07728104e+01,  1.30042800e+00,  1.43710361e+01,\n         1.54374354e+00,  2.61377333e-01,  1.02329413e+00,\n         6.44856378e+00,  2.38899998e+01,  8.93115045e+00,\n        -5.33381589e+00,  1.61492210e+01,  4.43196277e+00,\n        -8.43339102e+00,  3.18106445e+00,  8.07073012e+00,\n         4.29664121e+00,  4.08549694e+00,  8.17614149e+00,\n         4.64202496e+00,  1.21647283e+01,  1.59326273e+00,\n         1.16627749e+01,  5.81313835e+00,  1.48492425e+01,\n        -1.30961211e+01,  6.62172155e+00, -1.32636459e+00,\n...\n         7.07701615e+00,  1.35038037e+01,  4.62591046e+00,\n        -7.01639144e+00, -1.54494861e+01,  1.07971148e+01,\n        -6.13593027e+00,  1.01554356e+01, -3.04307193e-01,\n         5.08451764e+00,  1.16330776e+01,  2.22113452e+00,\n         1.18821725e+01,  1.55602979e+01,  1.33246268e+01,\n         6.11503398e+00,  7.79468290e+00, -1.23504922e+00,\n         1.70461419e+01,  1.37072890e+01,  3.42653922e+00,\n         8.04135462e+00,  1.24842549e+01,  6.13943889e+00,\n         1.51420100e+01,  1.32765881e+01,  1.00406316e+01,\n         1.06649000e+01,  5.95217982e-01, -4.29066328e-01,\n         1.34283081e+00,  1.38072058e+00, -8.11330582e+00,\n         1.03521489e+01,  6.13636929e+00,  2.78886275e+00,\n         2.39378808e+00,  2.69622168e+00,  4.54648430e+00,\n        -2.86060739e+00,  1.50613658e+01,  8.45691443e+00,\n         5.77899540e+00,  5.19428493e+00,  6.00098044e+00,\n         8.86164602e+00,  9.92964024e+00,  7.05062428e-01,\n         1.18337006e+01, -4.77078082e+00,  2.40520759e+01,\n         3.63799846e+00, -3.43478834e+00,  3.35132225e+00,\n         6.87930293e+00, -6.83907201e+00, -1.61045716e-01,\n         4.14055948e+00]])tau(sample)float642.567 3.013 1.535 ... 6.453 4.329array([ 2.56668266,  3.0134607 ,  1.53489538,  4.47156129,  8.79417477,\n        4.28584094,  2.59241174,  5.19649942,  3.10004703,  4.58590457,\n        8.18362791,  5.48133729,  8.34133581,  5.1122518 ,  1.22644213,\n        3.03983094,  3.77586176,  2.37281161,  2.53307312,  1.87124261,\n        1.91919294,  1.18451916,  0.99384778,  2.14555459,  1.30671119,\n        1.57142819,  4.15343603,  2.2126497 ,  4.81977288,  1.13419858,\n        3.47090745,  6.00192831,  2.31322578,  5.81782427,  6.17424921,\n        3.15742714,  1.39584712,  5.22195381,  3.42967979,  4.15723397,\n        8.03197157,  3.77186702,  3.95064176,  2.90019392,  2.1772773 ,\n        6.79855445,  7.8076857 ,  4.39346602,  6.58572909,  5.06344339,\n        4.40658161,  1.90735387,  2.20097686,  0.92730205,  5.91314971,\n        5.87734467,  1.97516387, 15.43803695,  1.96151259,  5.98721655,\n        4.74840851, 10.03992426,  7.82304684,  4.16024825, 15.84371962,\n       10.14235828,  5.26840104,  9.15844044,  3.38097258,  1.65950745,\n       10.00698601,  5.18502054,  4.69441829,  2.85134427, 11.5663655 ,\n        4.57909686,  4.8971506 ,  3.4308284 ,  3.78567923,  3.46981489,\n        2.05154164,  1.25274357,  4.72574006,  2.53551072,  2.56232781,\n        1.63947794,  2.20400858,  9.6334663 ,  3.38007244,  1.77222817,\n        5.28435574,  8.59488295, 10.20098024,  2.40782977,  2.88337132,\n        0.89648017,  1.07294028, 10.62851796,  6.45328856,  4.32938379])log_tau(sample)float640.9426 1.103 0.4285 ... 1.865 1.465array([ 0.94261427,  1.10308915,  0.42846222,  1.49773763,  2.17408954,\n        1.45531679,  0.95258862,  1.64798521,  1.13141728,  1.52298738,\n        2.10213556,  1.7013491 ,  2.12122337,  1.63163997,  0.2041174 ,\n        1.1118019 ,  1.32862864,  0.86407558,  0.92943324,  0.62660271,\n        0.65190475,  0.16933692, -0.00617122,  0.76339807,  0.26751344,\n        0.45198488,  1.42393595,  0.79419076,  1.57272681,  0.1259263 ,\n        1.24441607,  1.7920808 ,  0.83864299,  1.76092635,  1.82038729,\n        1.1497575 ,  0.33350148,  1.65287162,  1.2324669 ,  1.42484994,\n        2.08343002,  1.32757011,  1.37387804,  1.0647776 ,  0.77807515,\n        1.91671001,  2.05510859,  1.48011844,  1.88490505,  1.62204676,\n        1.48309924,  0.64571687,  0.78890129, -0.07547593,  1.77717864,\n        1.77110507,  0.68065137,  2.7368344 ,  0.67371591,  1.78962662,\n        1.55780951,  2.30656957,  2.0570741 ,  1.42557475,  2.76277318,\n        2.31672054,  1.66172691,  2.21467591,  1.21816341,  0.50652084,\n        2.30328345,  1.6457738 ,  1.54637421,  1.04779056,  2.44810136,\n        1.52150179,  1.58865353,  1.23280175,  1.33122532,  1.24410125,\n        0.71859153,  0.22533601,  1.55302418,  0.93039509,  0.94091615,\n        0.49437786,  0.79027778,  2.26524311,  1.21789714,  0.57223761,\n        1.66475071,  2.15116702,  2.32248382,  0.87872583,  1.05896021,\n       -0.10927911,  0.0704028 ,  2.36354076,  1.86458986,  1.46542522])Indexes: (2)schoolPandasIndexPandasIndex(Index(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', 'St. Paul's', 'Mt. Hermon'],\n      dtype='object', name='school'))samplechaindrawPandasMultiIndexPandasIndex(MultiIndex([(0, 385),\n            (0, 210),\n            (0, 398),\n            (0, 410),\n            (0, 366),\n            (1,   7),\n            (3, 289),\n            (3, 380),\n            (2, 446),\n            (2, 305),\n            (1, 342),\n            (2, 262),\n            (3, 411),\n            (3, 327),\n            (3,  97),\n            (0, 104),\n            (0, 236),\n            (3, 397),\n            (0, 474),\n            (2, 376),\n            (2, 374),\n            (1, 281),\n            (1, 427),\n            (2, 184),\n            (3, 146),\n            (0, 217),\n            (3, 404),\n            (1, 270),\n            (3,  10),\n            (3, 128),\n            (2,  56),\n            (3,   3),\n            (2, 477),\n            (2, 105),\n            (0, 369),\n            (0, 186),\n            (0, 290),\n            (3,  16),\n            (2,  67),\n            (2,  57),\n            (1,  96),\n            (3,   1),\n            (0, 423),\n            (1, 335),\n            (0,  22),\n            (2, 243),\n            (2, 238),\n            (0, 418),\n            (1, 181),\n            (1, 331),\n            (3, 378),\n            (0, 118),\n            (2, 162),\n            (1, 436),\n            (3, 329),\n            (2,  25),\n            (3,  69),\n            (2,  96),\n            (0, 309),\n            (2,  64),\n            (0,   4),\n            (0, 359),\n            (0, 425),\n            (1, 191),\n            (2, 268),\n            (2,  19),\n            (2, 301),\n            (2, 255),\n            (3, 280),\n            (0, 404),\n            (3, 489),\n            (3, 148),\n            (2, 148),\n            (1, 313),\n            (0, 250),\n            (1, 302),\n            (2, 213),\n            (0,  21),\n            (0, 436),\n            (0,  39),\n            (0, 470),\n            (3, 255),\n            (0,   0),\n            (3, 290),\n            (1, 123),\n            (3, 201),\n            (1, 262),\n            (3, 459),\n            (2,  29),\n            (1, 271),\n            (1, 153),\n            (2, 190),\n            (1, 127),\n            (0, 130),\n            (3, 248),\n            (1, 470),\n            (3, 145),\n            (2, 415),\n            (2, 455),\n            (1,  67)],\n           name='sample'))Attributes: (6)created_at :2022-10-13T14:37:37.315398arviz_version :0.13.0.dev0inference_library :pymcinference_library_version :4.2.2sampling_time :7.480114936828613tuning_steps :1000"
  },
  {
    "objectID": "InferenceData.html#advance-operations-with-inferencedata",
    "href": "InferenceData.html#advance-operations-with-inferencedata",
    "title": "2  Working with InferenceData",
    "section": "2.3 Advance operations with InferenceData",
    "text": "2.3 Advance operations with InferenceData\nNow we delve into more advanced operations with InferenceData. While these operations are not essential to use ArviZ, they can be useful in some cases. Exploring these advanced functionalities will help you become more familiar with InferenceData and provide additional insights that may enhance your overall experience with ArviZ.\n\n2.3.1 Compute and store posterior pushforward quantities\nWe use “posterior pushforward quantities” to refer to quantities that are not variables in the posterior but deterministic computations using posterior variables.\nYou can use xarray for these pushforward operations and store them as a new variable in the posterior group. You’ll then be able to plot them with ArviZ functions, calculate stats and diagnostics on them (like mcse), or save and share the InferenceData object with the pushforward quantities included.\nThe first thing we are going to do is to store the posterior group in a variable called post to make the code more readable. And to compute the log of \\(\\tau\\).\n\npost = idata.posterior\npost[\"log_tau\"] = np.log(post[\"tau\"])\n\nCompute the rolling mean of \\(\\log(\\tau)\\) with xarray.DataArray.rolling, storing the result in the posterior:\n\npost[\"mlogtau\"] = post[\"log_tau\"].rolling({\"draw\": 50}).mean()\n\nUsing xarray for pushforward calculations has all the advantages of working with xarray. It also inherits the disadvantages of working with xarray, but we believe those to be outweighed by the advantages, and we have already shown how to extract the data as NumPy arrays. Working with InferenceData is working mainly with xarray objects and this is what is shown in this guide.\nSome examples of these advantages are specifying operations with named dimensions instead of positional ones (as seen in some previous sections), automatic alignment and broadcasting of arrays (as we’ll see now), or integration with Dask (as shown in the dask_for_arviz guide).\nIn this cell, you will compute pairwise differences between schools on their mean effects (variable theta). To do so, subtract the variable theta after renaming the school dimension to the original variable. Xarray then aligns and broadcasts the two variables because they have different dimensions, and the result is a 4D variable with all the pointwise differences.\nEventually, store the result in the theta_school_diff variable. Notice that the theta_shool_diff variable in the posterior has kept the named dimensions and coordinates:\n\npost[\"theta_school_diff\"] = post.theta - post.theta.rename(school=\"school_bis\")\npost\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:            (chain: 4, draw: 500, school: 8, school_bis: 8)\nCoordinates:\n  * chain              (chain) int64 0 1 2 3\n  * draw               (draw) int64 0 1 2 3 4 5 6 ... 494 495 496 497 498 499\n  * school             (school) &lt;U16 'Choate' 'Deerfield' ... 'Mt. Hermon'\n  * school_bis         (school_bis) &lt;U16 'Choate' 'Deerfield' ... 'Mt. Hermon'\nData variables:\n    mu                 (chain, draw) float64 7.872 3.385 9.1 ... 3.486 3.404\n    theta              (chain, draw, school) float64 12.32 9.905 ... 6.762 1.295\n    tau                (chain, draw) float64 4.726 3.909 4.844 ... 2.932 4.461\n    log_tau            (chain, draw) float64 1.553 1.363 1.578 ... 1.076 1.495\n    mlogtau            (chain, draw) float64 nan nan nan ... 1.494 1.496 1.511\n    theta_school_diff  (chain, draw, school, school_bis) float64 0.0 ... 0.0\nAttributes: (6)xarray.DatasetDimensions:chain: 4draw: 500school: 8school_bis: 8Coordinates: (4)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')school_bis(school_bis)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (6)mu(chain, draw)float647.872 3.385 9.1 ... 3.486 3.404array([[7.871796, 3.384554, 9.100476, ..., 3.656804, 5.427279, 2.735883],\n       [4.315817, 4.597701, 6.213762, ..., 4.841024, 5.946105, 4.666438],\n       [4.527603, 5.862628, 5.910703, ..., 2.871067, 4.095536, 1.775718],\n       [7.179504, 6.539699, 7.48362 , ..., 1.766733, 3.486112, 3.404464]])theta(chain, draw, school)float6412.32 9.905 14.95 ... 6.762 1.295array([[[12.320686,  9.905367, ..., 13.198059, 15.061366],\n        [11.285623,  9.129324, ..., 10.055223,  6.176724],\n        ...,\n        [10.402501,  6.907407, ...,  5.391061,  6.381239],\n        [ 6.661308,  7.413767, ...,  7.56657 ,  9.987618]],\n\n       [[ 5.596639,  6.959605, ...,  7.148854,  3.7909  ],\n        [ 4.96541 ,  1.779358, ...,  3.289161,  5.937311],\n        ...,\n        [ 7.030604,  6.214465, ...,  4.778138,  4.357112],\n        [ 4.884931,  3.952247, ...,  5.687935,  6.777772]],\n\n       [[11.81417 , -0.040477, ...,  0.318087,  0.661304],\n        [ 4.743465, 10.175684, ..., 11.271711,  7.12114 ],\n        ...,\n        [ 5.677651,  1.679108, ...,  1.280615,  7.627658],\n        [ 1.625447,  2.503281, ...,  2.362756, -2.967994]],\n\n       [[ 8.344508,  5.390855, ..., 12.46814 , 12.607797],\n        [ 8.93115 ,  6.852969, ...,  7.013971,  5.136297],\n        ...,\n        [ 4.182751,  7.554251, ...,  1.096098,  8.452282],\n        [ 0.192956,  6.498428, ...,  6.762455,  1.295051]]])tau(chain, draw)float644.726 3.909 4.844 ... 2.932 4.461array([[4.72574 , 3.908994, 4.844025, ..., 1.893838, 5.920062, 4.325896],\n       [1.97083 , 2.049029, 2.123765, ..., 2.17459 , 1.327551, 1.211995],\n       [3.501277, 2.893243, 4.273286, ..., 4.08978 , 2.72017 , 1.917011],\n       [6.07326 , 3.771867, 3.170537, ..., 2.740607, 2.932379, 4.461246]])log_tau(chain, draw)float641.553 1.363 1.578 ... 1.076 1.495array([[1.55302418, 1.36327995, 1.57774603, ..., 0.6386056 , 1.778347  ,\n        1.46461921],\n       [0.67845483, 0.71736608, 0.75319044, ..., 0.77684015, 0.28333575,\n        0.19226749],\n       [1.25312773, 1.06237808, 1.45238307, ..., 1.40849125, 1.00069436,\n        0.65076731],\n       [1.8038955 , 1.32757011, 1.15390112, ..., 1.00817931, 1.07581413,\n        1.49542809]])mlogtau(chain, draw)float64nan nan nan ... 1.494 1.496 1.511array([[       nan,        nan,        nan, ..., 1.16476321, 1.19559572,\n        1.22597193],\n       [       nan,        nan,        nan, ..., 0.12348971, 0.131342  ,\n        0.13737294],\n       [       nan,        nan,        nan, ..., 1.22653292, 1.21500516,\n        1.20868681],\n       [       nan,        nan,        nan, ..., 1.4938526 , 1.49647017,\n        1.5112594 ]])theta_school_diff(chain, draw, school, school_bis)float640.0 2.415 -2.631 ... -5.467 0.0array([[[[ 0.00000000e+00,  2.41531869e+00, -2.63092992e+00, ...,\n          -4.58110972e+00, -8.77373755e-01, -2.74068026e+00],\n         [-2.41531869e+00,  0.00000000e+00, -5.04624860e+00, ...,\n          -6.99642840e+00, -3.29269244e+00, -5.15599894e+00],\n         [ 2.63092992e+00,  5.04624860e+00,  0.00000000e+00, ...,\n          -1.95017980e+00,  1.75355616e+00, -1.09750340e-01],\n         ...,\n         [ 4.58110972e+00,  6.99642840e+00,  1.95017980e+00, ...,\n           0.00000000e+00,  3.70373596e+00,  1.84042946e+00],\n         [ 8.77373755e-01,  3.29269244e+00, -1.75355616e+00, ...,\n          -3.70373596e+00,  0.00000000e+00, -1.86330650e+00],\n         [ 2.74068026e+00,  5.15599894e+00,  1.09750340e-01, ...,\n          -1.84042946e+00,  1.86330650e+00,  0.00000000e+00]],\n\n        [[ 0.00000000e+00,  2.15629954e+00,  8.14635996e+00, ...,\n           8.89253514e+00,  1.23040040e+00,  5.10889901e+00],\n         [-2.15629954e+00,  0.00000000e+00,  5.99006042e+00, ...,\n           6.73623560e+00, -9.25899137e-01,  2.95259947e+00],\n         [-8.14635996e+00, -5.99006042e+00,  0.00000000e+00, ...,\n           7.46175179e-01, -6.91595956e+00, -3.03746095e+00],\n...\n         [-2.65379240e+00, -6.02529245e+00, -2.92707633e+00, ...,\n           0.00000000e+00,  4.32859842e-01, -6.92332351e+00],\n         [-3.08665224e+00, -6.45815229e+00, -3.35993618e+00, ...,\n          -4.32859842e-01,  0.00000000e+00, -7.35618335e+00],\n         [ 4.26953111e+00,  8.98031057e-01,  3.99624717e+00, ...,\n           6.92332351e+00,  7.35618335e+00,  0.00000000e+00]],\n\n        [[ 0.00000000e+00, -6.30547202e+00,  1.08737981e+00, ...,\n          -7.74350435e+00, -6.56949881e+00, -1.10209482e+00],\n         [ 6.30547202e+00,  0.00000000e+00,  7.39285182e+00, ...,\n          -1.43803234e+00, -2.64026792e-01,  5.20337720e+00],\n         [-1.08737981e+00, -7.39285182e+00,  0.00000000e+00, ...,\n          -8.83088416e+00, -7.65687862e+00, -2.18947462e+00],\n         ...,\n         [ 7.74350435e+00,  1.43803234e+00,  8.83088416e+00, ...,\n           0.00000000e+00,  1.17400554e+00,  6.64140953e+00],\n         [ 6.56949881e+00,  2.64026792e-01,  7.65687862e+00, ...,\n          -1.17400554e+00,  0.00000000e+00,  5.46740399e+00],\n         [ 1.10209482e+00, -5.20337720e+00,  2.18947462e+00, ...,\n          -6.64140953e+00, -5.46740399e+00,  0.00000000e+00]]]])Indexes: (4)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))schoolPandasIndexPandasIndex(Index(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', 'St. Paul's', 'Mt. Hermon'],\n      dtype='object', name='school'))school_bisPandasIndexPandasIndex(Index(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', 'St. Paul's', 'Mt. Hermon'],\n      dtype='object', name='school_bis'))Attributes: (6)created_at :2022-10-13T14:37:37.315398arviz_version :0.13.0.dev0inference_library :pymcinference_library_version :4.2.2sampling_time :7.480114936828613tuning_steps :1000\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis same operation using NumPy would require manual alignment of the two arrays to make sure they broadcast correctly. The could be something like:\ntheta_school_diff = theta[:, :, :, None] - theta[:, :, None, :]\n\n\n\n\n2.3.2 Advanced subsetting\nTo select the value corresponding to the difference between the Choate and Deerfield schools do:\n\npost[\"theta_school_diff\"].sel(school=\"Choate\", school_bis=\"Deerfield\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'theta_school_diff' (chain: 4, draw: 500)&gt;\n2.415 2.156 -0.04943 1.228 3.384 9.662 ... -1.656 -0.4021 1.524 -3.372 -6.305\nCoordinates:\n  * chain       (chain) int64 0 1 2 3\n  * draw        (draw) int64 0 1 2 3 4 5 6 7 ... 492 493 494 495 496 497 498 499\n    school      &lt;U16 'Choate'\n    school_bis  &lt;U16 'Deerfield'xarray.DataArray'theta_school_diff'chain: 4draw: 5002.415 2.156 -0.04943 1.228 3.384 ... -0.4021 1.524 -3.372 -6.305array([[ 2.41531869,  2.15629954, -0.04942665, ..., -1.568983  ,\n         3.49509445, -0.75245938],\n       [-1.36296658,  3.18605183, -3.57582959, ..., -0.03288082,\n         0.81613882,  0.93268411],\n       [11.85464717, -5.43221857,  0.85662645, ..., -1.62004592,\n         3.99854315, -0.87783417],\n       [ 2.95365309,  2.07818191,  4.51853032, ...,  1.5236138 ,\n        -3.37150005, -6.30547202]])Coordinates: (4)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])school()&lt;U16'Choate'array('Choate', dtype='&lt;U16')school_bis()&lt;U16'Deerfield'array('Deerfield', dtype='&lt;U16')Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))Attributes: (0)\n\n\nFor more advanced subsetting (the equivalent to what is sometimes called “fancy indexing” in NumPy) you need to provide the indices as DataArray objects:\n\nschool_idx = xr.DataArray([\"Choate\", \"Hotchkiss\", \"Mt. Hermon\"], dims=[\"pairwise_school_diff\"])\nschool_bis_idx = xr.DataArray(\n    [\"Deerfield\", \"Choate\", \"Lawrenceville\"], dims=[\"pairwise_school_diff\"]\n)\npost[\"theta_school_diff\"].sel(school=school_idx, school_bis=school_bis_idx)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'theta_school_diff' (chain: 4, draw: 500,\n                                       pairwise_school_diff: 3)&gt;\n2.415 -6.741 -1.84 2.156 -3.474 3.784 ... -2.619 6.923 -6.305 1.667 -6.641\nCoordinates:\n  * chain       (chain) int64 0 1 2 3\n  * draw        (draw) int64 0 1 2 3 4 5 6 7 ... 492 493 494 495 496 497 498 499\n    school      (pairwise_school_diff) &lt;U16 'Choate' 'Hotchkiss' 'Mt. Hermon'\n    school_bis  (pairwise_school_diff) &lt;U16 'Deerfield' 'Choate' 'Lawrenceville'\nDimensions without coordinates: pairwise_school_diffxarray.DataArray'theta_school_diff'chain: 4draw: 500pairwise_school_diff: 32.415 -6.741 -1.84 2.156 -3.474 ... -2.619 6.923 -6.305 1.667 -6.641array([[[  2.41531869,  -6.74108399,  -1.84042946],\n        [  2.15629954,  -3.47410767,   3.78363613],\n        [ -0.04942665,   4.28447846,   0.62431982],\n        ...,\n        [ -1.568983  ,   5.53829038,  -3.9072901 ],\n        [  3.49509445, -12.62680339,   9.21627977],\n        [ -0.75245938,  -7.16363884,  14.24249242]],\n\n       [[ -1.36296658,  -2.09672634,  -1.78220365],\n        [  3.18605183,   1.39310745,   1.30385219],\n        [ -3.57582959,  -3.73894295,   1.50957259],\n        ...,\n        [ -0.03288082,   0.22920766,   4.259231  ],\n        [  0.81613882,  -1.39409761,  -2.83464009],\n        [  0.93268411,   0.99243438,   1.52660383]],\n\n       [[ 11.85464717,  -4.83033352,  -2.06409623],\n        [ -5.43221857,  -2.45302911,  -0.29934021],\n        [  0.85662645,  -6.70463782,  -4.8911762 ],\n        ...,\n        [ -1.62004592,  -0.95078454,  -1.95522498],\n        [  3.99854315,  -0.85757228,   6.01710874],\n        [ -0.87783417,  -0.5224125 ,  -4.44552037]],\n\n       [[  2.95365309, -17.43005136,   4.73220433],\n        [  2.07818191,   3.61925306,   5.02518058],\n        [  4.51853032, -10.373311  ,  -2.59184142],\n        ...,\n        [  1.5236138 ,  -0.88482747,  -7.33031773],\n        [ -3.37150005,  -2.61944346,   6.92332351],\n        [ -6.30547202,   1.66679103,  -6.64140953]]])Coordinates: (4)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])school(pairwise_school_diff)&lt;U16'Choate' 'Hotchkiss' 'Mt. Hermon'array(['Choate', 'Hotchkiss', 'Mt. Hermon'], dtype='&lt;U16')school_bis(pairwise_school_diff)&lt;U16'Deerfield' ... 'Lawrenceville'array(['Deerfield', 'Choate', 'Lawrenceville'], dtype='&lt;U16')Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))Attributes: (0)\n\n\nUsing lists or NumPy arrays instead of DataArrays does column/row-based indexing. As you can see, the result has 9 values of theta_shool_diff instead of the 3 pairs of difference we selected in the previous cell:\n\npost[\"theta_school_diff\"].sel(\n    school=[\"Choate\", \"Hotchkiss\", \"Mt. Hermon\"],\n    school_bis=[\"Deerfield\", \"Choate\", \"Lawrenceville\"],\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'theta_school_diff' (chain: 4, draw: 500, school: 3,\n                                       school_bis: 3)&gt;\n2.415 0.0 -4.581 -4.326 -6.741 -11.32 ... 1.667 -6.077 -5.203 1.102 -6.641\nCoordinates:\n  * chain       (chain) int64 0 1 2 3\n  * draw        (draw) int64 0 1 2 3 4 5 6 7 ... 492 493 494 495 496 497 498 499\n  * school      (school) &lt;U16 'Choate' 'Hotchkiss' 'Mt. Hermon'\n  * school_bis  (school_bis) &lt;U16 'Deerfield' 'Choate' 'Lawrenceville'xarray.DataArray'theta_school_diff'chain: 4draw: 500school: 3school_bis: 32.415 0.0 -4.581 -4.326 -6.741 ... 1.667 -6.077 -5.203 1.102 -6.641array([[[[  2.41531869,   0.        ,  -4.58110972],\n         [ -4.3257653 ,  -6.74108399, -11.3221937 ],\n         [  5.15599894,   2.74068026,  -1.84042946]],\n\n        [[  2.15629954,   0.        ,   8.89253514],\n         [ -1.31780813,  -3.47410767,   5.41842747],\n         [ -2.95259947,  -5.10889901,   3.78363613]],\n\n        [[ -0.04942665,   0.        ,  -2.43482132],\n         [  4.2350518 ,   4.28447846,   1.84965714],\n         [  3.00971448,   3.05914114,   0.62431982]],\n\n        ...,\n\n        [[ -1.568983  ,   0.        ,  -7.17971855],\n         [  3.96930738,   5.53829038,  -1.64142817],\n         [  1.70344545,   3.27242845,  -3.9072901 ]],\n\n        [[  3.49509445,   0.        ,  13.23754201],\n         [ -9.13170894, -12.62680339,   0.61073863],\n...\n         [  5.69743498,   3.61925306,  12.43928685],\n         [ -1.71667129,  -3.7948532 ,   5.02518058]],\n\n        [[  4.51853032,   0.        ,  -1.82804532],\n         [ -5.85478068, -10.373311  , -12.20135632],\n         [  3.75473422,  -0.7637961 ,  -2.59184142]],\n\n        ...,\n\n        [[  1.5236138 ,   0.        ,  -1.17473412],\n         [  0.63878633,  -0.88482747,  -2.05956159],\n         [ -4.63196981,  -6.15558361,  -7.33031773]],\n\n        [[ -3.37150005,   0.        ,   2.6537924 ],\n         [ -5.99094351,  -2.61944346,   0.03434894],\n         [  0.89803106,   4.26953111,   6.92332351]],\n\n        [[ -6.30547202,   0.        ,  -7.74350435],\n         [ -4.63868099,   1.66679103,  -6.07671332],\n         [ -5.2033772 ,   1.10209482,  -6.64140953]]]])Coordinates: (4)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])school(school)&lt;U16'Choate' 'Hotchkiss' 'Mt. Hermon'array(['Choate', 'Hotchkiss', 'Mt. Hermon'], dtype='&lt;U16')school_bis(school_bis)&lt;U16'Deerfield' ... 'Lawrenceville'array(['Deerfield', 'Choate', 'Lawrenceville'], dtype='&lt;U16')Indexes: (4)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))schoolPandasIndexPandasIndex(Index(['Choate', 'Hotchkiss', 'Mt. Hermon'], dtype='object', name='school'))school_bisPandasIndexPandasIndex(Index(['Deerfield', 'Choate', 'Lawrenceville'], dtype='object', name='school_bis'))Attributes: (0)\n\n\n\n\n2.3.3 Add new chains using concat\nAfter checking the mcse and realizing you need more samples, you rerun the model with two chains and obtain an idata_rerun object.\n\nidata_rerun = (\n    idata.sel(chain=[0, 1])\n    .copy()\n    .assign_coords(coords={\"chain\": [4, 5]}, groups=\"posterior_groups\")\n)\n\nYou can combine the two into a single InferenceData object using the concat function from ArviZ:\n\nidata_complete = az.concat(idata, idata_rerun, dim=\"chain\")\nidata_complete.posterior.dims[\"chain\"]\n\n6\n\n\n\n\n2.3.4 Add groups to InferenceData objects\nTo add new groups to InferenceData objects you can use the extend method if the new groups are already in an InferenceData object or the add_groups method if the new groups are dictionaries or xarray.Dataset objects.\n\nrng = np.random.default_rng(3)\nidata.add_groups(\n    {\"predictions\": {\"obs\": rng.normal(size=(4, 500, 2))}},\n    dims={\"obs\": [\"new_school\"]},\n    coords={\"new_school\": [\"Essex College\", \"Moordale\"]},\n)\nidata\n\n\n\n            \n              \n                arviz.InferenceData\n              \n              \n              \n            \n                  \n                  posterior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:            (chain: 4, draw: 500, school: 8, school_bis: 8)\nCoordinates:\n  * chain              (chain) int64 0 1 2 3\n  * draw               (draw) int64 0 1 2 3 4 5 6 ... 494 495 496 497 498 499\n  * school             (school) &lt;U16 'Choate' 'Deerfield' ... 'Mt. Hermon'\n  * school_bis         (school_bis) &lt;U16 'Choate' 'Deerfield' ... 'Mt. Hermon'\nData variables:\n    mu                 (chain, draw) float64 7.872 3.385 9.1 ... 3.486 3.404\n    theta              (chain, draw, school) float64 12.32 9.905 ... 6.762 1.295\n    tau                (chain, draw) float64 4.726 3.909 4.844 ... 2.932 4.461\n    log_tau            (chain, draw) float64 1.553 1.363 1.578 ... 1.076 1.495\n    mlogtau            (chain, draw) float64 nan nan nan ... 1.494 1.496 1.511\n    theta_school_diff  (chain, draw, school, school_bis) float64 0.0 ... 0.0\nAttributes: (6)xarray.DatasetDimensions:chain: 4draw: 500school: 8school_bis: 8Coordinates: (4)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')school_bis(school_bis)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (6)mu(chain, draw)float647.872 3.385 9.1 ... 3.486 3.404array([[7.871796, 3.384554, 9.100476, ..., 3.656804, 5.427279, 2.735883],\n       [4.315817, 4.597701, 6.213762, ..., 4.841024, 5.946105, 4.666438],\n       [4.527603, 5.862628, 5.910703, ..., 2.871067, 4.095536, 1.775718],\n       [7.179504, 6.539699, 7.48362 , ..., 1.766733, 3.486112, 3.404464]])theta(chain, draw, school)float6412.32 9.905 14.95 ... 6.762 1.295array([[[12.320686,  9.905367, ..., 13.198059, 15.061366],\n        [11.285623,  9.129324, ..., 10.055223,  6.176724],\n        ...,\n        [10.402501,  6.907407, ...,  5.391061,  6.381239],\n        [ 6.661308,  7.413767, ...,  7.56657 ,  9.987618]],\n\n       [[ 5.596639,  6.959605, ...,  7.148854,  3.7909  ],\n        [ 4.96541 ,  1.779358, ...,  3.289161,  5.937311],\n        ...,\n        [ 7.030604,  6.214465, ...,  4.778138,  4.357112],\n        [ 4.884931,  3.952247, ...,  5.687935,  6.777772]],\n\n       [[11.81417 , -0.040477, ...,  0.318087,  0.661304],\n        [ 4.743465, 10.175684, ..., 11.271711,  7.12114 ],\n        ...,\n        [ 5.677651,  1.679108, ...,  1.280615,  7.627658],\n        [ 1.625447,  2.503281, ...,  2.362756, -2.967994]],\n\n       [[ 8.344508,  5.390855, ..., 12.46814 , 12.607797],\n        [ 8.93115 ,  6.852969, ...,  7.013971,  5.136297],\n        ...,\n        [ 4.182751,  7.554251, ...,  1.096098,  8.452282],\n        [ 0.192956,  6.498428, ...,  6.762455,  1.295051]]])tau(chain, draw)float644.726 3.909 4.844 ... 2.932 4.461array([[4.72574 , 3.908994, 4.844025, ..., 1.893838, 5.920062, 4.325896],\n       [1.97083 , 2.049029, 2.123765, ..., 2.17459 , 1.327551, 1.211995],\n       [3.501277, 2.893243, 4.273286, ..., 4.08978 , 2.72017 , 1.917011],\n       [6.07326 , 3.771867, 3.170537, ..., 2.740607, 2.932379, 4.461246]])log_tau(chain, draw)float641.553 1.363 1.578 ... 1.076 1.495array([[1.55302418, 1.36327995, 1.57774603, ..., 0.6386056 , 1.778347  ,\n        1.46461921],\n       [0.67845483, 0.71736608, 0.75319044, ..., 0.77684015, 0.28333575,\n        0.19226749],\n       [1.25312773, 1.06237808, 1.45238307, ..., 1.40849125, 1.00069436,\n        0.65076731],\n       [1.8038955 , 1.32757011, 1.15390112, ..., 1.00817931, 1.07581413,\n        1.49542809]])mlogtau(chain, draw)float64nan nan nan ... 1.494 1.496 1.511array([[       nan,        nan,        nan, ..., 1.16476321, 1.19559572,\n        1.22597193],\n       [       nan,        nan,        nan, ..., 0.12348971, 0.131342  ,\n        0.13737294],\n       [       nan,        nan,        nan, ..., 1.22653292, 1.21500516,\n        1.20868681],\n       [       nan,        nan,        nan, ..., 1.4938526 , 1.49647017,\n        1.5112594 ]])theta_school_diff(chain, draw, school, school_bis)float640.0 2.415 -2.631 ... -5.467 0.0array([[[[ 0.00000000e+00,  2.41531869e+00, -2.63092992e+00, ...,\n          -4.58110972e+00, -8.77373755e-01, -2.74068026e+00],\n         [-2.41531869e+00,  0.00000000e+00, -5.04624860e+00, ...,\n          -6.99642840e+00, -3.29269244e+00, -5.15599894e+00],\n         [ 2.63092992e+00,  5.04624860e+00,  0.00000000e+00, ...,\n          -1.95017980e+00,  1.75355616e+00, -1.09750340e-01],\n         ...,\n         [ 4.58110972e+00,  6.99642840e+00,  1.95017980e+00, ...,\n           0.00000000e+00,  3.70373596e+00,  1.84042946e+00],\n         [ 8.77373755e-01,  3.29269244e+00, -1.75355616e+00, ...,\n          -3.70373596e+00,  0.00000000e+00, -1.86330650e+00],\n         [ 2.74068026e+00,  5.15599894e+00,  1.09750340e-01, ...,\n          -1.84042946e+00,  1.86330650e+00,  0.00000000e+00]],\n\n        [[ 0.00000000e+00,  2.15629954e+00,  8.14635996e+00, ...,\n           8.89253514e+00,  1.23040040e+00,  5.10889901e+00],\n         [-2.15629954e+00,  0.00000000e+00,  5.99006042e+00, ...,\n           6.73623560e+00, -9.25899137e-01,  2.95259947e+00],\n         [-8.14635996e+00, -5.99006042e+00,  0.00000000e+00, ...,\n           7.46175179e-01, -6.91595956e+00, -3.03746095e+00],\n...\n         [-2.65379240e+00, -6.02529245e+00, -2.92707633e+00, ...,\n           0.00000000e+00,  4.32859842e-01, -6.92332351e+00],\n         [-3.08665224e+00, -6.45815229e+00, -3.35993618e+00, ...,\n          -4.32859842e-01,  0.00000000e+00, -7.35618335e+00],\n         [ 4.26953111e+00,  8.98031057e-01,  3.99624717e+00, ...,\n           6.92332351e+00,  7.35618335e+00,  0.00000000e+00]],\n\n        [[ 0.00000000e+00, -6.30547202e+00,  1.08737981e+00, ...,\n          -7.74350435e+00, -6.56949881e+00, -1.10209482e+00],\n         [ 6.30547202e+00,  0.00000000e+00,  7.39285182e+00, ...,\n          -1.43803234e+00, -2.64026792e-01,  5.20337720e+00],\n         [-1.08737981e+00, -7.39285182e+00,  0.00000000e+00, ...,\n          -8.83088416e+00, -7.65687862e+00, -2.18947462e+00],\n         ...,\n         [ 7.74350435e+00,  1.43803234e+00,  8.83088416e+00, ...,\n           0.00000000e+00,  1.17400554e+00,  6.64140953e+00],\n         [ 6.56949881e+00,  2.64026792e-01,  7.65687862e+00, ...,\n          -1.17400554e+00,  0.00000000e+00,  5.46740399e+00],\n         [ 1.10209482e+00, -5.20337720e+00,  2.18947462e+00, ...,\n          -6.64140953e+00, -5.46740399e+00,  0.00000000e+00]]]])Indexes: (4)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))schoolPandasIndexPandasIndex(Index(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', 'St. Paul's', 'Mt. Hermon'],\n      dtype='object', name='school'))school_bisPandasIndexPandasIndex(Index(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', 'St. Paul's', 'Mt. Hermon'],\n      dtype='object', name='school_bis'))Attributes: (6)created_at :2022-10-13T14:37:37.315398arviz_version :0.13.0.dev0inference_library :pymcinference_library_version :4.2.2sampling_time :7.480114936828613tuning_steps :1000\n                      \n                  \n            \n            \n            \n                  \n                  posterior_predictive\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (chain: 4, draw: 500, school: 8)\nCoordinates:\n  * chain    (chain) int64 0 1 2 3\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 ... 492 493 494 495 496 497 498 499\n  * school   (school) &lt;U16 'Choate' 'Deerfield' ... \"St. Paul's\" 'Mt. Hermon'\nData variables:\n    obs      (chain, draw, school) float64 ...\nAttributes: (4)xarray.DatasetDimensions:chain: 4draw: 500school: 8Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (1)obs(chain, draw, school)float64...[16000 values with dtype=float64]Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))schoolPandasIndexPandasIndex(Index(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', 'St. Paul's', 'Mt. Hermon'],\n      dtype='object', name='school'))Attributes: (4)arviz_version :0.13.0.dev0created_at :2022-10-13T14:37:41.460544inference_library :pymcinference_library_version :4.2.2\n                      \n                  \n            \n            \n            \n                  \n                  predictions\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:     (chain: 4, draw: 500, new_school: 2)\nCoordinates:\n  * chain       (chain) int64 0 1 2 3\n  * draw        (draw) int64 0 1 2 3 4 5 6 7 ... 492 493 494 495 496 497 498 499\n  * new_school  (new_school) &lt;U13 'Essex College' 'Moordale'\nData variables:\n    obs         (chain, draw, new_school) float64 2.041 -2.556 ... -0.2822\nAttributes: (2)xarray.DatasetDimensions:chain: 4draw: 500new_school: 2Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])new_school(new_school)&lt;U13'Essex College' 'Moordale'array(['Essex College', 'Moordale'], dtype='&lt;U13')Data variables: (1)obs(chain, draw, new_school)float642.041 -2.556 ... -1.015 -0.2822array([[[ 2.04091912, -2.55566503],\n        [ 0.41809885, -0.56776961],\n        [-0.45264929, -0.21559716],\n        ...,\n        [-0.80265585,  0.40858787],\n        [ 0.89066617,  0.91324226],\n        [ 0.30152948, -2.85103878]],\n\n       [[-1.02941822,  0.81504467],\n        [-0.86725243, -1.00340203],\n        [-2.30495532,  1.26656886],\n        ...,\n        [-1.40028095,  1.9391935 ],\n        [-0.37582993, -0.76872586],\n        [ 0.11466401, -0.89829659]],\n\n       [[ 0.02963037, -0.96028439],\n        [ 0.56533507,  0.05565896],\n        [-1.36828642,  1.0376982 ],\n        ...,\n        [ 0.23222422,  0.36513287],\n        [ 0.31840946, -0.56685801],\n        [ 2.39826354,  0.91078977]],\n\n       [[-1.42283401, -0.74058959],\n        [ 0.83390251,  0.53293412],\n        [ 0.13188271, -0.03434879],\n        ...,\n        [ 1.57846099,  0.24653314],\n        [ 0.64302486,  1.42710376],\n        [-1.01529472, -0.28215614]]])Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))new_schoolPandasIndexPandasIndex(Index(['Essex College', 'Moordale'], dtype='object', name='new_school'))Attributes: (2)created_at :2023-12-05T15:20:21.107981arviz_version :0.16.1\n                      \n                  \n            \n            \n            \n                  \n                  log_likelihood\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (chain: 4, draw: 500, school: 8)\nCoordinates:\n  * chain    (chain) int64 0 1 2 3\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 ... 492 493 494 495 496 497 498 499\n  * school   (school) &lt;U16 'Choate' 'Deerfield' ... \"St. Paul's\" 'Mt. Hermon'\nData variables:\n    obs      (chain, draw, school) float64 -4.173 -3.24 -4.321 ... -3.853 -3.986\nAttributes: (4)xarray.DatasetDimensions:chain: 4draw: 500school: 8Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (1)obs(chain, draw, school)float64-4.173 -3.24 ... -3.853 -3.986array([[[-4.173302, -3.239676, ..., -3.336817, -3.823773],\n        [-4.247812, -3.2279  , ..., -3.537121, -3.861641],\n        ...,\n        [-4.315149, -3.227492, ..., -4.01645 , -3.85803 ],\n        [-4.638855, -3.223242, ..., -3.765806, -3.81556 ]],\n\n       [[-4.742346, -3.226936, ..., -3.81026 , -3.913306],\n        [-4.806083, -3.415006, ..., -4.303568, -3.866033],\n        ...,\n        [-4.604134, -3.237464, ..., -4.095612, -3.899455],\n        [-4.814336, -3.303445, ..., -3.979458, -3.851396]],\n\n       [[-4.209169, -3.54477 , ..., -4.784774, -4.007715],\n        [-4.828914, -3.245192, ..., -3.447873, -3.846044],\n        ...,\n        [-4.734294, -3.421292, ..., -4.619213, -3.838812],\n        [-5.172804, -3.372593, ..., -4.444141, -4.155052]],\n\n       [[-4.485518, -3.255562, ..., -3.374531, -3.80988 ],\n        [-4.435035, -3.228102, ..., -3.824988, -3.882012],\n        ...,\n        [-4.88757 , -3.222517, ..., -4.650233, -3.828734],\n        [-5.345281, -3.232797, ..., -3.852936, -3.986156]]])Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))schoolPandasIndexPandasIndex(Index(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', 'St. Paul's', 'Mt. Hermon'],\n      dtype='object', name='school'))Attributes: (4)arviz_version :0.13.0.dev0created_at :2022-10-13T14:37:37.487399inference_library :pymcinference_library_version :4.2.2\n                      \n                  \n            \n            \n            \n                  \n                  sample_stats\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:              (chain: 4, draw: 500)\nCoordinates:\n  * chain                (chain) int64 0 1 2 3\n  * draw                 (draw) int64 0 1 2 3 4 5 6 ... 494 495 496 497 498 499\nData variables: (12/16)\n    max_energy_error     (chain, draw) float64 ...\n    energy_error         (chain, draw) float64 ...\n    lp                   (chain, draw) float64 ...\n    index_in_trajectory  (chain, draw) int64 ...\n    acceptance_rate      (chain, draw) float64 ...\n    diverging            (chain, draw) bool ...\n    ...                   ...\n    smallest_eigval      (chain, draw) float64 ...\n    step_size_bar        (chain, draw) float64 ...\n    step_size            (chain, draw) float64 ...\n    energy               (chain, draw) float64 ...\n    tree_depth           (chain, draw) int64 ...\n    perf_counter_diff    (chain, draw) float64 ...\nAttributes: (6)xarray.DatasetDimensions:chain: 4draw: 500Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])Data variables: (16)max_energy_error(chain, draw)float64...[2000 values with dtype=float64]energy_error(chain, draw)float64...[2000 values with dtype=float64]lp(chain, draw)float64...[2000 values with dtype=float64]index_in_trajectory(chain, draw)int64...[2000 values with dtype=int64]acceptance_rate(chain, draw)float64...[2000 values with dtype=float64]diverging(chain, draw)bool...[2000 values with dtype=bool]process_time_diff(chain, draw)float64...[2000 values with dtype=float64]n_steps(chain, draw)float64...[2000 values with dtype=float64]perf_counter_start(chain, draw)float64...[2000 values with dtype=float64]largest_eigval(chain, draw)float64...[2000 values with dtype=float64]smallest_eigval(chain, draw)float64...[2000 values with dtype=float64]step_size_bar(chain, draw)float64...[2000 values with dtype=float64]step_size(chain, draw)float64...[2000 values with dtype=float64]energy(chain, draw)float64...[2000 values with dtype=float64]tree_depth(chain, draw)int64...[2000 values with dtype=int64]perf_counter_diff(chain, draw)float64...[2000 values with dtype=float64]Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))Attributes: (6)arviz_version :0.13.0.dev0created_at :2022-10-13T14:37:37.324929inference_library :pymcinference_library_version :4.2.2sampling_time :7.480114936828613tuning_steps :1000\n                      \n                  \n            \n            \n            \n                  \n                  prior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (chain: 1, draw: 500, school: 8)\nCoordinates:\n  * chain    (chain) int64 0\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 ... 492 493 494 495 496 497 498 499\n  * school   (school) &lt;U16 'Choate' 'Deerfield' ... \"St. Paul's\" 'Mt. Hermon'\nData variables:\n    tau      (chain, draw) float64 ...\n    theta    (chain, draw, school) float64 ...\n    mu       (chain, draw) float64 ...\nAttributes: (4)xarray.DatasetDimensions:chain: 1draw: 500school: 8Coordinates: (3)chain(chain)int640array([0])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (3)tau(chain, draw)float64...[500 values with dtype=float64]theta(chain, draw, school)float64...[4000 values with dtype=float64]mu(chain, draw)float64...[500 values with dtype=float64]Indexes: (3)chainPandasIndexPandasIndex(Index([0], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))schoolPandasIndexPandasIndex(Index(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', 'St. Paul's', 'Mt. Hermon'],\n      dtype='object', name='school'))Attributes: (4)arviz_version :0.13.0.dev0created_at :2022-10-13T14:37:26.602116inference_library :pymcinference_library_version :4.2.2\n                      \n                  \n            \n            \n            \n                  \n                  prior_predictive\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (chain: 1, draw: 500, school: 8)\nCoordinates:\n  * chain    (chain) int64 0\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 ... 492 493 494 495 496 497 498 499\n  * school   (school) &lt;U16 'Choate' 'Deerfield' ... \"St. Paul's\" 'Mt. Hermon'\nData variables:\n    obs      (chain, draw, school) float64 ...\nAttributes: (4)xarray.DatasetDimensions:chain: 1draw: 500school: 8Coordinates: (3)chain(chain)int640array([0])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (1)obs(chain, draw, school)float64...[4000 values with dtype=float64]Indexes: (3)chainPandasIndexPandasIndex(Index([0], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))schoolPandasIndexPandasIndex(Index(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', 'St. Paul's', 'Mt. Hermon'],\n      dtype='object', name='school'))Attributes: (4)arviz_version :0.13.0.dev0created_at :2022-10-13T14:37:26.604969inference_library :pymcinference_library_version :4.2.2\n                      \n                  \n            \n            \n            \n                  \n                  observed_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (school: 8)\nCoordinates:\n  * school   (school) &lt;U16 'Choate' 'Deerfield' ... \"St. Paul's\" 'Mt. Hermon'\nData variables:\n    obs      (school) float64 ...\nAttributes: (4)xarray.DatasetDimensions:school: 8Coordinates: (1)school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (1)obs(school)float64...[8 values with dtype=float64]Indexes: (1)schoolPandasIndexPandasIndex(Index(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', 'St. Paul's', 'Mt. Hermon'],\n      dtype='object', name='school'))Attributes: (4)arviz_version :0.13.0.dev0created_at :2022-10-13T14:37:26.606375inference_library :pymcinference_library_version :4.2.2\n                      \n                  \n            \n            \n            \n                  \n                  constant_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (school: 8)\nCoordinates:\n  * school   (school) &lt;U16 'Choate' 'Deerfield' ... \"St. Paul's\" 'Mt. Hermon'\nData variables:\n    scores   (school) float64 ...\nAttributes: (4)xarray.DatasetDimensions:school: 8Coordinates: (1)school(school)&lt;U16'Choate' ... 'Mt. Hermon'array(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', \"St. Paul's\", 'Mt. Hermon'], dtype='&lt;U16')Data variables: (1)scores(school)float64...[8 values with dtype=float64]Indexes: (1)schoolPandasIndexPandasIndex(Index(['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter',\n       'Hotchkiss', 'Lawrenceville', 'St. Paul's', 'Mt. Hermon'],\n      dtype='object', name='school'))Attributes: (4)arviz_version :0.13.0.dev0created_at :2022-10-13T14:37:26.607471inference_library :pymcinference_library_version :4.2.2"
  },
  {
    "objectID": "InferenceData.html#final-remarks",
    "href": "InferenceData.html#final-remarks",
    "title": "2  Working with InferenceData",
    "section": "2.4 Final remarks",
    "text": "2.4 Final remarks\nWe have discussed a few of the most common operations with InferenceData objects. If you want to learn more about InferenceData, you can check the InferenceData API documentation.\nIf you have doubs about how to use InferenceData with ArviZ functions, you can ask questions at PyMC’s discourse"
  },
  {
    "objectID": "distributions.html#random-variables",
    "href": "distributions.html#random-variables",
    "title": "3  Random variables, distributions, and uncertainty",
    "section": "3.1 Random variables",
    "text": "3.1 Random variables\nFrom a Bayesian perspective probabilities represent a degree of (un)certainty about the occurrence of an event. It is a measure of the likelihood that a particular hypothesis or event is true, given the available data and prior knowledge. We assign the value 0 to something impossible and 1 to something certain. When we are unsure we assign a value in between. For example, we could say that the probability of rain tomorrow is 0.32. This means that we are 32% certain that it will rain tomorrow.\nIn practice we usually do not care about individual probabilities, instead we work with probability distributions. A probability distribution describes the probabilities associated with each possible outcome of an experiment. In statistics, the term “experiment” is used in a very wide sense. It could mean a well-planned experiment in a laboratory, but it could also mean the result of a poll, the observation of the weather tomorrow, or the number of people that will visit a website next week.\nLet’s consider the experiment of observing the weather tomorrow. The possible outcomes of this experiment include the following outcomes:\n\nRainy\nSunny\nCloudy\n\nNotice that we are omitting the possibility of snow, or hail. In other words, we are assigning 0 probability to those outcomes. It is usually the case that we do not ponder all the possible outcomes of an experiment, either because we deliberately assume them to be irrelevant, because we don’t know about them, or because is too complex/expensive/time-consuming/etc to take them all into account.\nAnother important thing to notice, from this example, is that these outcomes are words (or strings if you want). To work with them we need to assign a number to each outcome. For example, we could assign the numbers 0 to Rainy, 1 to Sunny, and 2 to Cloudy. This mapping from the outcomes to the numbers is called a random variable. This is a funny and potentially misleading name as its mathematical definition is not random (the mapping is deterministic) nor a variable (it is a function). The mapping is arbitrary, -1 to Rainy, 0 to Sunny, and 4 to Cloudy is also valid. But once we pick one mapping, we keep using it for the rest of the experiment or analysis. One common source of confusion is understanding where the randomness comes from if the mapping is deterministic. The randomness comes from the uncertainty about the outcome of the experiment, i.e. the weather tomorrow. We are not sure if it will be rainy tomorrow until tomorrow comes.\nRandom variables can be classified into two main types: discrete and continuous.\n\nDiscrete Random Variables: They can take on a countable number of distinct values. We already saw an example of a discrete random variable, the weather tomorrow. It can take on three values: Rainy, Sunny, and Cloudy. No intermediate values are allowed in our experiment, even when it is true that it can be partially sunny and still rain. And it has to be at least partially cloudy to rain. But we are not considering those possibilities.\nContinuous Random Variables: They can take on any value within a certain range. For example, the temperature tomorrow is a continuous random variable. If we use a Celcius scale, then it can take on any value between -273.15 Celsius to \\(+ \\infty\\). Of course, in practice, the expected temperature is restricted to a much narrower range. The lowest recorded temperature on Earth is −89.2 °C and the highest is 56.7 °C, and that range will be even narrower if we consider a particular region of our planet."
  },
  {
    "objectID": "distributions.html#probability-mass-and-density-functions",
    "href": "distributions.html#probability-mass-and-density-functions",
    "title": "3  Random variables, distributions, and uncertainty",
    "section": "3.2 Probability mass and density functions",
    "text": "3.2 Probability mass and density functions\nThe probability distribution of a discrete random variable is often described using a probability mass function (PMF), which gives the probability of each possible outcome. For instance, the following plot shows the probability mass function of a categorical distribution with three possible outcomes, like Rainy, Sunny, and Cloudy.\n\npz.Categorical([0.15, 0.6, 0.25]).plot_pdf();\n\n\n\n\nFigure 3.1: PMF of a Categorical disribution\n\n\n\n\nUsually, there is more than one probability distribution that we can use to represent the same set of probabilities, for instance, we could use a binomial distribution.\n\npz.Binomial(2, 0.6).plot_pdf();\n\n\n\n\nFigure 3.2: PMF of a Binomial distribution\n\n\n\n\nThe probability distribution of a continuous random variable is described using a probability density function (PDF), which specifies the likelihood of the random variable falling within a particular interval. For instance, we could use a normal distribution to describe the temperature tomorrow.\n\npz.Normal(30, 4).plot_pdf();\n\n\n\n\nFigure 3.3: PDF of a normal distribution\n\n\n\n\nor maybe a skew normal like this if we expect higher temperatures like during summer.\n\npz.SkewNormal(38, 5, -2).plot_pdf();\n\n\n\n\nFigure 3.4: PDF of a skew-normal distribution\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that we use plot_pdf() instead of plot_pmf(), this is because PreliZ uses the same method to plot both discrete and continuous distributions. In the case of discrete distributions, it will plot the probability mass function (PMF), and in the case of continuous distributions, it will plot the probability density function (PDF).\n\n\nOne issue when interpreting a PDF is that the y-axis is a density, not a probability. To get probability from a PDF we need to integrate the density over a given interval. This is something straightforward to do with a computer. But not that easy to do “visually”, human eyes/brains are not very good at that task. One way to alleviate this issue is to accompany a PDF with a point interval, like in the following plot.\n\npz.SkewNormal(38, 5, -2).plot_pdf(pointinterval=True);\n\n\n\n\nFigure 3.5: PDF of a SkewNormal distribution with a pointinterval\n\n\n\n\nThe point interval shows the quantiles of the distribution. Quantiles divide a dataset into equal probability intervals. For example, deciles divide a dataset into 10 equal-probability intervals, and quartiles divide a dataset into 4 equal-probability intervals. The most common quantile is the median (or 50th percentile), which divides a dataset into two equal-probability intervals where half of the data falls below the median and half of the data falls above the median.\nThe point interval in Figure 3.5 shows the 5th, 25th, 50th, 75th, and 95th percentiles. The point is the median. The tick line is the interquartile range (the central 50% of the distribution) and the thin line is the central 90% of the distribution."
  },
  {
    "objectID": "distributions.html#cumulative-distribution-functions",
    "href": "distributions.html#cumulative-distribution-functions",
    "title": "3  Random variables, distributions, and uncertainty",
    "section": "3.3 Cumulative distribution functions",
    "text": "3.3 Cumulative distribution functions\nIn the previous section, we saw that we can use PMFs and PDFs to represent the probability distribution of a random variable. But there are other ways to represent a distribution. For example, we could use the cumulative distribution function (CDF).\nThe CDF is defined as the probability that the random variable takes a value less than or equal to \\(x\\). The CDF is defined for both discrete and continuous random variables. Figure 3.6 shows the CDF of a categorical distribution with three possible outcomes. Compare it with Figure 3.1\n\npz.Categorical([0.15, 0.6, 0.25]).plot_cdf();\n\n\n\n\nFigure 3.6: CDF of a Categorical distribution\n\n\n\n\nFigure 3.7 shows the CDF of a normal distribution (compare it with Figure 3.3).\n\npz.Normal(30, 4).plot_cdf();\n\n\n\n\nFigure 3.7: CDF of a normal distribution\n\n\n\n\nThe CDF is usually easier to read than the PDF, as we already saw y-axis for a PDF is a density that has no intrinsic meaning, and to get probability from a PDF we need to evaluate areas. Instead for a CDF the y-axis is a probability. For the PMF/PDF it is easier to get the mode (the highest value for the point/curve), and for the CDF it is easier to get the median (the value of \\(x\\) for which \\(y=0.5\\)), or other quantiles. From the CDF it is also easier to quickly get quantities like the probability of getting a temperature equal or lower than 35 degrees. It is the value of the CDF at 35. From Figure 3.7 we can see that it is roughly 0.9 or 90%, if you want more accuracy you could use a matplotlib/ArviZ style with a grid (like arviz-darkgrid) or use the cdf() function.\n\npz.Normal(30, 4).cdf(35)\n\n0.8943502263331446\n\n\nFrom the CDF we can also easily get the probability of a range of values. For example, the probability of the temperature being between 25 and 35 degrees is the difference between the CDF at 35 and the CDF at 25. From Figure 3.7 we can get that it is roughly 0.9 or 90%. Again even when you can get a good estimate just by looking at the graph you can use the cdf() function to get a more accurate estimate. But the fact that you can get a good estimate by looking at the graph is a good feature.\n\nnp.diff(pz.Normal(30, 4).cdf([25, 35]))\n\narray([0.78870045])"
  },
  {
    "objectID": "distributions.html#inverse-cumulative-distribution-functions",
    "href": "distributions.html#inverse-cumulative-distribution-functions",
    "title": "3  Random variables, distributions, and uncertainty",
    "section": "3.4 Inverse cumulative distribution functions",
    "text": "3.4 Inverse cumulative distribution functions\nSometimes we may want to use the inverse of the CDF. This is known as the quantile function or the percent point function (PPF). The PPF is also defined for both discrete and continuous random variables. For example, Figure 3.8 shows the PPF of a categorical distribution with three possible outcomes and Figure 3.9 shows the PPF of a normal distribution.\n\npz.Categorical([0.15, 0.6, 0.25]).plot_ppf();\n\n\n\n\nFigure 3.8: PPF of a Categorical distribution\n\n\n\n\n\npz.Normal(30, 4).plot_ppf();\n\n\n\n\nFigure 3.9: PPF of a normal distribution"
  },
  {
    "objectID": "distributions.html#distributions-in-arviz",
    "href": "distributions.html#distributions-in-arviz",
    "title": "3  Random variables, distributions, and uncertainty",
    "section": "3.5 Distributions in ArviZ",
    "text": "3.5 Distributions in ArviZ\nThe PMF/PDF, CDF, and PPF are convenient ways to represent distributions for which we know the analytical form. But in practice, we often work with distributions that we don’t know their analytical form. Instead, we have a set of samples from the distribution. A clear example is a posterior distribution, computed using an MCMC method. For those cases, we still want useful visualization that we can use for ourselves or to show others. Some common methods are:\n\nHistograms\nKernel density estimation (KDE)\nEmpirical cumulative distribution function (ECDF)\nQuantile dot plots\n\nWe will discuss these methods in the next subsections with special emphasis on how they are implemented in ArviZ.\n\n3.5.1 Histograms\nHistograms are a very simple and effective way to represent a distribution. The basic idea is to divide the range of the data into a set of bins and count how many data points fall into each bin. Then we use as many bars as bins, with the height of the bars being proportional to the counts. The following video shows a step-by-step animation of a histogram being built.\nVideo\nHistograms can be used to represent both discrete and continuous random variables. Discrete variables are usually represented using integers. When ArviZ is asked to plot integer data it will use histograms as the default method. Arguably the most important parameter of a histogram is the number of bins. Too few bins and we will miss details, too many and we will plot noise. You can pick the number of bins with a bit of trial and error, especially when you are sure of what you want to show. However, there are many methods to compute the number of bins automatically from the data, like the Freedman–Diaconis rule or the Sturges’ rule. By default, ArviZ computes the number of bins using both rules and then picks the one that gives the largest number of bins. This is the same approach used by np.histogram(., bins=\"auto) and plt.hist(., bins=\"auto). Additionally, when the data is of type integers, ArviZ will preserve that structure and will associate bins to integers, instead of floats. If the number of unique integers is relatively small then, it will associate one bin to each integer. For example, in the following plot each bar is associated with an integer in the interval [0, 9].\n\nd_values = pz.Poisson(3).rvs(500)\naz.plot_dist(d_values);\n\n\n\n\nFigure 3.10: Histogram from a sample of integers. Each bin corresponds to a single integer.\n\n\n\n\nWhen the discrete values take higher values, like in Figure 3.11, bins are still associated with integers but many integers are binned together.\n\nd_values = pz.Poisson(100).rvs(500)\naz.plot_dist(d_values);\n\n\n\n\nFigure 3.11: Histogram from a sample of integers. Bins group together many integers.\n\n\n\n\nIf you don’t like the default binning criteria of ArviZ, you can change it by passing the bins argument using the hist_kwargs.\n\nd_values = pz.Poisson(100).rvs(500)\nax = az.plot_dist(d_values, hist_kwargs={\"bins\":\"auto\"})\nplt.setp(ax.get_xticklabels(), rotation=45);\n\n\n\n\nFigure 3.12: Histogram from a sample of integers, with bins automatically computed by Matplotlib, not ArviZ.\n\n\n\n\n\n\n3.5.2 KDE\nKernel density estimation (KDE) is a non-parametric way to estimate the probability density function from a sample. Intuitivelly you can think of it as the smooth version of a histogram. Conceptually you place a kernel function like a Gaussian on top of a data point, then you sum all the Gaussians, generally evaluated over a grid and not over the data points. Results are normalized so the total area under the curve is one. The following video shows a step-by-step animation of a KDE being built. You can see a version with border corrections and without them. Border corrections avoid adding a positive density outside the range of the data.\nVideo\nThe following block of code shows a very simple example of a KDE.\n\n_, ax = plt.subplots(figsize=(12, 4))\nbandwidth = 0.4\nnp.random.seed(19)\ndatapoints = 7\ny = np.random.normal(7, size=datapoints)\nx = np.linspace(y.min() - bandwidth * 3, y.max() + bandwidth * 3, 100)\nkernels = np.transpose([pz.Normal(i, bandwidth).pdf(x) for i in y])\nkernels *= 1/datapoints  # normalize the results\nax.plot(x, kernels, 'k--', alpha=0.5)\nax.plot(y, np.zeros(len(y)), 'C1o')\nax.plot(x, kernels.sum(1))\nax.set_xticks([])\nax.set_yticks([]);\n\n\n\n\nThe most important parameter of a KDE is the bandwidth which controls the degree of smoothness of the resulting curve. It is analogous to the number of bins for the histograms. ArviZ’s default method to compute the bandwidth works well for a wide range of distributions including multimodal ones. Compared to other KDEs in the Python ecosystem, the KDE implemented in ArviZ automatically handles the boundaries of a distribution. ArviZ will assign a density of zero to any point outside the range of the data.\nThe following example shows a KDE computed from a sample from a Gamma distribution. Notice that ArviZ computes a KDE instead of a histogram, and notice that there is no density for negative values.\n\nc_values = pz.Gamma(2, 3).rvs(1000)\naz.plot_dist(c_values);\n\n\n\n\nFigure 3.13: KDE from a sample of floats. By default, ArviZ computes a KDE instead of a histogram.\n\n\n\n\n\n\n3.5.3 ECDF\nBoth histograms and KDEs are ways to approximate the PMF/PDF of a distribution from a sample. But sometimes we may want to approximate the CDF instead. The empirical cumulative distribution function (ECDF) is a non-parametric way to estimate the CDF. It is a step function that jumps up by 1/N at each observed data point, where N is the total number of data points. The following video shows a step-by-step animation of an ECDF being built.\nVideo\nThe following block of code shows a very simple example of an ECDF.\n\nc_values = pz.Poisson(3).rvs(500)\naz.plot_ecdf(c_values);\n\n\n\n\nFigure 3.14: empirical cumulative distribution function\n\n\n\n\n\n\n3.5.4 Quantile dot plots\nA quantile dot plot displays the distribution of a sample in terms of its quantiles. Reading the median or other quantiles from quantile dot plots is generally easy, we just need to count the number of dots.\nThe folowing video shows a step-by-step animation of a quantile dot plot being built.\nVideo\nFrom Figure 3.15 we can easily see that 30% of the data is below 2. We do this by noticing that we have a total of 10 dots and 3 of them are below 2.\n\nc_values = pz.Poisson(2.4).rvs(500)\naz.plot_dot(c_values, nquantiles=10);\n\n\n\n\nFigure 3.15: Quantile dot plot\n\n\n\n\nThe number of quantiles (nquantiles) is something you will need to choose by yourself, usually, it is a good idea to keep this number relatively small and “round”, as the main feature of a quantile dot plot is that finding probability intervals reduces to counting dots. It is easier to count and compute proportion if you have 10, or 20 dots than if you have 11 or 57. But sometimes a larger number could be a good idea too, for instance, if you or your audience wants to focus on the tails of the distribution a larger number of dots will give you more resolution and you still will be counting only a rather small number dots so it will be easy to compute proportions."
  },
  {
    "objectID": "MCMC_diagnostics.html#from-the-mcmc-theory-to-practical-diagnostics",
    "href": "MCMC_diagnostics.html#from-the-mcmc-theory-to-practical-diagnostics",
    "title": "4  MCMC Diagnostics",
    "section": "4.1 From the MCMC theory to practical diagnostics",
    "text": "4.1 From the MCMC theory to practical diagnostics\nThe theory describes certain behaviors of MCMCs methods, many diagnoses are based on evaluating whether the theoretical results are empirically verified. For example, MCMC theory says that:\n\nThe initial value is irrelevant, we must always arrive at the same result\nThe samples are not really independent, but the value of a point only depends on the previous point, there are no long-range correlations.\nIf we look at the sample as a sequence we should not be able to find any patterns.\n\nFor example, for a sufficiently long sample, the first portion must be indistinguishable from the last (and so should any other combination of regions).\n\nFor the same problem, each sample generated will be different from the others, but for practical purposes, the samples should be indistinguishable from each other.\n\nWe are going to see that many diagnostics need multiple chains. Each chain is an independent MCMC run. The logic is that by comparing independent runs we can more easily sport issues than running a single instance. This multiple-chain approach also takes advantage of modern hardware. If you have a CPU with 4 cores you can get 4 independent chains in essentially the same time that one single chain.\nTo keep the focus on the diagnostics and not on any particular Bayesian model. We are going to first create 3 synthetic samples, we will use them to emulate samples from a posterior distribution.\n\ngood_sample: A random sample from a Gamma(2, 5). This is an example of a good sample because we are generating independent and identically distributed (iid) draws. This is the ideal scenario.\nbad_sample_0: We sorted good_sample, split it into two chains, and then added a small Gaussian error. This is a representation of a bad sample because values are not independent (we sorted the values!) and they do not come from the same distribution, because of the split the first half has values that are lower than the second. This represents a scenario where the sampler has very poor mixing.\nbad_sample_1: we start from good_chains, and turn into a poor sample by randomly introducing portions where consecutive samples are highly correlated to each other. This represents a common scenario, a sampler can resolve a region of the parameter space very well, but get stuck into one or more regions.\n\n\n\nShow the code for more details\ngood_sample = pz.Gamma(2, 5).rvs((2, 2000))\nbad_sample0 = np.random.normal(np.sort(good_sample, axis=None), 0.05,\n                               size=4000).reshape(2, -1)\n\nbad_sample1 = good_sample.copy()\nfor i in np.random.randint(1900, size=4):\n    bad_sample1[i%2:,i:i+100] = pz.Beta(i, 150).rvs(size=100)\n\nsample = {\"good_sample\":good_sample,\n          \"bad_sample_0\":bad_sample0,\n          \"bad_sample_1\":bad_sample1}\n\n\n\n\n\n\n\n\nNote\n\n\n\nAfter reading this chapter a good exercise is to come back here and modify these synthetic samples and run one or more diagnostics. If you want to make the exercise even more fun challenge yourself to predict what the diagnostics will be before running, or the other way around how you should change the samples to get a given result. This is a good test of your understanding and a good way to correct possible misunderstandings."
  },
  {
    "objectID": "MCMC_diagnostics.html#trace-plots",
    "href": "MCMC_diagnostics.html#trace-plots",
    "title": "4  MCMC Diagnostics",
    "section": "4.2 Trace plots",
    "text": "4.2 Trace plots\nA trace plot is created by drawing the sampled values at each iteration step. Ideally, we should expect to see a very noisy plot, some people call it a caterpillar. The reason is that draws should be uncorrelated from each other, the value of a draw should not provide any hint about the previous or next draw. Also, the draws from the first iterations should be indistinguishable from the ones coming from the last iterations the middle iterations, or any other region. The ideal scenario is the lack of any clear pattern as we can see at the right panel of Figure 4.1.\nIn ArviZ by calling the function az.plot_trace(.) we get a trace plot on the right and on the left a KDE (for continuous variables) or a histogram (for discrete variables). Figure 4.1 is an example of this. The KDE/histogram can help to spot differences between chains, ideally, distributions should overlap.\n\naz.plot_trace(sample, var_names=\"good_sample\");\n\n\n\n\nFigure 4.1: Trace plot of a sample without issues\n\n\n\n\nThis represents a scenario where the sampler is visiting two different regions of the parameter space and is not able to jump from one to the other. Additionally, it is also moving very slowly within each region. Figure 4.2 shows two problems. On the one hand, each chain is visiting a different region of the parameter space. We can see this from the trace plot itself and the KDE. On the other hand, even within each region, the sampler is having trouble properly exploring the space, notice how it keeps moving up, instead of being stationary.\n\n\nCode\naz.plot_trace(sample, var_names=\"bad_sample_0\");\n\n\n\n\n\nFigure 4.2: Trace plot of two chains that has not converged\n\n\n\n\nFinally from Figure 4.3, we can see another common scenario. This time we see that globally everything looks fine. But there are 3 regions where the sampler gets stuck, see the orange lines at the bottom of the traceplot. For the first two regions, starting at iterations 736 and 916, both chains got stuck. For the third region, starting in 1463, only one chain got stuck.\n\n\nCode\naxes = az.plot_trace(sample, var_names=\"bad_sample_1\")\nfor pos in [736, 916, 1463]:\n    axes[0, 1].plot((pos, pos+100), (0, 0), \"C2\", lw=3)\n\n\n\n\n\nFigure 4.3: Trace plot showing a sampler being stuck in some regions (see orange lines)\n\n\n\n\nTrace plots are probably the first plots we make after inference and also probably the most popular plots in Bayesian literature. But there are more as we will see next."
  },
  {
    "objectID": "MCMC_diagnostics.html#rank-plots",
    "href": "MCMC_diagnostics.html#rank-plots",
    "title": "4  MCMC Diagnostics",
    "section": "4.3 Rank plots",
    "text": "4.3 Rank plots\nThe basic idea is the following. For a parameter we take all the chains and order the values from lowest to highest and assign them a rank, that is, to the lowest value assign 0, to the following 1, and so on until we reach the total number of samples, (number of chains multiplied by the number of draws per chain). Then we regroup the rankings according to the chains that gave rise to them and for each chain we make a histogram. If the chains were indistinguishable we would expect the histograms to be uniform. Since there is no reason for one chain to have more low (or medium or high) rankings than the rest.\nFigure 4.4 shows rank plots for good_sample, bad_sample_0 and bad_sample_1. Notice how good_sample looks pretty close to uniform, bad_sample_1 is harder to tell, because overall it looks very close to uniform, except for the penultimate bar. On the contrary bad_sample_0 really looks bad.\n\n\nCode\n_, ax = plt.subplots(3, 1, figsize=(10, 8))\naz.plot_rank(sample, ax=ax);\n\n\n\n\n\nFigure 4.4: Rank plot for good_sample, bad_sample_0 and bad_sample_1"
  },
  {
    "objectID": "MCMC_diagnostics.html#hat-r-r-hat",
    "href": "MCMC_diagnostics.html#hat-r-r-hat",
    "title": "4  MCMC Diagnostics",
    "section": "4.4 \\(\\hat R\\) (R-hat)",
    "text": "4.4 \\(\\hat R\\) (R-hat)\nPlots are often useful for discovering patterns, but sometimes we want numbers, for example when quickly evaluating many parameters it may be easier to look at numbers than plots. Number are also easier to plug into some automatic routine, that call for human attention only if some threshold is exceeded.\n\\(\\hat R\\) is a numerical diagnostic that answers the question. Did the chains mix properly? But I also like to think of it as the score assigned by a jury in a trace (or rank) plot contest.\nThe version implemented in ArviZ does several things under the hood, but the central idea is that it compares the variance between chains with the variance within each chain. Ideally, we should get \\(\\hat R = 1\\), in practice \\(\\hat R \\lessapprox 1.01\\) are considered safe and in the first modeling phases, even higher values like \\(\\hat R \\approx 1.1\\) may be fine.\nUsing ArviZ we can get the \\(\\hat R\\) usando az.rhat(⋅), az.summary(⋅) and az.plot_forest(⋅, r_hat=True)\n\naz.rhat(sample)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:       ()\nData variables:\n    good_sample   float64 0.9999\n    bad_sample_0  float64 2.655\n    bad_sample_1  float64 1.007xarray.DatasetDimensions:Coordinates: (0)Data variables: (3)good_sample()float640.9999array(0.99990983)bad_sample_0()float642.655array(2.65486329)bad_sample_1()float641.007array(1.0074539)Indexes: (0)Attributes: (0)\n\n\n\n4.4.1 Effective Sample Size (ESS)\nSince the samples of an MCMC are (auto)correlated, the amount of “useful” information is less than a sample of the same size but iid.\nWe can estimate the effective sample size (ESS), that is, the size of a sample with the equivalent amount of information but without autocorrelation. This is useful to determine if the sample we have is large enough. It is recommended that the ESS be greater than 100 per chain. That is, for 4 chains we want a minimum of 400 effective samples.\nWith ArviZ we can get az.ess(⋅), az.summary(⋅) and az.plot_forest(⋅, ess=True)\n\naz.ess(sample)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:       ()\nData variables:\n    good_sample   float64 3.92e+03\n    bad_sample_0  float64 2.348\n    bad_sample_1  float64 218.9xarray.DatasetDimensions:Coordinates: (0)Data variables: (3)good_sample()float643.92e+03array(3920.35351664)bad_sample_0()float642.348array(2.34808285)bad_sample_1()float64218.9array(218.88660103)Indexes: (0)Attributes: (0)\n\n\nWe see that az.summary(⋅) returns two ESS values, ess_bulk and ess_tail. This is because different regions of the parameter space may have different ESS values since not all regions are sampled with the same efficiency. Intuitively, one may think that when sampling a distribution like a Gaussian it is easier to obtain better sample quality around the mean than around the tails, simply because we have more samples from that region. For some models, it could be the other way around, but the take-home message remains, not all regions are necessarily sampled with the same efficiency\n\naz.summary(sample, kind=\"diagnostics\")\n\n\n\n\n\n\n\n\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\ngood_sample\n0.004\n0.003\n3920.0\n4012.0\n1.00\n\n\nbad_sample_0\n0.182\n0.148\n2.0\n11.0\n2.65\n\n\nbad_sample_1\n0.022\n0.016\n219.0\n3817.0\n1.01\n\n\n\n\n\n\n\nIf we are going to use the MCMC samples to calculate central values such as means or medians then we have to make sure that the ess_bulk is sufficiently large, however, if we want to calculate intervals such as an HDI 94% we have to make sure that ess_tail be appropriate.\nArviZ offers several functions linked to the ESS. For example, if we want to evaluate the performance of the sampler for several regions at the same time we can use az.plot_ess.\n\n_, ax = plt.subplots(3, 1, figsize=(10, 8))\naz.plot_ess(sample, ax=ax);\n\n\n\n\nA simple way to increase the ESS is to increase the number of samples, but it could be the case that the ESS grows very slowly with the number of samples, so even if we increased the number of samples 10 times we could still be very far from our target value. One way to estimate “how many more samples do we need” is to use az.plot_ess(⋅, kind=\"evolution\"). This graph shows us how the ESS changed with each iteration, which allows us to make predictions.\nFrom Figure 4.5 we can see that the ESS grows linearly with the number of samples for good_sample, and it does not grow at all for bad_sample_0. In the latter case, this is an indication that there is virtually no hope of improving the ESS simply by increasing the number of draws.\n\n_, axes = plt.subplots(2, 1) \naz.plot_ess(sample, var_names=[\"good_sample\", \"bad_sample_0\"], kind=\"evolution\", ax=axes);\n\n\n\n\nFigure 4.5: ESS evolution plot for good_sample and bad_sample_0.\n\n\n\n\n\n\n4.4.2 Monte Carlo standard error (MCSE)\nAn advantage of the ESS is that it is scale-free, it does not matter if one parameter varies between 0.1 and 0.2 and another between -2000 and 0, an ESS of 400 has the same meaning for both parameters. In models with many parameters, we can quickly identify which parameters are most problematic. However, when reporting results it is not very informative to know whether the ESS was 1372 or 1501. Instead, we would like to know the order of the errors we are making when approximating the posterior. This information is given by the Monte Carlo standard error (MCSE). Like the ESS, the MCSE takes into account the autocorrelation of the samples. This error should be below the desired precision in our results. That is, if for a parameter the MCSE is 0.1, it does not make sense to report that the mean of that parameter is 3.15. Since the correct value could easily be between 3.4 and 2.8.\nWith ArviZ we can get the MCSE with az.mcse(⋅) or az.summary(⋅).\n\naz.mcse(sample)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:       ()\nData variables:\n    good_sample   float64 0.00447\n    bad_sample_0  float64 0.1823\n    bad_sample_1  float64 0.02204xarray.DatasetDimensions:Coordinates: (0)Data variables: (3)good_sample()float640.00447array(0.0044704)bad_sample_0()float640.1823array(0.18231423)bad_sample_1()float640.02204array(0.02203518)Indexes: (0)Attributes: (0)"
  },
  {
    "objectID": "MCMC_diagnostics.html#diagnosis-of-gradient-based-algorithms",
    "href": "MCMC_diagnostics.html#diagnosis-of-gradient-based-algorithms",
    "title": "4  MCMC Diagnostics",
    "section": "4.5 Diagnosis of gradient-based algorithms",
    "text": "4.5 Diagnosis of gradient-based algorithms\nDue to its internal workings, algorithms like NUTS offer some specific tests that are not available to other methods. These tests are generally very sensitive.\nTo exemplify this we are going to load two InferenceData from pre-calculated models. The details of how these data were generated are not relevant at the moment. We will only say that they are two models that are mathematically equivalent but parameterized in different ways. In this case, the parameterization affects the efficiency of the sampler. The centered model is sampled more efficiently than the uncentered model.\n\nidata_cm = az.load_arviz_data(\"centered_eight\")\nidata_ncm = az.load_arviz_data(\"non_centered_eight\")\n\n\n4.5.1 Transition energy vs marginal energy\nWe can think of a Hamiltonian Monte Carlo as a two-step process\n\nDeterministic sampling (following the Hamiltonian)\nA random walk in momentum space\n\nIf the transition energy distribution is similar to the marginal energy distribution, then NUTS can generate samples of the marginal energy distribution that are almost independent between transitions. We can evaluate this visually or numerically, calculating the Bayesian Fraction of Missing Information (BFMI), as shown in the following figure.\n\n_, axes = plt.subplots(1, 2, sharey=True, sharex=True, figsize=(12, 4), constrained_layout=True)\n\nfor ax, idata, nombre in zip(axes.ravel(), (idata_cm, idata_ncm), (\"centered\", \"non-centered\")):\n    az.plot_energy(idata, ax=ax)\n    ax.set_title(nombre)\n\n\n\n\n\n\n4.5.2 Divergences\nOne advantage of NUTS is that it fails with style. This happens, for example, when trying to go from regions of low curvature to regions of high curvature. In these cases, the numerical trajectories may diverge. Essentially this happens because in these cases there is no single set of hyper-parameters that allows efficient sampling of both regions. So one region is sampled properly and when the sampler moves to the other region it fails. Divergent numerical trajectories are extremely sensitive identifiers of pathological neighborhoods.\nThe following example shows two things the non-centered model shows several divergences (turquoise circles) grouped in one region. In the centered model, which has no divergence, you can see that around that same region, there are samples for smaller values of tau. That is to say, the ‘uncentered’ model fails to sample a region, but at least it warns that it is having problems sampling that region!\n\n_, axes = plt.subplots(1, 2, sharey=True, sharex=True, figsize=(10, 5), constrained_layout=True)\n\n\nfor ax, idata, nombre in zip(axes.ravel(), (idata_cm, idata_ncm), (\"centered\", \"non-centered\")):\n    az.plot_pair(idata, var_names=['theta', 'tau'], coords={'school':\"Choate\"}, kind='scatter',\n                 divergences=True, divergences_kwargs={'color':'C1'},\n                 ax=ax)\n    ax.set_title(nombre)"
  },
  {
    "objectID": "MCMC_diagnostics.html#what-to-do-when-the-diagnoses-are-wrong",
    "href": "MCMC_diagnostics.html#what-to-do-when-the-diagnoses-are-wrong",
    "title": "4  MCMC Diagnostics",
    "section": "4.6 What to do when the diagnoses are wrong?",
    "text": "4.6 What to do when the diagnoses are wrong?\n\nMore samples or more tuning steps. This is usually only useful when the problems are minor\nBurn-in. Modern software like PyMC uses several samples to tune the hyper-parameters of the sampling methods. By default, these samples are eliminated, so in general, it is not necessary to do Burn-in manually.\nChange sampling method!\nReparameterize the model\nImprove priors\n\nThe folk theorem of computational statistics: When you have computational problems, there is often a problem with your model. The recommendation is NOT to change the priors to improve sampling quality. The recommendation is that if the sampling is bad, perhaps the model is too. In that case, we can think about improving the model, one way to improve it is to use prior knowledge to improve the priors.\n\nSome models can be expressed in more than one way, all mathematically equivalent. In those cases, some parameterizations may be more efficient than others. For example, as we will see later with hierarchical linear models.\nIn the case of divergences, these are usually eliminated by increasing the acceptance rate, for instance in PyMC you can do pm.sample(..., target_accept=x) where x is 0.8 by default and the maximum value is 1. If you reach 0.99 you should probably do something else.\nModern probabilistic programming languages, usually provide useful warning messages and tips if they detect issues with sampling, paying attention to those messages can save you a lot of time."
  },
  {
    "objectID": "case_study_model_comparison.html#information-criteria-for-hierarchical-and-multi-likelihood-models",
    "href": "case_study_model_comparison.html#information-criteria-for-hierarchical-and-multi-likelihood-models",
    "title": "7  Model Comparison (case study)",
    "section": "7.1 Information criteria for hierarchical and multi-likelihood models",
    "text": "7.1 Information criteria for hierarchical and multi-likelihood models\nThere are many situations where one model can be used for several prediction tasks at the same time. Hierarchical models or models with multiple observations are examples of such cases. With two observations for example, the same model can be used to predict only the first observation, only the second or both observations at the same time.\nBefore estimating the predictive accuracy, there are two important questions to answer: what is the predictive task we are interested in and, whether or not the exchangeability criteria is met. This section will show several alternative ways to define the predictive task using the same model."
  },
  {
    "objectID": "case_study_model_comparison.html#the-data",
    "href": "case_study_model_comparison.html#the-data",
    "title": "7  Model Comparison (case study)",
    "section": "7.2 The data",
    "text": "7.2 The data\nWe are going to analyze data from the 2018-2019 season of Spain’s highest women’s football league. We will start by loading the already cleaned up data. It is a dataframe summarizing all the matches of the season. Each row represents a match. You can see the head of the dataframe below.\n\ndf = pd.read_csv(\"data/18-19_df.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nhome_team\naway_team\nhome_goals\naway_goals\n\n\n\n\n0\nAtlético de Madrid\nAthletic Club\n3\n0\n\n\n1\nBarcelona\nAthletic Club\n2\n1\n\n\n2\nR.C.D. Espanyol\nAthletic Club\n1\n2\n\n\n3\nFundación Albacete\nAthletic Club\n0\n1\n\n\n4\nGranadilla\nAthletic Club\n3\n1"
  },
  {
    "objectID": "case_study_model_comparison.html#base-model",
    "href": "case_study_model_comparison.html#base-model",
    "title": "7  Model Comparison (case study)",
    "section": "7.3 Base model",
    "text": "7.3 Base model\nThe model used is taken from this blog post which was added as an example notebook to PyMC docs. This notebook will only describe the model quite concisely and will not discuss the model implementation in order to focus on information criteria calculation. To read more about the models please refer to the two posts and references therein.\nWe are trying to model a league in which all teams play against each other twice. We indicate the number of goals scored by the home and the away team in the \\(g\\)-th game of the season (\\(n\\) matches) as \\(y_{g,h}\\) and \\(y_{g,a}\\) respectively. The model assumes the goals scored by a team follow a Poisson distribution:\n\\[y_{g,j} | \\theta_{g,j} \\sim \\text{Poiss}(\\theta_{g,j})\\]\nwhere \\(j = {h, a}\\) representing either home or away team. We will therefore start with a model containing two observation vectors: \\(\\mathbf{y_h} = (y_{1,h}, y_{2,h}, \\dots, y_{n,h})\\) and \\(\\mathbf{y_a} = (y_{1,a}, \\dots, y_{n,a})\\). In order to take into account each team’s scoring and defensive power and also the advantage of playing home, we will use different formulas for \\(\\theta_{g,h}\\) and for \\(\\theta_{g,a}\\):\n\\[\n\\begin{align}\n\\theta_{g,h} &= \\alpha + home + atts_{home\\_team} + defs_{away\\_team}\\\\\n\\theta_{g,a} &= \\alpha + atts_{away\\_team} + defs_{home\\_team}\n\\end{align}\n\\]\nThe expected number of goals score by the home team \\(\\theta_{g,h}\\) depends on an intercept, \\(\\alpha\\), \\(home\\) to quantify the home advantage, on the attacking power of the home team and on the defensive power of the away team. Similarly, the expected number of goals score by the away team \\(\\theta_{g,a}\\) also depends on the intercept but not on the home advantage, and now, consequently, we use the attacking power of the away team and the defensive power of the home team. Summing up and including the priors, our base model is the following one:\n\\[\n\\begin{align}\n\\alpha &\\sim \\text{Normal}(0,5) \\\\\nhome &\\sim \\text{Normal}(0,5) \\\\\nsd_{att} &\\sim \\text{HalfStudentT}(3,2.5) \\\\\nsd_{def} &\\sim \\text{HalfStudentT}(3,2.5) \\\\\natts_* &\\sim \\text{Normal}(0,sd_{att}) \\\\\ndefs_* &\\sim \\text{Normal}(0,sd_{def}) \\\\\n\\mathbf{y}_h &\\sim \\text{Poiss}(\\theta_h) \\\\\n\\mathbf{y}_a &\\sim \\text{Poiss}(\\theta_a)\n\\end{align}\n\\]\nwhere \\(\\theta_j\\) has been defined above, \\(atts = atts_* - \\text{mean}(atts_*)\\) and \\(defs\\) is defined like \\(atts\\).\n\n7.3.1 Data preparation\n\ndf = pd.read_csv(\"data/18-19_df.csv\")\nhome_team_idxs, team_names = pd.factorize(df.home_team, sort=True)\naway_team_idxs, _ = pd.factorize(df.away_team, sort=True)\nnum_teams = len(team_names)\ndf\n\n\n\n\n\n\n\n\nhome_team\naway_team\nhome_goals\naway_goals\n\n\n\n\n0\nAtlético de Madrid\nAthletic Club\n3\n0\n\n\n1\nBarcelona\nAthletic Club\n2\n1\n\n\n2\nR.C.D. Espanyol\nAthletic Club\n1\n2\n\n\n3\nFundación Albacete\nAthletic Club\n0\n1\n\n\n4\nGranadilla\nAthletic Club\n3\n1\n\n\n...\n...\n...\n...\n...\n\n\n235\nRayo Vallecano\nValencia\n1\n1\n\n\n236\nReal Betis\nValencia\n4\n0\n\n\n237\nReal Sociedad\nValencia\n6\n0\n\n\n238\nSevilla F.C.\nValencia\n2\n2\n\n\n239\nSporting Huelva\nValencia\n0\n2\n\n\n\n\n240 rows × 4 columns\n\n\n\n\n\n7.3.2 Model implementation\n\ncoords = {\"team\": team_names, \"match\": np.arange(len(df))}\nwith pm.Model(coords=coords) as m_base:\n    # constant data\n    home_team = pm.Data(\"home_team\", home_team_idxs, dims=\"match\")\n    away_team = pm.Data(\"away_team\", away_team_idxs, dims=\"match\")\n    \n    # global model parameters\n    home = pm.Normal('home', mu=0, sigma=5)\n    sd_att = pm.HalfStudentT('sd_att', nu=3, sigma=2.5)\n    sd_def = pm.HalfStudentT('sd_def', nu=3, sigma=2.5)\n    intercept = pm.Normal('intercept', mu=0, sigma=5)\n\n    # team-specific model parameters\n    atts_star = pm.Normal(\"atts_star\", mu=0, sigma=sd_att, dims=\"team\")\n    defs_star = pm.Normal(\"defs_star\", mu=0, sigma=sd_def, dims=\"team\")\n\n    atts = atts_star - pt.mean(atts_star)\n    defs = defs_star - pt.mean(defs_star)\n    home_theta = pt.exp(intercept + home + atts[home_team] + defs[away_team])\n    away_theta = pt.exp(intercept + atts[away_team] + defs[home_team])\n\n    # likelihood of observed data\n    home_goals = pm.Poisson('home_goals', mu=home_theta, observed=df.home_goals, dims=\"match\")\n    away_goals = pm.Poisson('away_goals', mu=away_theta, observed=df.away_goals, dims=\"match\")\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/pymc/data.py:433: UserWarning: The `mutable` kwarg was not specified. Before v4.1.0 it defaulted to `pm.Data(mutable=True)`, which is equivalent to using `pm.MutableData()`. In v4.1.0 the default changed to `pm.Data(mutable=False)`, equivalent to `pm.ConstantData`. Use `pm.ConstantData`/`pm.MutableData` or pass `pm.Data(..., mutable=False/True)` to avoid this warning.\n  warnings.warn(\n\n\n\n\n7.3.3 Inference\n\nwith m_base:\n    idata = pm.sample(draws=2000,\n                      random_seed=1375,\n                      idata_kwargs={\"log_likelihood\":True})\n\n# define helpers to make code less verbose\nlog_lik = idata.log_likelihood\nconst = idata.constant_data\nidata\n\nAuto-assigning NUTS sampler...\n\n\nInitializing NUTS using jitter+adapt_diag...\n\n\nMultiprocess sampling (2 chains in 2 jobs)\n\n\nNUTS: [home, sd_att, sd_def, intercept, atts_star, defs_star]\n\n\n\n\n\n\n\n    \n      \n      100.00% [6000/6000 00:06&lt;00:00 Sampling 2 chains, 0 divergences]\n    \n    \n\n\nSampling 2 chains for 1_000 tune and 2_000 draw iterations (2_000 + 4_000 draws total) took 7 seconds.\n\n\nWe recommend running at least 4 chains for robust computation of convergence diagnostics\n\n\n\n\n            \n              \n                arviz.InferenceData\n              \n              \n              \n            \n                  \n                  posterior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:    (chain: 2, draw: 2000, team: 16)\nCoordinates:\n  * chain      (chain) int64 0 1\n  * draw       (draw) int64 0 1 2 3 4 5 6 ... 1993 1994 1995 1996 1997 1998 1999\n  * team       (team) &lt;U18 'Athletic Club' 'Atlético de Madrid' ... 'Valencia'\nData variables:\n    home       (chain, draw) float64 0.3303 0.2422 0.1773 ... 0.2994 0.1682\n    intercept  (chain, draw) float64 0.07803 0.1418 0.1268 ... 0.1287 0.1314\n    atts_star  (chain, draw, team) float64 0.06324 0.8191 ... -0.4707 0.00939\n    defs_star  (chain, draw, team) float64 -0.274 -0.9993 ... 0.1082 0.1454\n    sd_att     (chain, draw) float64 0.3655 0.3003 0.3647 ... 0.559 0.382 0.4112\n    sd_def     (chain, draw) float64 0.5047 0.5138 0.373 ... 0.3951 0.26 0.39\nAttributes:\n    created_at:                 2023-12-05T15:21:50.617069\n    arviz_version:              0.16.1\n    inference_library:          pymc\n    inference_library_version:  5.9.2\n    sampling_time:              6.882747173309326\n    tuning_steps:               1000xarray.DatasetDimensions:chain: 2draw: 2000team: 16Coordinates: (3)chain(chain)int640 1array([0, 1])draw(draw)int640 1 2 3 4 ... 1996 1997 1998 1999array([   0,    1,    2, ..., 1997, 1998, 1999])team(team)&lt;U18'Athletic Club' ... 'Valencia'array(['Athletic Club', 'Atlético de Madrid', 'Barcelona',\n       'Fundación Albacete', 'Granadilla', 'Levante U.D.', 'Logroño',\n       'Madrid CFF', 'Málaga', 'R.C.D. Espanyol', 'Rayo Vallecano',\n       'Real Betis', 'Real Sociedad', 'Sevilla F.C.', 'Sporting Huelva',\n       'Valencia'], dtype='&lt;U18')Data variables: (6)home(chain, draw)float640.3303 0.2422 ... 0.2994 0.1682array([[0.33025565, 0.24223585, 0.17726687, ..., 0.32522131, 0.20223251,\n        0.24433803],\n       [0.26602128, 0.32492746, 0.28591786, ..., 0.21298872, 0.29935234,\n        0.16815886]])intercept(chain, draw)float640.07803 0.1418 ... 0.1287 0.1314array([[ 0.07802723,  0.14178739,  0.12683636, ...,  0.08309616,\n         0.16616687,  0.19786176],\n       [ 0.06812337,  0.08149914, -0.00064154, ...,  0.10259586,\n         0.12865788,  0.13136112]])atts_star(chain, draw, team)float640.06324 0.8191 ... -0.4707 0.00939array([[[ 6.32394057e-02,  8.19092443e-01,  8.82786572e-01, ...,\n         -4.64519446e-01, -5.41289988e-01,  2.99342368e-02],\n        [ 1.77773309e-01,  5.78138973e-01,  3.96728955e-01, ...,\n         -9.30439993e-02, -4.27678309e-01, -7.27937719e-03],\n        [-1.35306671e-01,  9.28428981e-01,  8.66425817e-01, ...,\n         -2.80665853e-01, -6.04099120e-01, -8.83440415e-02],\n        ...,\n        [ 2.47708954e-01,  8.22270725e-01,  7.34975296e-01, ...,\n         -1.19388780e-01, -4.04313809e-01, -1.78795982e-02],\n        [ 1.09462560e-01,  8.03529312e-01,  8.39011594e-01, ...,\n          6.66718535e-02, -5.14665128e-01,  1.07024558e-01],\n        [ 2.67160322e-01,  7.38044502e-01,  7.39337480e-01, ...,\n          1.04559270e-01, -4.28151623e-01,  2.02489927e-01]],\n\n       [[ 1.50378092e-01,  5.79927059e-01,  8.34890616e-01, ...,\n         -1.09276255e-01, -6.75972627e-01, -3.38240522e-02],\n        [ 2.69548999e-01,  8.94120223e-01,  8.71160217e-01, ...,\n         -2.20587718e-01, -8.43150674e-01,  5.30894446e-02],\n        [ 1.69716038e-01,  6.08157759e-01,  6.93504687e-01, ...,\n         -1.39719768e-01, -8.97556622e-01, -1.02442150e-01],\n        ...,\n        [-6.15063784e-02,  6.09530091e-01,  1.03990345e+00, ...,\n         -8.30199414e-02, -4.73209404e-01, -8.14571046e-04],\n        [ 2.17740787e-01,  9.44740401e-01,  4.60821264e-01, ...,\n         -5.71933331e-02, -7.34531979e-01, -8.89991566e-02],\n        [-1.75813175e-01,  7.36288251e-01,  7.44368614e-01, ...,\n         -1.51467495e-01, -4.70659329e-01,  9.38962404e-03]]])defs_star(chain, draw, team)float64-0.274 -0.9993 ... 0.1082 0.1454array([[[-0.27402214, -0.99925546, -1.0458019 , ...,  0.14750006,\n          0.06918717,  0.18319264],\n        [-0.17718307, -0.79935874, -0.7215234 , ...,  0.2806272 ,\n          0.06650013,  0.13915554],\n        [-0.29867411, -0.4438742 , -0.77200758, ...,  0.51888109,\n          0.31392382,  0.32966583],\n        ...,\n        [-0.41091855, -0.52799437, -1.04419966, ...,  0.37350983,\n          0.21896869,  0.04619507],\n        [-0.217103  , -0.61306137, -0.7447104 , ...,  0.14974671,\n         -0.13877154,  0.14658991],\n        [-0.16432819, -0.70451337, -0.86796041, ...,  0.27808369,\n         -0.02094467,  0.16901912]],\n\n       [[-0.45052056, -0.75186061, -0.55539057, ...,  0.35454325,\n          0.07604531,  0.34539367],\n        [-0.49179989, -0.58693665, -0.7021918 , ...,  0.26342724,\n          0.22108556,  0.4241595 ],\n        [-0.41412923, -0.60616616, -0.21852556, ...,  0.27426366,\n         -0.02382733,  0.30938454],\n        ...,\n        [-0.17290495, -0.47007336, -0.86291837, ...,  0.5327183 ,\n          0.09432486,  0.0627611 ],\n        [-0.20390833, -0.58138672, -0.29920526, ...,  0.03558123,\n          0.15069679,  0.32059115],\n        [-0.2159428 , -0.64666099, -0.76417527, ...,  0.34436052,\n          0.10820805,  0.14539877]]])sd_att(chain, draw)float640.3655 0.3003 ... 0.382 0.4112array([[0.36553623, 0.30032146, 0.36469028, ..., 0.48784092, 0.39135549,\n        0.36059542],\n       [0.45978508, 0.46531116, 0.36865158, ..., 0.55899123, 0.38196855,\n        0.41115099]])sd_def(chain, draw)float640.5047 0.5138 0.373 ... 0.26 0.39array([[0.50473589, 0.51383584, 0.37301589, ..., 0.47389092, 0.5149574 ,\n        0.49920773],\n       [0.40532747, 0.47034015, 0.41492502, ..., 0.39513957, 0.25996149,\n        0.3899949 ]])Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999],\n      dtype='int64', name='draw', length=2000))teamPandasIndexPandasIndex(Index(['Athletic Club', 'Atlético de Madrid', 'Barcelona',\n       'Fundación Albacete', 'Granadilla', 'Levante U.D.', 'Logroño',\n       'Madrid CFF', 'Málaga', 'R.C.D. Espanyol', 'Rayo Vallecano',\n       'Real Betis', 'Real Sociedad', 'Sevilla F.C.', 'Sporting Huelva',\n       'Valencia'],\n      dtype='object', name='team'))Attributes: (6)created_at :2023-12-05T15:21:50.617069arviz_version :0.16.1inference_library :pymcinference_library_version :5.9.2sampling_time :6.882747173309326tuning_steps :1000\n                      \n                  \n            \n            \n            \n                  \n                  log_likelihood\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:     (chain: 2, draw: 2000, match: 240)\nCoordinates:\n  * chain       (chain) int64 0 1\n  * draw        (draw) int64 0 1 2 3 4 5 6 ... 1994 1995 1996 1997 1998 1999\n  * match       (match) int64 0 1 2 3 4 5 6 7 ... 233 234 235 236 237 238 239\nData variables:\n    home_goals  (chain, draw, match) float64 -1.513 -1.454 ... -1.415 -1.027\n    away_goals  (chain, draw, match) float64 -0.4398 -1.288 ... -1.33 -1.442\nAttributes:\n    created_at:                 2023-12-05T15:21:51.114390\n    arviz_version:              0.16.1\n    inference_library:          pymc\n    inference_library_version:  5.9.2xarray.DatasetDimensions:chain: 2draw: 2000match: 240Coordinates: (3)chain(chain)int640 1array([0, 1])draw(draw)int640 1 2 3 4 ... 1996 1997 1998 1999array([   0,    1,    2, ..., 1997, 1998, 1999])match(match)int640 1 2 3 4 5 ... 235 236 237 238 239array([  0,   1,   2, ..., 237, 238, 239])Data variables: (2)home_goals(chain, draw, match)float64-1.513 -1.454 ... -1.415 -1.027array([[[-1.51301887, -1.45391602, -1.00192257, ..., -3.53446127,\n         -1.54375431, -1.09062576],\n        [-1.53975014, -1.30920493, -1.00001757, ..., -4.3415079 ,\n         -1.32192302, -1.26261905],\n        [-1.58497355, -1.31510717, -1.04177765, ..., -4.31051897,\n         -1.46734954, -0.94219829],\n        ...,\n        [-1.60702628, -1.3079598 , -1.08757278, ..., -3.43493378,\n         -1.42473448, -1.04504454],\n        [-1.5141781 , -1.42766747, -1.00355268, ..., -5.56638969,\n         -1.31309857, -1.03223391],\n        [-1.5097608 , -1.41308353, -1.00332537, ..., -4.34746403,\n         -1.3069125 , -1.18311605]],\n\n       [[-1.8837592 , -1.31826159, -1.07819372, ..., -3.51725245,\n         -1.30870169, -1.08669603],\n        [-1.61276405, -1.31516385, -1.15162254, ..., -3.44239536,\n         -1.31403931, -0.9847142 ],\n        [-1.85494912, -1.30857477, -1.09214197, ..., -4.435115  ,\n         -1.32868747, -0.80558568],\n        ...,\n        [-1.63889147, -1.61793317, -1.05133429, ..., -4.35221639,\n         -1.43261165, -0.92874084],\n        [-1.49605048, -1.31153759, -1.02848857, ..., -4.06916355,\n         -1.31070933, -0.95414301],\n        [-1.56747805, -1.34406024, -1.00414062, ..., -4.45335602,\n         -1.41482098, -1.02669625]]])away_goals(chain, draw, match)float64-0.4398 -1.288 ... -1.33 -1.442array([[[-0.43980549, -1.2877724 , -1.65556283, ..., -0.98300394,\n         -1.44823195, -1.50398708],\n        [-0.71026983, -1.03203738, -1.39832798, ..., -1.07893968,\n         -1.32565411, -1.418872  ],\n        [-0.58184338, -1.28876991, -1.68674708, ..., -0.95571126,\n         -1.3538785 , -1.46782943],\n        ...,\n        [-0.81601015, -1.20651302, -1.42807865, ..., -1.08559276,\n         -1.36921266, -1.4576021 ],\n        [-0.73636394, -1.08321226, -1.51721751, ..., -0.97803613,\n         -1.35960094, -1.54186696],\n        [-0.77537624, -1.07631233, -1.31274781, ..., -1.17506719,\n         -1.307722  , -1.40394592]],\n\n       [[-0.63522061, -1.03043916, -1.36780395, ..., -0.8910887 ,\n         -1.35377586, -1.52245937],\n        [-0.78762927, -1.05586988, -1.40693466, ..., -0.90876106,\n         -1.38744035, -1.41057356],\n        [-0.70441923, -1.00070282, -1.62342688, ..., -0.80835679,\n         -1.47156361, -1.73414548],\n        ...,\n        [-0.66494849, -1.24981964, -1.91201198, ..., -0.7596563 ,\n         -1.30825711, -1.50145771],\n        [-0.74378806, -1.00009503, -1.37739991, ..., -1.05539507,\n         -1.67902003, -1.57254038],\n        [-0.52780607, -1.22582768, -1.78592183, ..., -0.984984  ,\n         -1.32995653, -1.44232701]]])Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999],\n      dtype='int64', name='draw', length=2000))matchPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       230, 231, 232, 233, 234, 235, 236, 237, 238, 239],\n      dtype='int64', name='match', length=240))Attributes: (4)created_at :2023-12-05T15:21:51.114390arviz_version :0.16.1inference_library :pymcinference_library_version :5.9.2\n                      \n                  \n            \n            \n            \n                  \n                  sample_stats\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:                (chain: 2, draw: 2000)\nCoordinates:\n  * chain                  (chain) int64 0 1\n  * draw                   (draw) int64 0 1 2 3 4 5 ... 1995 1996 1997 1998 1999\nData variables: (12/17)\n    energy_error           (chain, draw) float64 -0.03167 -0.355 ... -0.5792\n    step_size_bar          (chain, draw) float64 0.4857 0.4857 ... 0.4794 0.4794\n    step_size              (chain, draw) float64 0.4789 0.4789 ... 0.3951 0.3951\n    diverging              (chain, draw) bool False False False ... False False\n    acceptance_rate        (chain, draw) float64 0.7816 0.7298 ... 1.0 0.9425\n    energy                 (chain, draw) float64 745.1 743.1 ... 746.3 745.0\n    ...                     ...\n    process_time_diff      (chain, draw) float64 0.001706 0.001624 ... 0.001897\n    reached_max_treedepth  (chain, draw) bool False False False ... False False\n    n_steps                (chain, draw) float64 7.0 7.0 7.0 7.0 ... 7.0 7.0 7.0\n    smallest_eigval        (chain, draw) float64 nan nan nan nan ... nan nan nan\n    largest_eigval         (chain, draw) float64 nan nan nan nan ... nan nan nan\n    perf_counter_start     (chain, draw) float64 235.0 235.0 ... 239.0 239.0\nAttributes:\n    created_at:                 2023-12-05T15:21:50.633886\n    arviz_version:              0.16.1\n    inference_library:          pymc\n    inference_library_version:  5.9.2\n    sampling_time:              6.882747173309326\n    tuning_steps:               1000xarray.DatasetDimensions:chain: 2draw: 2000Coordinates: (2)chain(chain)int640 1array([0, 1])draw(draw)int640 1 2 3 4 ... 1996 1997 1998 1999array([   0,    1,    2, ..., 1997, 1998, 1999])Data variables: (17)energy_error(chain, draw)float64-0.03167 -0.355 ... -0.584 -0.5792array([[-0.03167221, -0.35499451,  0.57824485, ...,  0.15860556,\n        -0.57958419, -0.21441968],\n       [-0.1100725 ,  0.22453875,  0.57398894, ...,  1.64718629,\n        -0.58397327, -0.57921795]])step_size_bar(chain, draw)float640.4857 0.4857 ... 0.4794 0.4794array([[0.48572714, 0.48572714, 0.48572714, ..., 0.48572714, 0.48572714,\n        0.48572714],\n       [0.47941111, 0.47941111, 0.47941111, ..., 0.47941111, 0.47941111,\n        0.47941111]])step_size(chain, draw)float640.4789 0.4789 ... 0.3951 0.3951array([[0.47885447, 0.47885447, 0.47885447, ..., 0.47885447, 0.47885447,\n        0.47885447],\n       [0.39505258, 0.39505258, 0.39505258, ..., 0.39505258, 0.39505258,\n        0.39505258]])diverging(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])acceptance_rate(chain, draw)float640.7816 0.7298 0.6407 ... 1.0 0.9425array([[0.78158284, 0.72977166, 0.64073725, ..., 0.96351589, 0.9736453 ,\n        0.64367688],\n       [0.55780742, 0.94017065, 0.8589024 , ..., 0.47092736, 1.        ,\n        0.94253476]])energy(chain, draw)float64745.1 743.1 739.2 ... 746.3 745.0array([[745.13313681, 743.09156763, 739.17708998, ..., 742.486203  ,\n        741.23486064, 749.40072259],\n       [751.46427813, 742.83965227, 749.39194631, ..., 748.5941118 ,\n        746.3312308 , 745.04235135]])max_energy_error(chain, draw)float640.5963 1.149 ... -1.443 -0.5792array([[ 0.59631534,  1.14890455,  0.80427583, ..., -0.64822214,\n        -0.57958419,  1.18818561],\n       [ 1.60856828,  0.22453875,  0.57398894, ...,  1.79048346,\n        -1.44294171, -0.57921795]])index_in_trajectory(chain, draw)int642 4 -6 -3 -4 -7 ... 3 -6 4 7 -4 -6array([[ 2,  4, -6, ...,  6,  4,  1],\n       [ 1,  2, -2, ...,  7, -4, -6]])perf_counter_diff(chain, draw)float640.001706 0.001623 ... 0.001897array([[0.00170575, 0.00162336, 0.0016622 , ..., 0.0016064 , 0.00163405,\n        0.00161251],\n       [0.00193325, 0.00174363, 0.00187132, ..., 0.0018967 , 0.00186535,\n        0.001897  ]])lp(chain, draw)float64-725.8 -719.8 ... -728.7 -721.7array([[-725.75800322, -719.77683363, -724.00298022, ..., -730.31455878,\n        -722.67614208, -722.08371526],\n       [-725.63253351, -727.72154654, -733.92357173, ..., -735.79584136,\n        -728.72621127, -721.67202016]])tree_depth(chain, draw)int643 3 3 3 3 3 3 3 ... 3 3 3 3 3 3 3 3array([[3, 3, 3, ..., 3, 3, 3],\n       [3, 3, 3, ..., 3, 3, 3]])process_time_diff(chain, draw)float640.001706 0.001624 ... 0.001897array([[0.0017059, 0.0016236, 0.0016624, ..., 0.0016064, 0.0016343,\n        0.0016126],\n       [0.0019335, 0.0017437, 0.0018714, ..., 0.0018964, 0.0018655,\n        0.0018971]])reached_max_treedepth(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])n_steps(chain, draw)float647.0 7.0 7.0 7.0 ... 7.0 7.0 7.0 7.0array([[7., 7., 7., ..., 7., 7., 7.],\n       [7., 7., 7., ..., 7., 7., 7.]])smallest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nanarray([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])largest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nanarray([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])perf_counter_start(chain, draw)float64235.0 235.0 235.0 ... 239.0 239.0array([[234.96797334, 234.96985235, 234.97163688, ..., 239.11695442,\n        239.11871773, 239.1205102 ],\n       [234.9463309 , 234.9484582 , 234.9503762 , ..., 238.99898658,\n        239.00105456, 239.00309166]])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999],\n      dtype='int64', name='draw', length=2000))Attributes: (6)created_at :2023-12-05T15:21:50.633886arviz_version :0.16.1inference_library :pymcinference_library_version :5.9.2sampling_time :6.882747173309326tuning_steps :1000\n                      \n                  \n            \n            \n            \n                  \n                  observed_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:     (match: 240)\nCoordinates:\n  * match       (match) int64 0 1 2 3 4 5 6 7 ... 233 234 235 236 237 238 239\nData variables:\n    home_goals  (match) int64 3 2 1 0 3 2 1 2 0 1 0 2 ... 0 3 0 0 1 1 1 4 6 2 0\n    away_goals  (match) int64 0 1 2 1 1 0 3 0 2 1 0 2 ... 2 0 0 0 1 4 1 0 0 2 2\nAttributes:\n    created_at:                 2023-12-05T15:21:50.639903\n    arviz_version:              0.16.1\n    inference_library:          pymc\n    inference_library_version:  5.9.2xarray.DatasetDimensions:match: 240Coordinates: (1)match(match)int640 1 2 3 4 5 ... 235 236 237 238 239array([  0,   1,   2, ..., 237, 238, 239])Data variables: (2)home_goals(match)int643 2 1 0 3 2 1 2 ... 0 1 1 1 4 6 2 0array([3, 2, 1, 0, 3, 2, 1, 2, 0, 1, 0, 2, 1, 0, 0, 2, 2, 0, 1, 1, 0, 1,\n       0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 2, 0, 1,\n       0, 1, 3, 0, 3, 2, 4, 2, 1, 2, 1, 2, 0, 0, 1, 3, 2, 6, 3, 2, 1, 3,\n       4, 1, 4, 2, 3, 1, 2, 0, 2, 2, 2, 3, 1, 1, 3, 0, 0, 1, 0, 1, 1, 0,\n       1, 1, 2, 2, 0, 2, 0, 0, 2, 2, 0, 0, 0, 1, 1, 1, 1, 4, 6, 2, 1, 2,\n       1, 4, 4, 1, 1, 3, 3, 3, 0, 2, 3, 6, 7, 1, 3, 2, 0, 2, 2, 0, 3, 3,\n       3, 0, 5, 0, 4, 6, 1, 1, 1, 7, 1, 1, 1, 1, 3, 5, 2, 0, 1, 3, 9, 1,\n       2, 2, 1, 3, 0, 4, 3, 2, 2, 0, 1, 2, 3, 3, 1, 2, 1, 2, 1, 1, 0, 0,\n       1, 2, 1, 1, 2, 5, 4, 1, 0, 2, 1, 2, 0, 1, 2, 2, 0, 0, 0, 6, 3, 6,\n       2, 1, 2, 1, 1, 0, 3, 0, 1, 2, 2, 4, 4, 3, 2, 2, 3, 3, 1, 2, 3, 1,\n       1, 1, 0, 2, 6, 2, 3, 3, 1, 0, 3, 0, 0, 1, 1, 1, 4, 6, 2, 0])away_goals(match)int640 1 2 1 1 0 3 0 ... 0 1 4 1 0 0 2 2array([0, 1, 2, 1, 1, 0, 3, 0, 2, 1, 0, 2, 2, 0, 0, 4, 1, 1, 4, 2, 4, 3,\n       3, 4, 3, 2, 3, 3, 3, 4, 1, 2, 3, 6, 0, 1, 4, 4, 4, 4, 3, 5, 2, 3,\n       0, 2, 1, 0, 2, 1, 0, 0, 1, 1, 5, 0, 0, 1, 0, 0, 0, 1, 1, 4, 0, 1,\n       1, 1, 2, 0, 0, 1, 4, 0, 2, 0, 0, 0, 1, 3, 2, 1, 1, 3, 2, 2, 1, 2,\n       1, 2, 0, 0, 0, 1, 1, 6, 4, 3, 1, 1, 0, 2, 0, 1, 3, 3, 0, 0, 0, 2,\n       0, 0, 0, 1, 1, 2, 0, 4, 2, 1, 0, 1, 0, 0, 1, 3, 1, 1, 1, 1, 1, 0,\n       0, 2, 3, 0, 1, 0, 1, 1, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n       1, 3, 0, 0, 4, 2, 1, 0, 0, 1, 2, 2, 1, 0, 0, 3, 2, 1, 0, 3, 3, 0,\n       0, 3, 0, 4, 1, 0, 1, 1, 1, 3, 1, 2, 2, 3, 1, 1, 2, 0, 4, 0, 0, 2,\n       0, 2, 1, 0, 2, 1, 1, 1, 1, 0, 1, 1, 0, 1, 3, 1, 2, 1, 1, 0, 1, 1,\n       0, 0, 1, 1, 0, 2, 0, 0, 1, 2, 0, 0, 0, 1, 4, 1, 0, 0, 2, 2])Indexes: (1)matchPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       230, 231, 232, 233, 234, 235, 236, 237, 238, 239],\n      dtype='int64', name='match', length=240))Attributes: (4)created_at :2023-12-05T15:21:50.639903arviz_version :0.16.1inference_library :pymcinference_library_version :5.9.2\n                      \n                  \n            \n            \n            \n                  \n                  constant_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:    (match: 240)\nCoordinates:\n  * match      (match) int64 0 1 2 3 4 5 6 7 ... 232 233 234 235 236 237 238 239\nData variables:\n    home_team  (match) int32 1 2 9 3 4 5 6 7 8 10 ... 4 5 6 7 8 10 11 12 13 14\n    away_team  (match) int32 0 0 0 0 0 0 0 0 0 0 ... 15 15 15 15 15 15 15 15 15\nAttributes:\n    created_at:                 2023-12-05T15:21:50.641451\n    arviz_version:              0.16.1\n    inference_library:          pymc\n    inference_library_version:  5.9.2xarray.DatasetDimensions:match: 240Coordinates: (1)match(match)int640 1 2 3 4 5 ... 235 236 237 238 239array([  0,   1,   2, ..., 237, 238, 239])Data variables: (2)home_team(match)int321 2 9 3 4 5 6 ... 8 10 11 12 13 14array([ 1,  2,  9,  3,  4,  5,  6,  7,  8, 10, 11, 12, 13, 14, 15,  0,  2,\n        9,  3,  4,  5,  6,  7,  8, 10, 11, 12, 13, 14, 15,  0,  1,  9,  3,\n        4,  5,  6,  7,  8, 10, 11, 12, 13, 14, 15,  0,  1,  2,  3,  4,  5,\n        6,  7,  8, 10, 11, 12, 13, 14, 15,  0,  1,  2,  9,  4,  5,  6,  7,\n        8, 10, 11, 12, 13, 14, 15,  0,  1,  2,  9,  3,  5,  6,  7,  8, 10,\n       11, 12, 13, 14, 15,  0,  1,  2,  9,  3,  4,  6,  7,  8, 10, 11, 12,\n       13, 14, 15,  0,  1,  2,  9,  3,  4,  5,  7,  8, 10, 11, 12, 13, 14,\n       15,  0,  1,  2,  9,  3,  4,  5,  6,  8, 10, 11, 12, 13, 14, 15,  0,\n        1,  2,  9,  3,  4,  5,  6,  7, 10, 11, 12, 13, 14, 15,  0,  1,  2,\n        9,  3,  4,  5,  6,  7,  8, 11, 12, 13, 14, 15,  0,  1,  2,  9,  3,\n        4,  5,  6,  7,  8, 10, 12, 13, 14, 15,  0,  1,  2,  9,  3,  4,  5,\n        6,  7,  8, 10, 11, 13, 14, 15,  0,  1,  2,  9,  3,  4,  5,  6,  7,\n        8, 10, 11, 12, 14, 15,  0,  1,  2,  9,  3,  4,  5,  6,  7,  8, 10,\n       11, 12, 13, 15,  0,  1,  2,  9,  3,  4,  5,  6,  7,  8, 10, 11, 12,\n       13, 14], dtype=int32)away_team(match)int320 0 0 0 0 0 0 ... 15 15 15 15 15 15array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,  2,  2,  2,\n        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  9,  9,  9,  9,  9,  9,\n        9,  9,  9,  9,  9,  9,  9,  9,  9,  3,  3,  3,  3,  3,  3,  3,  3,\n        3,  3,  3,  3,  3,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n        4,  4,  4,  4,  4,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n        5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n        6,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  8,\n        8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8, 10, 10, 10,\n       10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n       11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n       12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n       13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n       14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n       15, 15], dtype=int32)Indexes: (1)matchPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       230, 231, 232, 233, 234, 235, 236, 237, 238, 239],\n      dtype='int64', name='match', length=240))Attributes: (4)created_at :2023-12-05T15:21:50.641451arviz_version :0.16.1inference_library :pymcinference_library_version :5.9.2\n                      \n                  \n            \n            \n              \n            \n            \n\n\n\n\n7.3.4 Information criterion calculation\nDue to the presence of the two likelihoods in our model, we cannot call az.loo or az.waic straight away because the predictive task to evaluate is ambiguous. The calculation of information criteria requires pointwise likelihood values, \\(p(y_i|\\theta)\\) with \\(y_i\\) indicating observation \\(i\\)-th and \\(\\theta\\) representing all the parameters in the model. We need to define \\(y_i\\), what does one observation represent in our model.\nAs we were introducing above, this model alone can tackle several predictive tasks. These predictive tasks can be identified by the definition of one observation which at the same time defines how are pointwise likelihood values to be calculated. Here are some examples:\n\nWe could be a group of students supporting different teams with budget to travel only to one away match of our respective teams. We may want to travel to the match where our team will score the most goals (while being the away team and also independently of the winner of the match). We will therefore assess the predictive accuracy of our model using only \\(\\mathbf{y}_a\\).\nWe could also be football fans without any clear allegiance who love an intense match between two teams of similar strength. Based on previous experience, we may consider matches that end up 3-3 or 4-4 the ones that better fit our football taste. Now we need to assess the predictive accuracy using the result of the whole match.\nEven another alternative would be wanting to be present at the match where a single team scores the most goals. In this situation, we would have to put both home and away goals in the same bag and assess the predictive accuracy on the ability to predict values from this bag, we may call the observations in this hypothetical bag “number of goals scored per match and per team”.\n\nThere are even more examples of predictive tasks where this particular model can be of use. However, it is important to keep in mind that this model predicts the number of goals scored. Its results can be used to estimate probabilities of victory and other derived quantities, but calculating the likelihood of these derived quantities may not be straighforward. And as you can see above, there isn’t one unique predictive task: it all depends on the specific question you’re interested in. As often in statistics, the answer to these questions lies outside the model, you must tell the model what to do, not the other way around.\nEven though we know that the predictive task is ambiguous, we will start trying to calculate az.loo with idata_base and then work on the examples above and a couple more to show how would this kind of tasks be performed with ArviZ. But before that, let’s see what ArviZ says when you naively ask it for the LOO of a multi-likelihood model:\n\n# This will raise an error\n#az.loo(idata)\n\nAs expected, ArviZ has no way of knowing what predictive task we have in mind so it raises an error.\n\n7.3.4.1 Predicting the goals scored by the away team\nIn this particular case, we are interested in predicting the goals scored by the away team. We will still use the goals scored by the home team, but won’t take them into account when assessing the predictive accuracy. Below there is an illustration of how would cross validation be performed to assess the predictive accuracy in this particular case:\n This can also be seen from a mathematical point of view. We can write the pointwise log likelihood in the following way so it defines the predictive task at hand:\n\\[ p(y_i|\\theta) = p(y_{i,h}|\\theta_{i,h}) = \\text{Poiss}(y_{i,h}; \\theta_{i,h}) \\]\nwith \\(i\\) being the match indicator (\\(g\\)) in this case. These are precisely the values stored in the home_goals of the log_likelihood group of idata_base.\nWe can tell ArviZ to use these values using the argument var_name.\n\naz.loo(idata, var_name=\"home_goals\")\n\nComputed from 4000 posterior samples and 240 observations log-likelihood matrix.\n\n         Estimate       SE\nelpd_loo  -372.35    11.54\np_loo       14.98        -\n------\n\nPareto k diagnostic values:\n                         Count   Pct.\n(-Inf, 0.5]   (good)      240  100.0%\n (0.5, 0.7]   (ok)          0    0.0%\n   (0.7, 1]   (bad)         0    0.0%\n   (1, Inf)   (very bad)    0    0.0%\n\n\n\n\n7.3.4.2 Predicting the outcome of a match\nAnother option is being interested in the outcome of the matches. In our current model, the outcome of a match is not who wins or the aggregate of scored goals by both teams, the outcome is the goals scored by the home team and by the away team, both quantities at the same time. Below there is an illustration on how would cross validation be used to assess the predictive accuracy in this situation:\n\nThe one observation in this situation is therefore a vector with two components: \\(y_i = (y_{i,h}, y_{i,a})\\). Like above, we also have \\(n\\) observations. The pointwise likelihood is therefore a product:\n\\[\np(y_i|\\theta) = p(y_{i,h}|\\theta_{i,h})p(y_{i,a}|\\theta_{i,a}) =\n\\text{Poiss}(y_{i,h}; \\theta_{i,h})\\text{Poiss}(y_{i,a}; \\theta_{i,a})\n\\]\nwith \\(i\\) still being equal to the match indicator \\(g\\). Therefore, we have \\(n\\) observations like in the previous example, but each observation has two components.\nWe can calculate the product as a sum of logarithms and store the result in a new variable inside the log_likelihood group.\n\nlog_lik[\"matches\"] = log_lik.home_goals + log_lik.away_goals\naz.loo(idata, var_name=\"matches\")\n\nComputed from 4000 posterior samples and 240 observations log-likelihood matrix.\n\n         Estimate       SE\nelpd_loo  -716.68    15.84\np_loo       27.65        -\n------\n\nPareto k diagnostic values:\n                         Count   Pct.\n(-Inf, 0.5]   (good)      240  100.0%\n (0.5, 0.7]   (ok)          0    0.0%\n   (0.7, 1]   (bad)         0    0.0%\n   (1, Inf)   (very bad)    0    0.0%\n\n\n\n\n7.3.4.3 Predicting the goals scored per match and per team\nAnother example described above is being interested in the scored goals per match and per team. In this situation, our observations are a scalar once again.\n\nThe expression of the likelihood is basically the same as the one in the first example (both cases are scalars), but the difference is in the index, but that does not make it less significant:\n\\[\np(y_i|\\theta) = p(y_{i}|\\theta_{i}) =\n\\text{Poiss}(y_{i}; \\theta_{i})\n\\]\nwith \\(i\\) not being equal to the match indicator \\(g\\) anymore. Now, we will consider \\(i\\) as an index iterating over the values in\n\\[\\big\\{(1,h), (2,h), \\dots, (n-1,h), (n,h), (1,a), (2,a) \\dots (n-1,a), (n,a)\\big\\}\\]\nTherefore, unlike in previous cases, we have \\(2n\\) observations.\nWe can obtain the pointwise log likelihood corresponding to this case by concatenating the pointwise log likelihoods of home_goals and away_goals. Then, like in the previous case, store the result in a new variable inside the log_likelihood group.\n\nlog_lik[\"goals\"] = xr.concat((log_lik.home_goals, log_lik.away_goals), \"match\").rename({\"match\": \"goal\"})\naz.loo(idata, var_name=\"goals\")\n\nComputed from 4000 posterior samples and 480 observations log-likelihood matrix.\n\n         Estimate       SE\nelpd_loo  -716.71    17.42\np_loo       27.69        -\n------\n\nPareto k diagnostic values:\n                         Count   Pct.\n(-Inf, 0.5]   (good)      480  100.0%\n (0.5, 0.7]   (ok)          0    0.0%\n   (0.7, 1]   (bad)         0    0.0%\n   (1, Inf)   (very bad)    0    0.0%\n\n\n\n\n7.3.4.4 Predicting team level performance\nThe last example covered here is estimating the predictive accuracy at group level. This can be useful to assess the accuracy of predicting the whole season of a new team. In addition, this can also be used to evaluate the hierarchical part of the model.\nAlthough theoretically possible, importance sampling tends to fail at the group level due to all the observations being too informative. See this post for more details.\nIn this situation, we could describe the cross validation as excluding a team. When we exclude a team, we will exclude all the matches played by the team, not only the goals scored by the team but the whole match. Here is the illustration:\n\nIn the first column, we are excluding “Levante U.D.” which in the rows shown only appears once. In the second one, we are excluding “Athletic Club” which appears two times. This goes on following the order of appearance in the away team column.\n\ngroupby_sum_home = log_lik.groupby(const.home_team).sum().rename({\"home_team\": \"team\"})\ngroupby_sum_away = log_lik.groupby(const.away_team).sum().rename({\"away_team\": \"team\"})\n\nlog_lik[\"teams_match\"] = (\n    groupby_sum_home.home_goals + groupby_sum_home.away_goals + \n    groupby_sum_away.home_goals + groupby_sum_away.away_goals\n)\naz.loo(idata, var_name=\"teams_match\")\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/arviz/stats/stats.py:803: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.7 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  warnings.warn(\n\n\nComputed from 4000 posterior samples and 16 observations log-likelihood matrix.\n\n         Estimate       SE\nelpd_loo -1436.99    18.10\np_loo       51.34        -\n\nThere has been a warning during the calculation. Please check the results.\n------\n\nPareto k diagnostic values:\n                         Count   Pct.\n(-Inf, 0.5]   (good)        0    0.0%\n (0.5, 0.7]   (ok)          0    0.0%\n   (0.7, 1]   (bad)        11   68.8%\n   (1, Inf)   (very bad)    5   31.2%\n\n\n\n# this does something different, not sure this approach would make any sense though\nhome_goals_team = log_lik.home_goals.groupby(const.home_team).sum().rename({\"home_team\": \"team\"})\naway_goals_team = log_lik.away_goals.groupby(const.away_team).sum().rename({\"away_team\": \"team\"})\nlog_lik[\"teams\"] = home_goals_team + away_goals_team\naz.loo(idata, var_name=\"teams\")\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/arviz/stats/stats.py:803: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.7 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  warnings.warn(\n\n\nComputed from 4000 posterior samples and 16 observations log-likelihood matrix.\n\n         Estimate       SE\nelpd_loo  -719.09    28.10\np_loo       26.37        -\n\nThere has been a warning during the calculation. Please check the results.\n------\n\nPareto k diagnostic values:\n                         Count   Pct.\n(-Inf, 0.5]   (good)        0    0.0%\n (0.5, 0.7]   (ok)          3   18.8%\n   (0.7, 1]   (bad)        10   62.5%\n   (1, Inf)   (very bad)    3   18.8%"
  },
  {
    "objectID": "Presenting_results.html",
    "href": "Presenting_results.html",
    "title": "8  Presentation of results",
    "section": "",
    "text": "Summarizing results\nMonte Carlo Standard error and accuracy"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bessiere, Pierre, Emmanuel Mazer, Juan Manuel Ahuactzin, and Kamel\nMekhnacha. 2013. Bayesian Programming. 1 edition.\nBoca Raton: Chapman; Hall/CRC. https://www.crcpress.com/Bayesian-Programming/Bessiere-Mazer-Ahuactzin-Mekhnacha/p/book/9781439880326.\n\n\nCleveland, William S., and Robert McGill. 1984. “Graphical\nPerception: Theory, Experimentation, and Application to the Development\nof Graphical Methods.” Journal of the American Statistical\nAssociation 79 (387): 531–54. https://doi.org/10.1080/01621459.1984.10478080.\n\n\nDaniel Roy. 2015. Probabilistic Programming. http://probabilistic-programming.org.\n\n\nDiaconis, Persi. 2011. “Theories of Data\nAnalysis: From Magical\nThinking Through Classical\nStatistics.” In Exploring Data\nTables, Trends, and Shapes,\n1–36. John Wiley & Sons, Ltd. https://doi.org/10.1002/9781118150702.ch1.\n\n\nGhahramani, Zoubin. 2015. “Probabilistic Machine\nLearning and Artificial\nIntelligence.” Nature 521 (7553): 452–59.\nhttps://doi.org/10.1038/nature14541.\n\n\nHeer, Jeffrey, and Michael Bostock. 2010. “Crowdsourcing Graphical\nPerception: Using Mechanical Turk to Assess Visualization\nDesign.” In Proceedings of the SIGCHI\nConference on Human Factors in\nComputing Systems, 203–12.\nCHI ’10. New York, NY, USA: Association for Computing\nMachinery. https://doi.org/10.1145/1753326.1753357."
  }
]