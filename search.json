[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploratory Analysis of Bayesian Models",
    "section": "",
    "text": "‎\nWhile conceptually simple, Bayesian methods can be mathematically and numerically challenging. Probabilistic programming languages (PPLs) implement functions to easily build Bayesian models together with efficient automatic inference methods. This helps separate the model building from the inference, allowing practitioners to focus on their specific problems and leaving the PPLs to handle the computational details for them (Bessiere et al. 2013; Daniel Roy 2015; Ghahramani 2015). The inference process generates a posterior distribution - which has a central role in Bayesian statistics - together with other distributions like the posterior predictive distribution and the prior predictive distribution. The correct visualization, analysis, and interpretation of these distributions is key to properly answer the questions that motivated the inference process.\nWhen working with Bayesian models there are a series of related tasks that need to be addressed besides inference itself:\n\nDiagnoses of the quality of the inference (as this is generally done using numerical approximation methods)\nModel criticism, including evaluations of both model assumptions and model predictions\nComparison of models, including model selection or model averaging\nPreparation of the results for a particular audience\n\nWe collectivelly call all these tasks Exploratory analysis of Bayesian model, as we take many ideas from Exploratory data analysis and apply them to analyze Bayesian modesl.\nIn the words of Persi Diaconis (Diaconis 2011):\n\n“Exploratory data analysis seeks to reveal structure, or simple descriptions in data. We look at numbers or graphs and try to find patterns. We pursue leads suggested by background information, imagination, patterns perceived, and experience with other data analyses”.\n\nIn this book we will discuss how to use both numerical and visual summaries to help successfully performing such tasks that are central to the iterative and interactive modeling process. To do so, we first discuss some general principles of data visualization and uncertainty representation that are not exclusibe of Bayesian statistics.\n\n\n\n\nBessiere, Pierre, Emmanuel Mazer, Juan Manuel Ahuactzin, and Kamel Mekhnacha. 2013. Bayesian Programming. 1 edition. Boca Raton: Chapman; Hall/CRC. https://www.crcpress.com/Bayesian-Programming/Bessiere-Mazer-Ahuactzin-Mekhnacha/p/book/9781439880326.\n\n\nDaniel Roy. 2015. Probabilistic Programming. http://probabilistic-programming.org.\n\n\nDiaconis, Persi. 2011. “Theories of Data Analysis: From Magical Thinking Through Classical Statistics.” In Exploring Data Tables, Trends, and Shapes, 1–36. John Wiley & Sons, Ltd. https://doi.org/10.1002/9781118150702.ch1.\n\n\nGhahramani, Zoubin. 2015. “Probabilistic Machine Learning and Artificial Intelligence.” Nature 521 (7553): 452–59. https://doi.org/10.1038/nature14541."
  },
  {
    "objectID": "elements_of_visualization.html#coordinate-systems-and-axes",
    "href": "elements_of_visualization.html#coordinate-systems-and-axes",
    "title": "1  Elements of visualization",
    "section": "1.1 Coordinate systems and axes",
    "text": "1.1 Coordinate systems and axes\nData visualization requires defining position scales to determine where different data values are located in a graphic. In 2D visualizations, two numbers are required to uniquely specify a point. Thus, we need two position scales. The arrangement of these scales is known as a coordinate system. The most common coordinate system is the 2D Cartesian system, using x and y values with orthogonal axes. Conventionally with the x-axis running horizontally and the y-axis vertically. Figure 1.1 shows a Cartesian coordinate system.\n\n\n\nFigure 1.1: Cartesian coordinate system\n\n\nIn practice, we typically shift the axes so that they do not necessarily pass through the origin (0,0), and instead their location is determined by the data. We do this because it is usually more convenient and easier to read to have the axes to the left and bottom of the figure than in the middle. For instance Figure 1.2 plots the exact same points shown in Figure 1.1 but with the axes placed automatically by matplotlib.\n\n\n\nFigure 1.2: Cartesian coordinate system with axes automatically placed by matplotlib based on the data\n\n\nUsually, data has units, such as degrees Celsius for temperature, centimeters for length, or kilograms for weight. In case we are plotting variables of different types (and hence different units) we can adjust the aspect ratio of the axes as we wish. We can make a figure short and wide if it fits better on a page or screen. But we can also change the aspect ratio to highlight important differences, for example, if we want to emphasize changes along the y-axis we can make the figure tall and narrow. When both the x and y axes use the same units, it’s important to maintain an equal ratio to ensure that the relationship between data points on the graph accurately reflects their quantitative values.\nAfter the cartesian coordinate system, the most common coordinate system is the polar coordinate system. In this system, the position of a point is determined by the distance from the origin and the angle with respect to a reference axis. Polar coordinates are useful for representing periodic data, such as days of the week, or data that is naturally represented in a circular shape, such as wind direction. Figure Figure 1.3 shows a polar coordinate system.\n\n\n\nFigure 1.3: Polar coordinate system"
  },
  {
    "objectID": "elements_of_visualization.html#plot-elements",
    "href": "elements_of_visualization.html#plot-elements",
    "title": "1  Elements of visualization",
    "section": "1.2 Plot elements",
    "text": "1.2 Plot elements\nTo convey visual information we generally use shapes, including lines, circles, squares, etc. These elements have properties associated with them like, position, shape, and color. In addition, we can add text to the plot to provide additional information.\nArviZ uses both matplotlib and bokeh as plotting backends. While for basic use of ArviZ is not necessary to know about these libraries, being familiar with them is useful to better understand some of the arguments in ArviZ’s plots and/or to tweak the default plots generated with ArviZ. If you need to learn more about these libraries we recommend the official tutorials for matplotlib and bokeh."
  },
  {
    "objectID": "elements_of_visualization.html#good-practices-and-sources-of-error",
    "href": "elements_of_visualization.html#good-practices-and-sources-of-error",
    "title": "1  Elements of visualization",
    "section": "1.3 Good practices and sources of error",
    "text": "1.3 Good practices and sources of error\nUsing visualization to deceive third parties should not be the goal of an intellectually honest person, and you must also be careful not to deceive yourself. For example, it has been known for decades that a bar chart is more effective for comparing values than a pie chart. The reason is that our perceptual apparatus is quite good at evaluating lengths, but not very good at evaluating areas. Figure 1.4 shows different visual elements ordered according to the precision with which the human brain can detect differences and make comparisons between them (Cleveland and McGill 1984; Heer and Bostock 2010).\n\n\n\nFigure 1.4: Scale of elementary perceptual tasks, taken from The Truthful Art\n\n\n\n1.3.1 General principles for using colors\nHuman eyes work by essentially perceiving 3 wavelengths, this feature is used in technological devices such as screens to generate all colors from combinations of 3 components, Red, Green, and Blue. This is known as the RGB color model. But this is not the only possible system. A very common alternative is the CYMK color model, Cyan, Yellow, Magenta, and Black.\nTo analyze the perceptual attributes of color, it is better to think in terms of Hue, Saturation, and Lightness, HSL is an alternative representation of the RGB color model.\nThe hue is what we colloquially call “different colors”. Green, red, etc. Saturation is how colorful or washed out we perceive a given color. Two colors with different hues will look more different when they have more saturation. The lightness corresponds to the amount of light emitted (active screens) or reflected (impressions), ranging from black to white:\nVarying the tone is useful to easily distinguish categories as shown in Figure 1.5.\n\n\n\nFigure 1.5: Tone varitaions can be help to distinguish categories.\n\n\nIn principle, most humans are capable of distinguishing millions of tones, but if we want to associate categories with colors, the effectiveness of distinguishing them decreases drastically as the number of categories increases. This happens not only because the tones will be increasingly closer to each other, but also because we have a limited working memory. Associating a few colors (say 4) with categories (countries, temperature ranges, etc.) is usually easy. But unless there are pre-existing associations, remembering many categories becomes challenging and this exacerbates when colors are close to each other. This requires us to continually alternate between the graphic and the legend or text where the color-category association is indicated. Adding other elements besides color such as shapes can help, but in general, it will be more useful to try to keep the number of categories relatively low. In addition, it is important to take into account the presentation context, if we want to show a figure during a presentation where we only have a few seconds to dedicate to that figure, it is advisable to keep the figure as simple as possible. This may involve removing items and displaying only a subset of the data. If the figure is part of a text, where the reader will have the time to analyze for a longer period, perhaps the complexity can be somewhat greater.\nAlthough we mentioned before that human eyes are capable of distinguishing three main colors (red, green, and blue), the ability to distinguish these 3 colors varies between people, to the point that many individuals have difficulty distinguishing some colors. The most common case occurs with red and green. This is why it is important to avoid using those colors. An easy way to avoid this problem is to use colorblind-friendly palettes. We’ll see later that this is an easy thing to do when using ArviZ.\nVarying the lightness as in Figure 1.6 is useful when we want to represent a continuous scale. With the hue-based palette (left), it’s quite difficult to determine that our data shows two “spikes”, whereas this is easier to see with the lightness-modifying palette (right). Varying the lightness helps to see the structure of the data since changes in lightness are more intuitively processed as quantitative changes.\n\n\n\nFigure 1.6: Hue-based palette (left) vs lightness-modifying palette (right)\n\n\nOne detail that we should note is that the graph on the right of Figure 1.6 does not change only the lightness, it is not a map in gray or blue scales. That palette also changes the hue but in a very subtle way. This makes it aesthetically more pleasing and the subtle variation in hue contributes to increasing the perceptual distance between two values and therefore the ability to distinguish small differences.\nWhen using colors to represent numerical variables it is important to use uniformly perceptual maps like those offered by matplotlib or colorcet. These are maps where the colors vary in such a way that they adequately reflect changes in the data. Not all colormaps are perceptually uniform. Obtaining them is not trivial. Figure 1.7 shows the same image using different colormaps. We can see that widely used maps such as jet (also called rainbow) generate distortions in the image. In contrast viridis, a perceptually uniform color map does not generate such distortions.\n\n\n\nFigure 1.7: non-uniformly perceptual maps like jet can be very missleading\n\n\nA common criticism of perceptually smooth maps is that they appear more “flat” or “boring” at first glance. And instead maps like Jet, show greater contrast. But that is precisely one of the problems with maps like Jet, the magnitude of these contrasts does not correlate with changes in the data, so even extremes can occur, such as showing contrasts that are not there and hiding differences that are truly there."
  },
  {
    "objectID": "elements_of_visualization.html#style-sheets",
    "href": "elements_of_visualization.html#style-sheets",
    "title": "1  Elements of visualization",
    "section": "1.4 Style sheets",
    "text": "1.4 Style sheets\nMatplotlib allows users to easily switch between plotting styles by defining style sheets. ArviZ is delivered with a few additional styles that can be applied globally by writing az.style.use(name_of_style) or inside a with statement.\n\naz.style.use('arviz-white')\nx = np.linspace(0, 1, 100)\ndist = pz.Beta(2, 5).pdf(x)\n\nfig = plt.figure()\nfor i in range(10):\n    plt.plot(x, dist - i, f'C{i}', label=f'C{i}')\nplt.xlabel('x')\nplt.ylabel('f(x)', rotation=0, labelpad=15);\n\n\n\n\nFigure 1.8: arviz-white style use a color-blind friendly palette\n\n\n\n\naz.style is just an alias of matplotlib.pyplot.style, so everything you can do with one of them you can do with the other.\nThe styles arviz-colors, arviz-white, arviz-darkgrid, and arviz-whitegrid use the same color-blind friendly palette. This palette was designed using colorcyclepicker. arviz-doc, the style used in ArviZ documentation, uses another color-blind friendly and it looks gorgeous on-screen and in printed documents, although it can be tricky for projections if the room is not dark enough.\nIf you need to do plots in grey-scale we recommend restricting yourself to the first 3 colors of the ArviZ default palette (“C0”, “C1” and “C2”), otherwise, you may need to use different line styles or different markers. Another useful palette when the number of colors is restricted is the grayscale which assigns greys to the first four colors (“C0”, “C1”, “C2” and “C3”) and the ArviZ’s blue to “C5”. Check the style gallery to see all the available ArviZ’s styles.\n\n\n\n\nCleveland, William S., and Robert McGill. 1984. “Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods.” Journal of the American Statistical Association 79 (387): 531–54. https://doi.org/10.1080/01621459.1984.10478080.\n\n\nHeer, Jeffrey, and Michael Bostock. 2010. “Crowdsourcing Graphical Perception: Using Mechanical Turk to Assess Visualization Design.” In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 203–12. CHI ’10. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/1753326.1753357."
  },
  {
    "objectID": "distributions.html#random-variables",
    "href": "distributions.html#random-variables",
    "title": "2  Random variables, distributions, and uncertainty",
    "section": "2.1 Random variables",
    "text": "2.1 Random variables\nFrom a Bayesian perspective probabilities represent a degree of (un)certainty about the occurrence of an event. It is a measure of the likelihood that a particular hypothesis or event is true, given the available data and prior knowledge. We assign the value 0 to something impossible and 1 to something certain. When we are unsure we assign a value in between. For example, we could say that the probability of rain tomorrow is 0.32. This means that we are 32% certain that it will rain tomorrow.\nIn practice we usually do not care about individual probabilities, instead we work with probability distributions. A probability distribution describes the probabilities associated with each possible outcome of an experiment. In statistics, the term “experiment” is used in a very wide sense. It could mean a well-planned experiment in a laboratory, but it could also mean the result of a poll, the observation of the weather tomorrow, or the number of people that will visit a website next week.\nLet’s consider the experiment of observing the weather tomorrow. The possible outcomes of this experiment include the following outcomes:\n\nRainy\nSunny\nCloudy\n\nNotice that we are omitting the possibility of snow, or hail. In other words, we are assigning 0 probability to those outcomes. It is usually the case that we do not ponder all the possible outcomes of an experiment, either because we deliberately assume them to be irrelevant, because we don’t know about them, or because is too complex/expensive/time-consuming/etc to take them all into account.\nAnother important thing to notice, from this example, is that these outcomes are words (or strings if you want). To work with them we need to assign a number to each outcome. For example, we could assign the numbers 0 to Rainy, 1 to Sunny, and 2 to Cloudy. This mapping from the outcomes to the numbers is called a random variable. This is a funny and potentially misleading name as its mathematical definition is not random (the mapping is deterministic) nor a variable (it is a function). The mapping is arbitrary, -1 to Rainy, 0 to Sunny, and 4 to Cloudy is also valid. But once we pick one mapping, we keep using it for the rest of the experiment or analysis. One common source of confusion is understanding where the randomness comes from if the mapping is deterministic. The randomness comes from the uncertainty about the outcome of the experiment, i.e. the weather tomorrow. We are not sure if it will be rainy tomorrow until tomorrow comes.\nRandom variables can be classified into two main types: discrete and continuous.\n\nDiscrete Random Variables: They can take on a countable number of distinct values. We already saw an example of a discrete random variable, the weather tomorrow. It can take on three values: Rainy, Sunny, and Cloudy. No intermediate values are allowed in our experiment, even when it is true that it can be partially sunny and still rain. And it has to be at least partially cloudy to rain. But we are not considering those possibilities.\nContinuous Random Variables: They can take on any value within a certain range. For example, the temperature tomorrow is a continuous random variable. If we use a Celcius scale, then it can take on any value between -273.15 Celsius to \\(+ \\infty\\). Of course, in practice, the expected temperature is restricted to a much narrower range. The lowest recorded temperature on Earth is −89.2 °C and the highest is 56.7 °C, and that range will be even narrower if we consider a particular region of our planet."
  },
  {
    "objectID": "distributions.html#probability-mass-and-density-functions",
    "href": "distributions.html#probability-mass-and-density-functions",
    "title": "2  Random variables, distributions, and uncertainty",
    "section": "2.2 Probability mass and density functions",
    "text": "2.2 Probability mass and density functions\nThe probability distribution of a discrete random variable is often described using a probability mass function (PMF), which gives the probability of each possible outcome. For instance, the following plot shows the probability mass function of a categorical distribution with three possible outcomes, like Rainy, Sunny, and Cloudy.\n\npz.Categorical([0.15, 0.6, 0.25]).plot_pdf();\n\n\n\n\nFigure 2.1: PMF of a Categorical disribution\n\n\n\n\nUsually, there is more than one probability distribution that we can use to represent the same set of probabilities, for instance, we could use a binomial distribution.\n\npz.Binomial(2, 0.6).plot_pdf();\n\n\n\n\nFigure 2.2: PMF of a Binomial distribution\n\n\n\n\nThe probability distribution of a continuous random variable is described using a probability density function (PDF), which specifies the likelihood of the random variable falling within a particular interval. For instance, we could use a normal distribution to describe the temperature tomorrow.\n\npz.Normal(30, 4).plot_pdf();\n\n\n\n\nFigure 2.3: PDF of a normal distribution\n\n\n\n\nor maybe a skew normal like this if we expect higher temperatures like during summer.\n\npz.SkewNormal(38, 5, -2).plot_pdf();\n\n\n\n\nFigure 2.4: PDF of a skew-normal distribution\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that we use plot_pdf() instead of plot_pmf(), this is because PreliZ uses the same method to plot both discrete and continuous distributions. In the case of discrete distributions, it will plot the probability mass function (PMF), and in the case of continuous distributions, it will plot the probability density function (PDF).\n\n\nOne issue when interpreting a PDF is that the y-axis is a density, not a probability. To get probability from a PDF we need to integrate the density over a given interval. This is something straightforward to do with a computer. But not that easy to do “visually”, human eyes/brains are not very good at that task. One way to alleviate this issue is to accompany a PDF with a point interval, like in the following plot.\n\npz.SkewNormal(38, 5, -2).plot_pdf(pointinterval=True);\n\n\n\n\nFigure 2.5: PDF of a SkewNormal distribution with a pointinterval\n\n\n\n\nThe point interval shows the quantiles of the distribution. Quantiles divide a dataset into equal probability intervals. For example, deciles divide a dataset into 10 equal-probability intervals, and quartiles divide a dataset into 4 equal-probability intervals. The most common quantile is the median (or 50th percentile), which divides a dataset into two equal-probability intervals where half of the data falls below the median and half of the data falls above the median.\nThe point interval in Figure 2.5 shows the 5th, 25th, 50th, 75th, and 95th percentiles. The point is the median. The tick line is the interquartile range (the central 50% of the distribution) and the thin line is the central 90% of the distribution."
  },
  {
    "objectID": "distributions.html#cumulative-distribution-functions",
    "href": "distributions.html#cumulative-distribution-functions",
    "title": "2  Random variables, distributions, and uncertainty",
    "section": "2.3 Cumulative distribution functions",
    "text": "2.3 Cumulative distribution functions\nIn the previous section, we saw that we can use PMFs and PDFs to represent the probability distribution of a random variable. But there are other ways to represent a distribution. For example, we could use the cumulative distribution function (CDF).\nThe CDF is defined as the probability that the random variable takes a value less than or equal to \\(x\\). The CDF is defined for both discrete and continuous random variables. Figure 2.6 shows the CDF of a categorical distribution with three possible outcomes. Compare it with Figure 2.1\n\npz.Categorical([0.15, 0.6, 0.25]).plot_cdf();\n\n\n\n\nFigure 2.6: CDF of a Categorical distribution\n\n\n\n\nFigure 2.7 shows the CDF of a normal distribution (compare it with Figure 2.3).\n\npz.Normal(30, 4).plot_cdf();\n\n\n\n\nFigure 2.7: CDF of a normal distribution\n\n\n\n\nThe CDF is usually easier to read than the PDF, as we already saw y-axis for a PDF is a density that has no intrinsic meaning, and to get probability from a PDF we need to evaluate areas. Instead for a CDF the y-axis is a probability. For the PMF/PDF it is easier to get the mode (the highest value for the point/curve), and for the CDF it is easier to get the median (the value of \\(x\\) for which \\(y=0.5\\)), or other quantiles. From the CDF it is also easier to quickly get quantities like the probability of getting a temperature equal or lower than 35 degrees. It is the value of the CDF at 35. From Figure 2.7 we can see that it is roughly 0.9 or 90%, if you want more accuracy you could use a matplotlib/ArviZ style with a grid (like arviz-darkgrid) or use the cdf() function.\n\npz.Normal(30, 4).cdf(35)\n\n0.8943502263331446\n\n\nFrom the CDF we can also easily get the probability of a range of values. For example, the probability of the temperature being between 25 and 35 degrees is the difference between the CDF at 35 and the CDF at 25. From Figure 2.7 we can get that it is roughly 0.9 or 90%. Again even when you can get a good estimate just by looking at the graph you can use the cdf() function to get a more accurate estimate. But the fact that you can get a good estimate by looking at the graph is a good feature.\n\nnp.diff(pz.Normal(30, 4).cdf([25, 35]))\n\narray([0.78870045])"
  },
  {
    "objectID": "distributions.html#inverse-cumulative-distribution-functions",
    "href": "distributions.html#inverse-cumulative-distribution-functions",
    "title": "2  Random variables, distributions, and uncertainty",
    "section": "2.4 Inverse cumulative distribution functions",
    "text": "2.4 Inverse cumulative distribution functions\nSometimes we may want to use the inverse of the CDF. This is known as the quantile function or the percent point function (PPF). The PPF is also defined for both discrete and continuous random variables. For example, Figure 2.8 shows the PPF of a categorical distribution with three possible outcomes and Figure 2.9 shows the PPF of a normal distribution.\n\npz.Categorical([0.15, 0.6, 0.25]).plot_ppf();\n\n\n\n\nFigure 2.8: PPF of a Categorical distribution\n\n\n\n\n\npz.Normal(30, 4).plot_ppf();\n\n\n\n\nFigure 2.9: PPF of a normal distribution"
  },
  {
    "objectID": "distributions.html#distributions-in-arviz",
    "href": "distributions.html#distributions-in-arviz",
    "title": "2  Random variables, distributions, and uncertainty",
    "section": "2.5 Distributions in ArviZ",
    "text": "2.5 Distributions in ArviZ\nThe PMF/PDF, CDF, and PPF are convenient ways to represent distributions for which we know the analytical form. But in practice, we often work with distributions that we don’t know their analytical form. Instead, we have a set of samples from the distribution. A clear example is a posterior distribution, computed using an MCMC method. For those cases, we still want useful visualization that we can use for ourselves or to show others. Some common methods are:\n\nHistograms\nKernel density estimation (KDE)\nEmpirical cumulative distribution function (ECDF)\nQuantile dot plots\n\nWe will discuss these methods in the next subsections with special emphasis on how they are implemented in ArviZ.\n\n2.5.1 Histograms\nHistograms are a very simple and effective way to represent a distribution. The basic idea is to divide the range of the data into a set of bins and count how many data points fall into each bin. Then we use as many bars as bins, with the height of the bars being proportional to the counts. The following video shows a step-by-step animation of a histogram being built.\nVideo\nHistograms can be used to represent both discrete and continuous random variables. Discrete variables are usually represented using integers. When ArviZ is asked to plot integer data it will use histograms as the default method. Arguably the most important parameter of a histogram is the number of bins. Too few bins and we will miss details, too many and we will plot noise. You can pick the number of bins with a bit of trial and error, especially when you are sure of what you want to show. However, there are many methods to compute the number of bins automatically from the data, like the Freedman–Diaconis rule or the Sturges’ rule. By default, ArviZ computes the number of bins using both rules and then picks the one that gives the largest number of bins. This is the same approach used by np.histogram(., bins=\"auto) and plt.hist(., bins=\"auto). Additionally, when the data is of type integers, ArviZ will preserve that structure and will associate bins to integers, instead of floats. If the number of unique integers is relatively small then, it will associate one bin to each integer. For example, in the following plot each bar is associated with an integer in the interval [0, 9].\n\nd_values = pz.Poisson(3).rvs(500)\naz.plot_dist(d_values);\n\n\n\n\nFigure 2.10: Histogram from a sample of integers. Each bin corresponds to a single integer.\n\n\n\n\nWhen the discrete values take higher values, like in Figure 2.11, bins are still associated with integers but many integers are binned together.\n\nd_values = pz.Poisson(100).rvs(500)\naz.plot_dist(d_values);\n\n\n\n\nFigure 2.11: Histogram from a sample of integers. Bins group together many integers.\n\n\n\n\nIf you don’t like the default binning criteria of ArviZ, you can change it by passing the bins argument using the hist_kwargs.\n\nd_values = pz.Poisson(100).rvs(500)\nax = az.plot_dist(d_values, hist_kwargs={\"bins\":\"auto\"})\nplt.setp(ax.get_xticklabels(), rotation=45);\n\n\n\n\nFigure 2.12: Histogram from a sample of integers, with bins automatically computed by Matplotlib, not ArviZ.\n\n\n\n\n\n\n2.5.2 KDE\nKernel density estimation (KDE) is a non-parametric way to estimate the probability density function from a sample. Intuitivelly you can think of it as the smooth version of a histogram. Conceptually you place a kernel function like a Gaussian on top of a data point, then you sum all the Gaussians, generally evaluated over a grid and not over the data points. Results are normalized so the total area under the curve is one. The following video shows a step-by-step animation of a KDE being built. You can see a version with border corrections and without them. Border corrections avoid adding a positive density outside the range of the data.\nVideo\nThe following block of code shows a very simple example of a KDE.\n\n_, ax = plt.subplots(figsize=(12, 4))\nbandwidth = 0.4\nnp.random.seed(19)\ndatapoints = 7\ny = np.random.normal(7, size=datapoints)\nx = np.linspace(y.min() - bandwidth * 3, y.max() + bandwidth * 3, 100)\nkernels = np.transpose([pz.Normal(i, bandwidth).pdf(x) for i in y])\nkernels *= 1/datapoints  # normalize the results\nax.plot(x, kernels, 'k--', alpha=0.5)\nax.plot(y, np.zeros(len(y)), 'C1o')\nax.plot(x, kernels.sum(1))\nax.set_xticks([])\nax.set_yticks([]);\n\n\n\n\nThe most important parameter of a KDE is the bandwidth which controls the degree of smoothness of the resulting curve. It is analogous to the number of bins for the histograms. ArviZ’s default method to compute the bandwidth works well for a wide range of distributions including multimodal ones. Compared to other KDEs in the Python ecosystem, the KDE implemented in ArviZ automatically handles the boundaries of a distribution. ArviZ will assign a density of zero to any point outside the range of the data.\nThe following example shows a KDE computed from a sample from a Gamma distribution. Notice that ArviZ computes a KDE instead of a histogram, and notice that there is no density for negative values.\n\nc_values = pz.Gamma(2, 3).rvs(1000)\naz.plot_dist(c_values);\n\n\n\n\nFigure 2.13: KDE from a sample of floats. By default, ArviZ computes a KDE instead of a histogram.\n\n\n\n\n\n\n2.5.3 ECDF\nBoth histograms and KDEs are ways to approximate the PMF/PDF of a distribution from a sample. But sometimes we may want to approximate the CDF instead. The empirical cumulative distribution function (ECDF) is a non-parametric way to estimate the CDF. It is a step function that jumps up by 1/N at each observed data point, where N is the total number of data points. The following video shows a step-by-step animation of an ECDF being built.\nVideo\nThe following block of code shows a very simple example of an ECDF.\n\nc_values = pz.Poisson(3).rvs(500)\naz.plot_ecdf(c_values);\n\n\n\n\nFigure 2.14: empirical cumulative distribution function\n\n\n\n\n\n\n2.5.4 Quantile dot plots\nA quantile dot plot displays the distribution of a sample in terms of its quantiles. Reading the median or other quantiles from quantile dot plots is generally easy, we just need to count the number of dots.\nThe folowing video shows a step-by-step animation of a quantile dot plot being built.\nVideo\nFrom Figure 2.15 we can easily see that 30% of the data is below 2. We do this by noticing that we have a total of 10 dots and 3 of them are below 2.\n\nc_values = pz.Poisson(2.4).rvs(500)\naz.plot_dot(c_values, nquantiles=10);\n\n\n\n\nFigure 2.15: Quantile dot plot\n\n\n\n\nThe number of quantiles (nquantiles) is something you will need to choose by yourself, usually, it is a good idea to keep this number relatively small and “round”, as the main feature of a quantile dot plot is that finding probability intervals reduces to counting dots. It is easier to count and compute proportion if you have 10, or 20 dots than if you have 11 or 57. But sometimes a larger number could be a good idea too, for instance, if you or your audience wants to focus on the tails of the distribution a larger number of dots will give you more resolution and you still will be counting only a rather small number dots so it will be easy to compute proportions."
  },
  {
    "objectID": "MCMC_diagnostics.html",
    "href": "MCMC_diagnostics.html",
    "title": "3  MCMC Diagnostics",
    "section": "",
    "text": "The Achilles heel of computing posterior distributions is, most often than not, the computation of the denominator of Bayes’s theorem. Markov Chain Monte Carlo Methods (MCMC), such as Metropolis, Hamiltonian Monte Carlo (and its variants like NUTS) are clever mathematical and computational devices that let’s us circumvent this problem. The main idea is based on sampling values of the parameters of interest from an easy to sample distribution and then applying a rule, known as metropolis acceptance criterion, to decide if you accept or not that proposal. This rule is necessary as it provides the proper way of correcting the proposed samples to get samples from the true distribution, i.e the posterior distribution. The better performance of NUTS over Metropolis can be explained by the fact that the former uses a clever way to propose values.\nWhile MCMC methods can be successfully used to solve a huge variety of Bayesian models, this have some trade-offs. Most notably, MCMC methods can be slow for some problems, such as Big Data problems and what is most important for our current discussion finite MCMC chains are not guaranteed to converge to the true parameter value. Thus, a key step is to check whether we have a valid sample, otherwise any analysis from it will be totally flawed.\nThere are several tests we can perform, some are visual and some are numerical. These tests are designed to spot problems with our samples, but they are unable to prove we have the correct distribution; they can only provide evidence that the sample seems reasonable.\nIn this chapter we will cover the following topics:\n\nTrace plots\nRank plots\n\\(\\hat R\\)\nAutocorrelation\nEffective Sample Size\nDivergences"
  },
  {
    "objectID": "Presenting_results.html",
    "href": "Presenting_results.html",
    "title": "6  Presentation of results",
    "section": "",
    "text": "Summarizing results\nMonte Carlo Standard error and accuracy"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bessiere, Pierre, Emmanuel Mazer, Juan Manuel Ahuactzin, and Kamel\nMekhnacha. 2013. Bayesian Programming. 1 edition.\nBoca Raton: Chapman; Hall/CRC. https://www.crcpress.com/Bayesian-Programming/Bessiere-Mazer-Ahuactzin-Mekhnacha/p/book/9781439880326.\n\n\nCleveland, William S., and Robert McGill. 1984. “Graphical\nPerception: Theory, Experimentation, and Application to the Development\nof Graphical Methods.” Journal of the American Statistical\nAssociation 79 (387): 531–54. https://doi.org/10.1080/01621459.1984.10478080.\n\n\nDaniel Roy. 2015. Probabilistic Programming. http://probabilistic-programming.org.\n\n\nDiaconis, Persi. 2011. “Theories of Data\nAnalysis: From Magical\nThinking Through Classical\nStatistics.” In Exploring Data\nTables, Trends, and Shapes,\n1–36. John Wiley & Sons, Ltd. https://doi.org/10.1002/9781118150702.ch1.\n\n\nGhahramani, Zoubin. 2015. “Probabilistic Machine\nLearning and Artificial\nIntelligence.” Nature 521 (7553): 452–59.\nhttps://doi.org/10.1038/nature14541.\n\n\nHeer, Jeffrey, and Michael Bostock. 2010. “Crowdsourcing Graphical\nPerception: Using Mechanical Turk to Assess Visualization\nDesign.” In Proceedings of the SIGCHI\nConference on Human Factors in\nComputing Systems, 203–12.\nCHI ’10. New York, NY, USA: Association for Computing\nMachinery. https://doi.org/10.1145/1753326.1753357."
  }
]