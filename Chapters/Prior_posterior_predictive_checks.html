<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.554">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Exploratory Analysis of Bayesian Models - 5&nbsp; Prior and Posterior predictive checks</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../Chapters/Sensitivity_checks.html" rel="next">
<link href="../Chapters/MCMC_diagnostics.html" rel="prev">
<link href="../favicon.ico" rel="icon">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta name="citation_title" content="[[5]{.chapter-number}&nbsp; [Prior and Posterior predictive checks]{.chapter-title}]{#sec-ppc .quarto-section-identifier}">
<meta name="citation_language" content="en">
<meta name="citation_reference" content="citation_title=Graphical perception: Theory, experimentation, and application to the development of graphical methods;,citation_author=William S. Cleveland;,citation_author=Robert McGill;,citation_publication_date=1984;,citation_cover_date=1984;,citation_year=1984;,citation_fulltext_html_url= https://www.tandfonline.com/doi/abs/10.1080/01621459.1984.10478080;,citation_issue=387;,citation_doi=10.1080/01621459.1984.10478080;,citation_volume=79;,citation_journal_title=Journal of the American Statistical Association;,citation_publisher=Taylor &amp;amp;amp; Francis;">
<meta name="citation_reference" content="citation_title=Crowdsourcing graphical perception: Using mechanical turk to assess visualization design;,citation_author=Jeffrey Heer;,citation_author=Michael Bostock;,citation_publication_date=2010-04;,citation_cover_date=2010-04;,citation_year=2010;,citation_fulltext_html_url=https://doi.org/10.1145/1753326.1753357;,citation_doi=10.1145/1753326.1753357;,citation_isbn=978-1-60558-929-9;,citation_conference_title=Proceedings of the SIGCHI Conference on Human Factors in Computing Systems;,citation_conference=Association for Computing Machinery;,citation_series_title=CHI ’10;">
<meta name="citation_reference" content="citation_title=Theories of Data Analysis: From Magical Thinking Through Classical Statistics;,citation_abstract=This chapter contains sections titled: Intuitive Statistics— Some Inferential Problems Multiplicity— A Pervasive Problem Some Remedies Theories for Data Analysis Uses for Mathematics In Defense of Controlled Magical Thinking;,citation_author=Persi Diaconis;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_doi=10.1002/9781118150702.ch1;,citation_isbn=978-1-118-15070-2;,citation_inbook_title=Exploring Data Tables, Trends, and Shapes;">
<meta name="citation_reference" content="citation_title=Probabilistic Machine Learning and Artificial Intelligence;,citation_abstract=How can a machine learn from experience? Probabilistic modelling provides a framework for understanding what learning is, and has therefore emerged as one of the principal theoretical and practical approaches for designing machines that learn from data acquired through experience. The probabilistic framework, which describes how to represent and manipulate uncertainty about models and predictions, has a central role in scientific data analysis, machine learning, robotics, cognitive science and artificial intelligence. This Review provides an introduction to this framework, and discusses some of the state-of-the-art advances in the field, namely, probabilistic programming, Bayesian optimization, data compression and automatic model discovery.;,citation_author=Zoubin Ghahramani;,citation_publication_date=2015-05;,citation_cover_date=2015-05;,citation_year=2015;,citation_issue=7553;,citation_doi=10.1038/nature14541;,citation_issn=0028-0836;,citation_volume=521;,citation_journal_title=Nature;">
<meta name="citation_reference" content="citation_title=Bayesian Programming;,citation_author=Pierre Bessiere;,citation_author=Emmanuel Mazer;,citation_author=Juan Manuel Ahuactzin;,citation_author=Kamel Mekhnacha;,citation_publication_date=2013-12;,citation_cover_date=2013-12;,citation_year=2013;,citation_fulltext_html_url=https://www.crcpress.com/Bayesian-Programming/Bessiere-Mazer-Ahuactzin-Mekhnacha/p/book/9781439880326;,citation_isbn=978-1-4398-8032-6;">
<meta name="citation_reference" content="citation_title=Probabilistic Programming;,citation_author=Daniel Roy;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_fulltext_html_url=http://probabilistic-programming.org;">
<meta name="citation_reference" content="citation_title=Xarray: N-D Labeled Arrays and Datasets in Python;,citation_author=Stephan Hoyer;,citation_author=Joe Hamman;,citation_publication_date=2017-04;,citation_cover_date=2017-04;,citation_year=2017;,citation_issue=1;,citation_doi=10.5334/jors.148;,citation_issn=2049-9647;,citation_volume=5;,citation_journal_title=Journal of Open Research Software;">
<meta name="citation_reference" content="citation_title=Visualizing count data regressions using rootograms;,citation_author=Christian Kleiber;,citation_author=Achim Zeileis;,citation_publication_date=2016-07;,citation_cover_date=2016-07;,citation_year=2016;,citation_fulltext_html_url=http://dx.doi.org/10.1080/00031305.2016.1173590;,citation_issue=3;,citation_doi=10.1080/00031305.2016.1173590;,citation_issn=1537-2731;,citation_volume=70;,citation_journal_title=The American Statistician;,citation_publisher=Informa UK Limited;">
<meta name="citation_reference" content="citation_title=Satellite male groups in horseshoe crabs, limulus polyphemus;,citation_author=H. Jane Brockmann;,citation_publication_date=1996;,citation_cover_date=1996;,citation_year=1996;,citation_fulltext_html_url=https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1439-0310.1996.tb01099.x;,citation_issue=1;,citation_doi=10.1111/j.1439-0310.1996.tb01099.x;,citation_volume=102;,citation_journal_title=Ethology;">
<meta name="citation_reference" content="citation_title=Exploratory Data Analysis;,citation_author=John W. Tukey;,citation_publication_date=1977;,citation_cover_date=1977;,citation_year=1977;,citation_isbn=978-0-201-07616-5;">
<meta name="citation_reference" content="citation_title=The separation plot: A new visual method for evaluating the fit of binary models;,citation_author=Brian Greenhill;,citation_author=Michael D. Ward;,citation_author=Audrey Sacks;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_fulltext_html_url=https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-5907.2011.00525.x;,citation_issue=4;,citation_doi=10.1111/j.1540-5907.2011.00525.x;,citation_volume=55;,citation_journal_title=American Journal of Political Science;">
<meta name="citation_reference" content="citation_title=Detecting and diagnosing prior and likelihood sensitivity with power-scaling;,citation_author=Noa Kallioinen;,citation_author=Topi Paananen;,citation_author=Paul-Christian Bürkner;,citation_author=Aki Vehtari;,citation_publication_date=2023-12;,citation_cover_date=2023-12;,citation_year=2023;,citation_fulltext_html_url=https://doi.org/10.1007/s11222-023-10366-5;,citation_issue=1;,citation_doi=10.1007/s11222-023-10366-5;,citation_issn=1573-1375;,citation_volume=34;,citation_journal_title=Statistics and Computing;">
<meta name="citation_reference" content="citation_title=Graphical test for discrete uniformity and its applications in goodness-of-fit evaluation and multiple sample comparison;,citation_author=Teemu Säilynoja;,citation_author=Paul-Christian Bürkner;,citation_author=Aki Vehtari;,citation_publication_date=2022-03;,citation_cover_date=2022-03;,citation_year=2022;,citation_fulltext_html_url=https://doi.org/10.1007/s11222-022-10090-6;,citation_issue=2;,citation_doi=10.1007/s11222-022-10090-6;,citation_issn=1573-1375;,citation_volume=32;,citation_journal_title=Statistics and Computing;">
<meta name="citation_reference" content="citation_title=Validating bayesian inference algorithms with simulation-based calibration;,citation_author=Sean Talts;,citation_author=Michael Betancourt;,citation_author=Daniel Simpson;,citation_author=Aki Vehtari;,citation_author=Andrew Gelman;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_fulltext_html_url=https://arxiv.org/abs/1804.06788;">
<meta name="citation_reference" content="citation_title=On thinning of chains in MCMC;,citation_author=William A. Link;,citation_author=Mitchell J. Eaton;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_fulltext_html_url=https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/j.2041-210X.2011.00131.x;,citation_issue=1;,citation_doi=10.1111/j.2041-210X.2011.00131.x;,citation_volume=3;,citation_journal_title=Methods in Ecology and Evolution;">
<meta name="citation_reference" content="citation_title=Subsampling the Gibbs Sampler;,citation_author=Steven N. MacEachern;,citation_author=L. Mark Berliner;,citation_publication_date=1994;,citation_cover_date=1994;,citation_year=1994;,citation_fulltext_html_url=https://www.jstor.org/stable/2684714;,citation_issue=3;,citation_doi=10.2307/2684714;,citation_issn=0003-1305;,citation_volume=48;,citation_journal_title=The American Statistician;">
<meta name="citation_reference" content="citation_title=The Prior Can Often Only Be Understood in the Context of the Likelihood;,citation_author=Andrew Gelman;,citation_author=Daniel Simpson;,citation_author=Michael Betancourt;,citation_publication_date=2017-10;,citation_cover_date=2017-10;,citation_year=2017;,citation_fulltext_html_url=https://www.mdpi.com/1099-4300/19/10/555;,citation_issue=10;,citation_doi=10.3390/e19100555;,citation_issn=1099-4300;,citation_volume=19;,citation_journal_title=Entropy;">
<meta name="citation_reference" content="citation_title=Prior Knowledge Elicitation: The Past, Present, and Future;,citation_author=Petrus Mikkola;,citation_author=Osvaldo A. Martin;,citation_author=Suyog Chandramouli;,citation_author=Marcelo Hartmann;,citation_author=Oriol Abril Pla;,citation_author=Owen Thomas;,citation_author=Henri Pesonen;,citation_author=Jukka Corander;,citation_author=Aki Vehtari;,citation_author=Samuel Kaski;,citation_author=Paul-Christian Bürkner;,citation_author=Arto Klami;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://doi.org/10.1214/23-BA1381;,citation_issue=4;,citation_doi=10.1214/23-BA1381;,citation_volume=19;,citation_journal_title=Bayesian Analysis;,citation_publisher=International Society for Bayesian Analysis;">
<meta name="citation_reference" content="citation_title=Probability Theory: The Logic of Science;,citation_abstract=Going beyond the conventional mathematics of probability theory, this study views the subject in a wider context. It discusses new results, along with applications of probability theory to a variety of problems. The book contains many exercises and is suitable for use as a textbook on graduate-level courses involving data analysis. Aimed at readers already familiar with applied mathematics at an advanced undergraduate level or higher, it is of interest to scientists concerned with inference from incomplete information.;,citation_author=E. T. Jaynes;,citation_editor=G. Larry Bretthorst;,citation_publication_date=2003-06;,citation_cover_date=2003-06;,citation_year=2003;,citation_fulltext_html_url=https://bayes.wustl.edu/etj/prob/book.pdf;,citation_isbn=978-0-521-59271-0;">
<meta name="citation_reference" content="citation_title=PreliZ: A tool-box for prior elicitation;,citation_author=Alejandro Icazatti;,citation_author=Oriol Abril-Pla;,citation_author=Arto Klami;,citation_author=Osvaldo A Martin;,citation_publication_date=2023-09;,citation_cover_date=2023-09;,citation_year=2023;,citation_fulltext_html_url=https://joss.theoj.org/papers/10.21105/joss.05499;,citation_issue=89;,citation_doi=10.21105/joss.05499;,citation_volume=8;,citation_journal_title=Journal of Open Source Software;">
<meta name="citation_reference" content="citation_title=Bayesian workflow;,citation_author=Andrew Gelman;,citation_author=Aki Vehtari;,citation_author=Daniel Simpson;,citation_author=Charles C. Margossian;,citation_author=Bob Carpenter;,citation_author=Yuling Yao;,citation_author=Lauren Kennedy;,citation_author=Jonah Gabry;,citation_author=Paul-Christian Bürkner;,citation_author=Martin Modrák;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_fulltext_html_url=https://arxiv.org/abs/2011.01808;">
<meta name="citation_reference" content="citation_title=A web-based tool for eliciting probability distributions from experts;,citation_author=David E. Morris;,citation_author=Jeremy E. Oakley;,citation_author=John A. Crowe;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_fulltext_html_url=https://www.sciencedirect.com/science/article/pii/S1364815213002533;,citation_doi=10.1016/j.envsoft.2013.10.010;,citation_issn=1364-8152;,citation_volume=52;,citation_journal_title=Environmental Modelling &amp;amp;amp; Software;">
<meta name="citation_reference" content="citation_title=Bayesian Modeling and Computation in Python;,citation_author=Osvaldo A. Martin;,citation_author=Ravin Kumar;,citation_author=Junpeng Lao;,citation_publication_date=2021-12;,citation_cover_date=2021-12;,citation_year=2021;,citation_fulltext_html_url=https://bayesiancomputationbook.com/;,citation_isbn=978-0-367-89436-8;">
<meta name="citation_reference" content="citation_title=Bayesian Analysis with Python: A Practical Guide to probabilistic modeling, 3rd Edition;,citation_author=Osvaldo A Martin;,citation_publication_date=2024-02;,citation_cover_date=2024-02;,citation_year=2024;,citation_fulltext_html_url=https://bap.com.ar/;,citation_isbn=978-1-80512-716-1;">
<meta name="citation_reference" content="citation_title=BART: Bayesian additive regression trees;,citation_author=Hugh A. Chipman;,citation_author=Edward I. George;,citation_author=Robert E. McCulloch;,citation_publication_date=2010-03;,citation_cover_date=2010-03;,citation_year=2010;,citation_fulltext_html_url=http://projecteuclid.org/euclid.aoas/1273584455;,citation_issue=1;,citation_doi=10.1214/09-AOAS285;,citation_issn=1932-6157;,citation_volume=4;,citation_journal_title=The Annals of Applied Statistics;">
<meta name="citation_reference" content="citation_title=Practical bayesian model evaluation using leave-one-out cross-validation and WAIC;,citation_author=Aki Vehtari;,citation_author=Andrew Gelman;,citation_author=Jonah Gabry;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_fulltext_html_url=https://doi.org/10.1007/s11222-016-9696-4;,citation_issue=5;,citation_doi=10.1007/s11222-016-9696-4;,citation_volume=27;,citation_journal_title=Statistics and Computing;">
<meta name="citation_reference" content="citation_title=Using Stacking to Average Bayesian Predictive Distributions (with Discussion);,citation_author=Yuling Yao;,citation_author=Aki Vehtari;,citation_author=Daniel Simpson;,citation_author=Andrew Gelman;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_fulltext_html_url=https://doi.org/10.1214/17-BA1091;,citation_issue=3;,citation_doi=10.1214/17-BA1091;,citation_volume=13;,citation_journal_title=Bayesian Analysis;,citation_publisher=International Society for Bayesian Analysis;">
<meta name="citation_reference" content="citation_title=A Widely Applicable Bayesian Information Criterion;,citation_author=Sumio Watanabe;,citation_publication_date=2013-03;,citation_cover_date=2013-03;,citation_year=2013;,citation_fulltext_html_url=https://dl.acm.org/doi/10.5555/2567709.2502609;,citation_volume=14;,citation_journal_title=Journal of Machine Learning Research;">
<meta name="citation_reference" content="citation_title=A new look at the statistical model identification;,citation_author=H. Akaike;,citation_publication_date=1974;,citation_cover_date=1974;,citation_year=1974;,citation_issue=6;,citation_doi=10.1109/TAC.1974.1100705;,citation_volume=19;,citation_journal_title=IEEE Transactions on Automatic Control;">
<meta name="citation_reference" content="citation_title=Rank-Normalization, Folding, and Localization: An Improved \widehat{R} for Assessing Convergence of MCMC (with Discussion);,citation_author=Aki Vehtari;,citation_author=Andrew Gelman;,citation_author=Daniel Simpson;,citation_author=Bob Carpenter;,citation_author=Paul-Christian Bürkner;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://doi.org/10.1214/20-BA1221;,citation_issue=2;,citation_doi=10.1214/20-BA1221;,citation_volume=16;,citation_journal_title=Bayesian Analysis;,citation_publisher=International Society for Bayesian Analysis;">
<meta name="citation_reference" content="citation_title=Projective inference in high-dimensional problems: Prediction and feature selection;,citation_author=Juho Piironen;,citation_author=Markus Paasiniemi;,citation_author=Aki Vehtari;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_fulltext_html_url=https://doi.org/10.1214/20-EJS1711;,citation_issue=1;,citation_doi=10.1214/20-EJS1711;,citation_volume=14;,citation_journal_title=Electronic Journal of Statistics;,citation_publisher=Institute of Mathematical Statistics; Bernoulli Society;">
<meta name="citation_reference" content="citation_title=Bayesian additive regression trees for probabilistic programming;,citation_author=Miriana Quiroga;,citation_author=Pablo G Garay;,citation_author=Juan M. Alonso;,citation_author=Juan Martin Loyola;,citation_author=Osvaldo A Martin;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2206.03619;,citation_doi=10.48550/ARXIV.2206.03619;,citation_publisher=arXiv;">
<meta name="citation_reference" content="citation_title=Robust and efficient projection predictive inference;,citation_author=Yann McLatchie;,citation_author=Sölvi Rögnvaldsson;,citation_author=Frank Weber;,citation_author=Aki Vehtari;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2306.15581;">
<meta name="citation_reference" content="citation_title=Fitting percentage of body fat to simple body measurements;,citation_author=Roger W. Johnson;,citation_publication_date=1996;,citation_cover_date=1996;,citation_year=1996;,citation_fulltext_html_url=https://doi.org/10.1080/10691898.1996.11910505;,citation_issue=1;,citation_doi=10.1080/10691898.1996.11910505;,citation_volume=4;,citation_journal_title=Journal of Statistics Education;,citation_publisher=Taylor &amp;amp;amp; Francis;">
<meta name="citation_reference" content="citation_title=Non-parametric jensen-shannon divergence;,citation_author=Hoang-Vu Nguyen;,citation_author=Jilles Vreeken;,citation_editor=Annalisa Appice;,citation_editor=Pedro Pereira Rodrigues;,citation_editor=Vítor Santos Costa;,citation_editor=João Gama;,citation_editor=Alípio Jorge;,citation_editor=Carlos Soares;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_fulltext_html_url=https://doi.org/10.1007/978-3-319-23525-7_11;,citation_doi=10.1007/978-3-319-23525-7_11;,citation_conference_title=Machine learning and knowledge discovery in databases;,citation_conference=Springer International Publishing;">
<meta name="citation_reference" content="citation_title=Modern Applied Statistics with S;,citation_author=W. N. Venables;,citation_author=B. D. Ripley;,citation_publication_date=2002-08;,citation_cover_date=2002-08;,citation_year=2002;,citation_fulltext_html_url=https://doi.org/10.1007/978-0-387-21706-2;,citation_doi=10.1007/978-0-387-21706-2;,citation_isbn=978-0-387-95457-8;">
<meta name="citation_reference" content="citation_title=Satellite male groups in horseshoe crabs, limulus polyphemus;,citation_author=H. Jane Brockmann;,citation_publication_date=1996;,citation_cover_date=1996;,citation_year=1996;,citation_fulltext_html_url=https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1439-0310.1996.tb01099.x;,citation_issue=1;,citation_doi=10.1111/j.1439-0310.1996.tb01099.x;,citation_volume=102;,citation_journal_title=Ethology;">
<meta name="citation_reference" content="citation_title=Recommendations for visual predictive checks in bayesian workflow;,citation_author=Teemu Säilynoja;,citation_author=Andrew R. Johnson;,citation_author=Osvaldo A. Martin;,citation_author=Aki Vehtari;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_fulltext_html_url=https://arxiv.org/abs/2503.01509;">
<meta name="citation_reference" content="citation_title=Visualization in bayesian workflow;,citation_author=Jonah Gabry;,citation_author=Daniel Simpson;,citation_author=Aki Vehtari;,citation_author=Michael Betancourt;,citation_author=Andrew Gelman;,citation_publication_date=2019-01;,citation_cover_date=2019-01;,citation_year=2019;,citation_fulltext_html_url=https://doi.org/10.1111/rssa.12378;,citation_issue=2;,citation_doi=10.1111/rssa.12378;,citation_issn=0964-1998;,citation_volume=182;,citation_journal_title=Journal of the Royal Statistical Society Series A: Statistics in Society;">
<meta name="citation_reference" content="citation_title=Posterior Predictive p-Values;,citation_author=Xiao-Li Meng;,citation_publication_date=1994;,citation_cover_date=1994;,citation_year=1994;,citation_fulltext_html_url=https://doi.org/10.1214/aos/1176325622;,citation_issue=3;,citation_doi=10.1214/aos/1176325622;,citation_volume=22;,citation_journal_title=The Annals of Statistics;,citation_publisher=Institute of Mathematical Statistics;">
<meta name="citation_reference" content="citation_title=Bayesian Data Analysis;,citation_author=Andrew Gelman;,citation_author=John B. Carlin;,citation_author=Hal S. Stern;,citation_author=David B. Dunson;,citation_author=Aki Vehtari;,citation_author=Donald B. Rubin;,citation_publication_date=2013-11;,citation_cover_date=2013-11;,citation_year=2013;,citation_isbn=978-1-4398-4095-5;">
<meta name="citation_reference" content="citation_title=Two simple examples for understanding posterior p-values whose distributions are far from uniform;,citation_author=Andrew Gelman;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_fulltext_html_url=https://doi.org/10.1214/13-EJS854;,citation_issue=none;,citation_doi=10.1214/13-EJS854;,citation_volume=7;,citation_journal_title=Electronic Journal of Statistics;,citation_publisher=Institute of Mathematical Statistics; Bernoulli Society;">
<meta name="citation_reference" content="citation_title=Stable reliability diagrams for probabilistic classifiers;,citation_author=Timo Dimitriadis;,citation_author=Tilmann Gneiting;,citation_author=Alexander I. Jordan;,citation_publication_date=2021-02;,citation_cover_date=2021-02;,citation_year=2021;,citation_fulltext_html_url=https://www.pnas.org/doi/abs/10.1073/pnas.2016191118;,citation_issue=8;,citation_doi=10.1073/pnas.2016191118;,citation_volume=118;,citation_journal_title=Proceedings of the National Academy of Sciences;">
<meta name="citation_reference" content="citation_title=An Empirical Distribution Function for Sampling with Incomplete Information;,citation_author=Miriam Ayer;,citation_author=H. D. Brunk;,citation_author=G. M. Ewing;,citation_author=W. T. Reid;,citation_author=Edward Silverman;,citation_publication_date=1955;,citation_cover_date=1955;,citation_year=1955;,citation_fulltext_html_url=https://doi.org/10.1214/aoms/1177728423;,citation_issue=4;,citation_doi=10.1214/aoms/1177728423;,citation_volume=26;,citation_journal_title=The Annals of Mathematical Statistics;,citation_publisher=Institute of Mathematical Statistics;">
<meta name="citation_reference" content="citation_title=Prior knowledge elicitation: The past, present, and future;,citation_author=Petrus Mikkola;,citation_author=Osvaldo A. Martin;,citation_author=Suyog Chandramouli;,citation_author=Marcelo Hartmann;,citation_author=Oriol Abril Pla;,citation_author=Owen Thomas;,citation_author=Henri Pesonen;,citation_author=Jukka Corander;,citation_author=Aki Vehtari;,citation_author=Samuel Kaski;,citation_author=Paul-Christian Bürkner;,citation_author=Arto Klami;,citation_publication_date=2024-12;,citation_cover_date=2024-12;,citation_year=2024;,citation_issue=4;,citation_doi=10.1214/23-BA1381;,citation_volume=19;,citation_journal_title=Bayesian Analysis;">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../Chapters/Prior_posterior_predictive_checks.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Prior and Posterior predictive checks</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Exploratory Analysis of Bayesian Models</a> 
        <div class="sidebar-tools-main tools-wide">
    <div class="dropdown">
      <a href="" title="github" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="github"><i class="bi bi-github"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://github.com/arviz-devs/Exploratory-Analysis-of-Bayesian-Models">
            source
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://github.com/arviz-devs/Exploratory-Analysis-of-Bayesian-Models/issues/new">
            issues
            </a>
          </li>
      </ul>
    </div>
    <a href="https://numfocus.org/donate-to-arviz" title="donations" class="quarto-navigation-tool px-1" aria-label="donations"><i class="bi bi-coin"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">‎</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Chapters/Elements_of_visualization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Elements of Visualization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Chapters/DataTree.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Working with DataTree</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Chapters/Distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Random Variables, Distributions, and Uncertainty</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Chapters/MCMC_diagnostics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">MCMC Diagnostics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Chapters/Prior_posterior_predictive_checks.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Prior and Posterior predictive checks</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Chapters/Sensitivity_checks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Prior and likelihood sensitivity checks</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Chapters/Model_comparison.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Model Comparison</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Chapters/Case_study_model_comparison.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Model Comparison (case study)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Chapters/Variable_selection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Variable Selection</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Chapters/Prior_elicitation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Prior Elicitation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Chapters/Presenting_results.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Presentation of Results</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Chapters/Bayesian_workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Bayesian Workflow</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Chapters/References.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#prior-predictive-checks" id="toc-prior-predictive-checks" class="nav-link active" data-scroll-target="#prior-predictive-checks"><span class="header-section-number">5.1</span> Prior predictive checks</a>
  <ul class="collapse">
  <li><a href="#a-final-note-about-priors" id="toc-a-final-note-about-priors" class="nav-link" data-scroll-target="#a-final-note-about-priors"><span class="header-section-number">5.1.1</span> A final note about priors</a></li>
  </ul></li>
  <li><a href="#posterior-predictive-checks" id="toc-posterior-predictive-checks" class="nav-link" data-scroll-target="#posterior-predictive-checks"><span class="header-section-number">5.2</span> Posterior predictive checks</a>
  <ul class="collapse">
  <li><a href="#using-summary-statistics" id="toc-using-summary-statistics" class="nav-link" data-scroll-target="#using-summary-statistics"><span class="header-section-number">5.2.1</span> Using summary statistics</a></li>
  <li><a href="#pit-ecdfs" id="toc-pit-ecdfs" class="nav-link" data-scroll-target="#pit-ecdfs"><span class="header-section-number">5.2.2</span> PIT-ECDFs</a></li>
  <li><a href="#coverage" id="toc-coverage" class="nav-link" data-scroll-target="#coverage"><span class="header-section-number">5.2.3</span> Coverage</a></li>
  <li><a href="#sec-avoid-double-dipping" id="toc-sec-avoid-double-dipping" class="nav-link" data-scroll-target="#sec-avoid-double-dipping"><span class="header-section-number">5.2.4</span> Avoiding double-dipping</a></li>
  <li><a href="#hypothetical-outcome-plots" id="toc-hypothetical-outcome-plots" class="nav-link" data-scroll-target="#hypothetical-outcome-plots"><span class="header-section-number">5.2.5</span> Hypothetical Outcome Plots</a></li>
  </ul></li>
  <li><a href="#posterior-predictive-checks-for-discrete-data" id="toc-posterior-predictive-checks-for-discrete-data" class="nav-link" data-scroll-target="#posterior-predictive-checks-for-discrete-data"><span class="header-section-number">5.3</span> Posterior predictive checks for discrete data</a>
  <ul class="collapse">
  <li><a href="#posterior-predictive-checks-for-count-data" id="toc-posterior-predictive-checks-for-count-data" class="nav-link" data-scroll-target="#posterior-predictive-checks-for-count-data"><span class="header-section-number">5.3.1</span> Posterior predictive checks for count data</a></li>
  <li><a href="#posterior-predictive-checks-for-binary-data" id="toc-posterior-predictive-checks-for-binary-data" class="nav-link" data-scroll-target="#posterior-predictive-checks-for-binary-data"><span class="header-section-number">5.3.2</span> Posterior predictive checks for binary data</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-ppc" class="quarto-section-identifier"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Prior and Posterior predictive checks</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Models are simplifications of reality, sometimes even very crude simplifications. Thus, we can never fully trust them. While hoping they are good enough is an option, we should try to do better. One general approach to criticizing model is to judge them by their predictions. If a model is robust, its predictions should align well with observations, domain knowledge, or some other benchmark. There are at least four avenues to explore:</p>
<ul>
<li><p>Compare to prior knowledge. For example, if we are modelling the size of planets we can evaluate if the model is making predictions in a sensible range. Even if we are equipped with a very rudimentary knowledge of astronomy we know that planets are larger than persons and smaller than galaxies. So if the model is predicting that the size of a planet is 1 meter, then we know that the model is not that good. The better your prior knowledge is, the better you will be able to critique your model assumptions. If you are not an expert in the field, and maybe even if you are, you can always try to find someone who is.</p></li>
<li><p>Compare to observed data. We fit a model and compare the predictions to the same data that we used to fit the model. This is an internal consistency check of the model, and we should expect good agreement. But reality is complex and models can be too simple or they can be misspecified so there is a lot of potential in these types of checks. Additionally, even very good models might be good at recovering some aspects of the data but not others, for instance, a model could be good at predicting the bulk of the data, but it could overestimate extreme values.</p></li>
<li><p>Compared to unobserved data. We fit a model to one dataset and then evaluate it on a different dataset. This is similar to the previous point, but this is a more stringent test because the model is being asked to make predictions on data that it has not seen before. How similar the observed and unobserved data are will depend on the problem. For instance, a model trained with data from a particular population of elephants might do a good job at predicting the weight of elephants in general, but it might not do a good job at predicting the weight of other mammals like shrews.</p></li>
<li><p>Compare to other models. We fit different models to the same data and then compare the predictions of the models. This particular case is discussed in detail on <a href="Model_comparison.html" class="quarto-xref"><span>Chapter 7</span></a>.</p></li>
</ul>
<p>As we can see there are plenty of options to evaluate models. But we still have one additional ingredient to add to the mix, we have omitted the fact that we have different types of predictions. An attractive feature of the Bayesian model is that they are generative. This means that we can simulate synthetic data from models as long as the parameters are assigned a proper probability distribution, computationally we need a distribution from which we can generate random samples. We can take advantage of this feature to check models before or after fitting the data:</p>
<ul>
<li>Prior predictive: We generate synthetic observations without conditioning on the observed data. These are predictions that we can make before we have seen the actual data.</li>
<li>Posterior predictive: We generate synthetic observations after conditioning on the observed data. These are predictions that we can make after we have seen the data.</li>
</ul>
<p>Additionally, for models like linear regression where we have a set of covariates, we can generate synthetic data evaluated at the observed covariates (our “Xs”) or at different values (“X_new”). If we do the first we call it in-sample predictions, and if we do the second we call it out-of-sample predictions.</p>
<p>With so many options we can feel overwhelmed. Which ones we should use will depend on what we want to evaluate. We can use a combination of the previous options to evaluate models for different purposes. In the next sections, we will see how to implement some of these checks.</p>
<section id="prior-predictive-checks" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="prior-predictive-checks"><span class="header-section-number">5.1</span> Prior predictive checks</h2>
<p>The idea behind prior predictive checks is very general and simple: if a model is good it should be able generate data resembling our prior knowledge. We call these checks, prior predictive because we are generating synthetic data before we have seen the actual data.</p>
<p>The general algorithm for prior predictive checks is:</p>
<ol type="1">
<li>Draw <span class="math inline">\(N\)</span> realizations from a prior distribution.</li>
<li>For each draw, simulate new data from the likelihood.</li>
<li>Plot the results.</li>
<li>Use domain knowledge to assess whether simulated values reflect prior knowledge.</li>
<li>If simulated values do not reflect prior knowledge, change the prior distribution, likelihood, or both and repeat the simulation from step 1.</li>
<li>If simulated values reflect prior knowledge, compute the posterior.</li>
</ol>
<p>Notice that in step 4 we use domain knowledge, NOT observed data!</p>
<p>In steps 1 and 2 what we are doing is approximating this integral: <span class="math display">\[
p(y^\ast) = \int_{\Theta} p(y^\ast \mid \theta) \; p(\theta) \; d\theta
\]</span></p>
<p>where <span class="math inline">\(y^\ast\)</span> represents unobserved but potentially observable data. Notice that to compute <span class="math inline">\(y^\ast\)</span> we are evaluating the likelihood over all possible values ​​of the prior. Thus we are effectively marginalizing out the values of <span class="math inline">\(\theta\)</span>, the parameters.</p>
<p>To exemplify a prior predictive check, let’s try with a super simple example. Let’s say we want to model the height of humans. We know that the heights are positive numbers, so we should use a distribution that assigns zero mass to negative values. But we also know that at least for adults using a normal distribution could be a good approximation. So we create the following model, without too much thought, and then draw 500 samples from the prior predictive distribution.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true">PyMC</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false">CmdStanPy</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<div id="d527318d" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="cf">with</span> pm.Model() <span class="im">as</span> model: </span>
<span id="cb1-2"><a href="#cb1-2"></a>    <span class="co"># Priors for unknown model parameters</span></span>
<span id="cb1-3"><a href="#cb1-3"></a>    mu <span class="op">=</span> pm.Normal(<span class="st">'mu'</span>, mu<span class="op">=</span><span class="dv">0</span>, sigma<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb1-4"><a href="#cb1-4"></a>    sigma <span class="op">=</span> pm.HalfNormal(<span class="st">'sigma'</span>, sigma<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb1-5"><a href="#cb1-5"></a>    <span class="co"># Likelihood (sampling distribution) of observations</span></span>
<span id="cb1-6"><a href="#cb1-6"></a>    y_obs <span class="op">=</span> pm.Normal(<span class="st">'Y_obs'</span>, mu<span class="op">=</span>mu, sigma<span class="op">=</span>sigma, observed<span class="op">=</span>y)</span>
<span id="cb1-7"><a href="#cb1-7"></a>    <span class="co"># draw 500 samples from the prior predictive</span></span>
<span id="cb1-8"><a href="#cb1-8"></a>    dt <span class="op">=</span> pm.sample_prior_predictive(samples<span class="op">=</span><span class="dv">500</span>, random_seed<span class="op">=</span>SEED)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<div class="sourceCode" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a>stan_code <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="st">data {</span></span>
<span id="cb2-3"><a href="#cb2-3"></a><span class="st">    int&lt;lower=0&gt; N;</span></span>
<span id="cb2-4"><a href="#cb2-4"></a><span class="st">    array[N] real y;</span></span>
<span id="cb2-5"><a href="#cb2-5"></a><span class="st">}</span></span>
<span id="cb2-6"><a href="#cb2-6"></a><span class="st">parameters {</span></span>
<span id="cb2-7"><a href="#cb2-7"></a><span class="st">    real mu;</span></span>
<span id="cb2-8"><a href="#cb2-8"></a><span class="st">    real&lt;lower=0&gt; sigma;</span></span>
<span id="cb2-9"><a href="#cb2-9"></a><span class="st">}</span></span>
<span id="cb2-10"><a href="#cb2-10"></a><span class="st">model {</span></span>
<span id="cb2-11"><a href="#cb2-11"></a><span class="st">    // Priors</span></span>
<span id="cb2-12"><a href="#cb2-12"></a><span class="st">    mu ~ normal(0, 10);</span></span>
<span id="cb2-13"><a href="#cb2-13"></a><span class="st">    sigma ~ normal(0, 10);</span></span>
<span id="cb2-14"><a href="#cb2-14"></a><span class="st">    </span></span>
<span id="cb2-15"><a href="#cb2-15"></a><span class="st">    // Likelihood</span></span>
<span id="cb2-16"><a href="#cb2-16"></a><span class="st">    y ~ normal(mu, sigma);</span></span>
<span id="cb2-17"><a href="#cb2-17"></a><span class="st">}</span></span>
<span id="cb2-18"><a href="#cb2-18"></a><span class="st">generated quantities {</span></span>
<span id="cb2-19"><a href="#cb2-19"></a><span class="st">    real prior_mu = normal_rng(0, 10);</span></span>
<span id="cb2-20"><a href="#cb2-20"></a><span class="st">    real&lt;lower=0&gt; prior_sigma = abs(normal_rng(0, 10));</span></span>
<span id="cb2-21"><a href="#cb2-21"></a><span class="st">    array[N] real y_prior_pred;</span></span>
<span id="cb2-22"><a href="#cb2-22"></a><span class="st">    for (i in 1:N) {</span></span>
<span id="cb2-23"><a href="#cb2-23"></a><span class="st">        y_prior_pred[i] = normal_rng(prior_mu, prior_sigma);</span></span>
<span id="cb2-24"><a href="#cb2-24"></a><span class="st">    }</span></span>
<span id="cb2-25"><a href="#cb2-25"></a><span class="st">}</span></span>
<span id="cb2-26"><a href="#cb2-26"></a><span class="st">"""</span></span>
<span id="cb2-27"><a href="#cb2-27"></a></span>
<span id="cb2-28"><a href="#cb2-28"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">"./stan_code.stan"</span>, <span class="st">"w"</span>) <span class="im">as</span> f:</span>
<span id="cb2-29"><a href="#cb2-29"></a>    <span class="bu">print</span>(stan_code, <span class="bu">file</span><span class="op">=</span>f)</span>
<span id="cb2-30"><a href="#cb2-30"></a></span>
<span id="cb2-31"><a href="#cb2-31"></a><span class="co"># Compile the model</span></span>
<span id="cb2-32"><a href="#cb2-32"></a>model <span class="op">=</span> cmdstanpy.CmdStanModel(stan_file<span class="op">=</span><span class="st">"./stan_code.stan"</span>)</span>
<span id="cb2-33"><a href="#cb2-33"></a></span>
<span id="cb2-34"><a href="#cb2-34"></a><span class="co"># Prepare the data</span></span>
<span id="cb2-35"><a href="#cb2-35"></a>stan_data <span class="op">=</span> {</span>
<span id="cb2-36"><a href="#cb2-36"></a>    <span class="st">'N'</span>: <span class="bu">len</span>(y),</span>
<span id="cb2-37"><a href="#cb2-37"></a>    <span class="st">'y'</span>: y</span>
<span id="cb2-38"><a href="#cb2-38"></a>}</span>
<span id="cb2-39"><a href="#cb2-39"></a></span>
<span id="cb2-40"><a href="#cb2-40"></a><span class="co"># Sample from the prior predictive distribution</span></span>
<span id="cb2-41"><a href="#cb2-41"></a>prior_samples <span class="op">=</span> model.sample(</span>
<span id="cb2-42"><a href="#cb2-42"></a>    eata<span class="op">=</span>stan_data,</span>
<span id="cb2-43"><a href="#cb2-43"></a>    fixed_param<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb2-44"><a href="#cb2-44"></a>    iter_sampling<span class="op">=</span><span class="dv">500</span>,</span>
<span id="cb2-45"><a href="#cb2-45"></a>    seed<span class="op">=</span>SEED</span>
<span id="cb2-46"><a href="#cb2-46"></a>)</span>
<span id="cb2-47"><a href="#cb2-47"></a></span>
<span id="cb2-48"><a href="#cb2-48"></a><span class="co"># Convert to ArviZ</span></span>
<span id="cb2-49"><a href="#cb2-49"></a>dt <span class="op">=</span> azp.from_cmdstanpy(</span>
<span id="cb2-50"><a href="#cb2-50"></a>    prior<span class="op">=</span>prior_samples,</span>
<span id="cb2-51"><a href="#cb2-51"></a>    prior_predictive<span class="op">=</span><span class="st">"y_prior_pred"</span>,</span>
<span id="cb2-52"><a href="#cb2-52"></a>    observed_data<span class="op">=</span>{<span class="st">"y"</span>: y},</span>
<span id="cb2-53"><a href="#cb2-53"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<p>The figure below displays samples from the prior predictive distribution (shown as solid blue lines). To aid interpretation, we have included two reference values: the average length/height of a newborn (approximately 50 cm) and the average height of adult males in the Netherlands (around 182 cm). Reference values are meaningful benchmarks derived from domain knowledge—not from the observed data—and help assess whether predictions are on a reasonable scale. While there are no strict rules for selecting reference values, different analysts might choose different benchmarks based on context</p>
<div id="cell-fig-prior_pred_check_wide" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a>pc <span class="op">=</span> azp.plot_ppc_dist(dt, group<span class="op">=</span><span class="st">"prior_predictive"</span>)</span>
<span id="cb3-2"><a href="#cb3-2"></a>azp.add_lines(pc, values<span class="op">=</span>(<span class="dv">50</span>, <span class="dv">182</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-prior_pred_check_wide" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-prior_pred_check_wide-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Prior_posterior_predictive_checks_files/figure-html/fig-prior_pred_check_wide-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-prior_pred_check_wide-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.1: The prior predictive check for the model of heights. We can see that the bulk of the samples are outside the reference values.
</figcaption>
</figure>
</div>
</div>
</div>
<p>We can see that our model is bananas the bulk of the prior predictive distribution is outside of our reference values and the model is predicting values below 0. This is a clear indication that the model is not a good representation of our prior knowledge.</p>
<p>In many cases, data will be informative enough to overcome poorly selected priors, but this isn’t guaranteed. To address this, we can tighten our priors. While there’s no universal rule for doing so, a good guideline is to choose priors that concentrate most of the prior predictive distribution’s mass within a plausible range—such as between our reference values.</p>
<p>Such priors are often called weakly informative priors. Though not strictly defined, these priors produce a prior predictive distribution with little to no probability mass in unrealistic or impossible regions. For example, a normal distribution with a mean of 160 and a standard deviation of 10 assigns negligible weight to negative values while still accommodating a wide range of plausible heights.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-2-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-1" role="tab" aria-controls="tabset-2-1" aria-selected="true">PyMC</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-2-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-2" role="tab" aria-controls="tabset-2-2" aria-selected="false">CmdStanPy</a></li></ul>
<div class="tab-content">
<div id="tabset-2-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-2-1-tab">
<div id="5a41a56f" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="cf">with</span> pm.Model() <span class="im">as</span> model: </span>
<span id="cb4-2"><a href="#cb4-2"></a>    <span class="co"># Priors for unknown model parameters</span></span>
<span id="cb4-3"><a href="#cb4-3"></a>    mu <span class="op">=</span> pm.Normal(<span class="st">'mu'</span>, mu<span class="op">=</span><span class="dv">160</span>, sigma<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb4-4"><a href="#cb4-4"></a>    sigma <span class="op">=</span> pm.HalfNormal(<span class="st">'sigma'</span>, sigma<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb4-5"><a href="#cb4-5"></a>    <span class="co"># Likelihood (sampling distribution) of observations</span></span>
<span id="cb4-6"><a href="#cb4-6"></a>    y_obs <span class="op">=</span> pm.Normal(<span class="st">'Y_obs'</span>, mu<span class="op">=</span>mu, sigma<span class="op">=</span>sigma, observed<span class="op">=</span>y)</span>
<span id="cb4-7"><a href="#cb4-7"></a>    <span class="co"># draw 500 samples from the prior predictive</span></span>
<span id="cb4-8"><a href="#cb4-8"></a>    dt <span class="op">=</span> pm.sample_prior_predictive(samples<span class="op">=</span><span class="dv">500</span>, random_seed<span class="op">=</span>SEED)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div id="tabset-2-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-2-2-tab">
<div class="sourceCode" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a>stan_code <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="st">data {</span></span>
<span id="cb5-3"><a href="#cb5-3"></a><span class="st">    int&lt;lower=0&gt; N;</span></span>
<span id="cb5-4"><a href="#cb5-4"></a><span class="st">    array[N] real y;</span></span>
<span id="cb5-5"><a href="#cb5-5"></a><span class="st">}</span></span>
<span id="cb5-6"><a href="#cb5-6"></a><span class="st">parameters {</span></span>
<span id="cb5-7"><a href="#cb5-7"></a><span class="st">    real mu;</span></span>
<span id="cb5-8"><a href="#cb5-8"></a><span class="st">    real&lt;lower=0&gt; sigma;</span></span>
<span id="cb5-9"><a href="#cb5-9"></a><span class="st">}</span></span>
<span id="cb5-10"><a href="#cb5-10"></a><span class="st">model {</span></span>
<span id="cb5-11"><a href="#cb5-11"></a><span class="st">    // Priors</span></span>
<span id="cb5-12"><a href="#cb5-12"></a><span class="st">    mu ~ normal(160, 10);</span></span>
<span id="cb5-13"><a href="#cb5-13"></a><span class="st">    sigma ~ normal(0, 10);</span></span>
<span id="cb5-14"><a href="#cb5-14"></a><span class="st">    </span></span>
<span id="cb5-15"><a href="#cb5-15"></a><span class="st">    // Likelihood</span></span>
<span id="cb5-16"><a href="#cb5-16"></a><span class="st">    y ~ normal(mu, sigma);</span></span>
<span id="cb5-17"><a href="#cb5-17"></a><span class="st">}</span></span>
<span id="cb5-18"><a href="#cb5-18"></a><span class="st">generated quantities {</span></span>
<span id="cb5-19"><a href="#cb5-19"></a><span class="st">    real prior_mu = normal_rng(160, 10);</span></span>
<span id="cb5-20"><a href="#cb5-20"></a><span class="st">    real&lt;lower=0&gt; prior_sigma = abs(normal_rng(0, 10));</span></span>
<span id="cb5-21"><a href="#cb5-21"></a><span class="st">    array[N] real y_prior_pred;</span></span>
<span id="cb5-22"><a href="#cb5-22"></a><span class="st">    for (i in 1:N) {</span></span>
<span id="cb5-23"><a href="#cb5-23"></a><span class="st">        y_prior_pred[i] = normal_rng(prior_mu, prior_sigma);</span></span>
<span id="cb5-24"><a href="#cb5-24"></a><span class="st">    }</span></span>
<span id="cb5-25"><a href="#cb5-25"></a><span class="st">}</span></span>
<span id="cb5-26"><a href="#cb5-26"></a><span class="st">"""</span></span>
<span id="cb5-27"><a href="#cb5-27"></a></span>
<span id="cb5-28"><a href="#cb5-28"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">"./stan_code.stan"</span>, <span class="st">"w"</span>) <span class="im">as</span> f:</span>
<span id="cb5-29"><a href="#cb5-29"></a>    <span class="bu">print</span>(stan_code, <span class="bu">file</span><span class="op">=</span>f)</span>
<span id="cb5-30"><a href="#cb5-30"></a></span>
<span id="cb5-31"><a href="#cb5-31"></a><span class="co"># Compile the model</span></span>
<span id="cb5-32"><a href="#cb5-32"></a>model <span class="op">=</span> cmdstanpy.CmdStanModel(stan_file<span class="op">=</span><span class="st">"./stan_code.stan"</span>)</span>
<span id="cb5-33"><a href="#cb5-33"></a></span>
<span id="cb5-34"><a href="#cb5-34"></a><span class="co"># Prepare the data</span></span>
<span id="cb5-35"><a href="#cb5-35"></a>stan_data <span class="op">=</span> {</span>
<span id="cb5-36"><a href="#cb5-36"></a>    <span class="st">'N'</span>: <span class="bu">len</span>(y),</span>
<span id="cb5-37"><a href="#cb5-37"></a>    <span class="st">'y'</span>: y</span>
<span id="cb5-38"><a href="#cb5-38"></a>}</span>
<span id="cb5-39"><a href="#cb5-39"></a></span>
<span id="cb5-40"><a href="#cb5-40"></a><span class="co"># Sample from the prior predictive distribution</span></span>
<span id="cb5-41"><a href="#cb5-41"></a>prior_samples <span class="op">=</span> model.sample(</span>
<span id="cb5-42"><a href="#cb5-42"></a>    data<span class="op">=</span>stan_data,</span>
<span id="cb5-43"><a href="#cb5-43"></a>    fixed_param<span class="op">=</span><span class="va">True</span>,  <span class="co"># Sample from priors only</span></span>
<span id="cb5-44"><a href="#cb5-44"></a>    eter_sampling<span class="op">=</span><span class="dv">500</span>,</span>
<span id="cb5-45"><a href="#cb5-45"></a>    seed<span class="op">=</span>SEED</span>
<span id="cb5-46"><a href="#cb5-46"></a>)</span>
<span id="cb5-47"><a href="#cb5-47"></a></span>
<span id="cb5-48"><a href="#cb5-48"></a><span class="co"># Convert to ArviZ</span></span>
<span id="cb5-49"><a href="#cb5-49"></a>dt <span class="op">=</span> azp.from_cmdstanpy(</span>
<span id="cb5-50"><a href="#cb5-50"></a>    prior<span class="op">=</span>prior_samples,</span>
<span id="cb5-51"><a href="#cb5-51"></a>    prior_predictive<span class="op">=</span><span class="st">"y_prior_pred"</span>,</span>
<span id="cb5-52"><a href="#cb5-52"></a>    observed_data<span class="op">=</span>{<span class="st">"y"</span>: y},</span>
<span id="cb5-53"><a href="#cb5-53"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<p>We repeat the prior predictive checks with the new prior predictive distribution. We can see that the bulk of the prior predictive distribution is within the reference values.</p>
<div id="cell-fig-prior_pred_check_narrow" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a>pc <span class="op">=</span> azp.plot_ppc_dist(dt, group<span class="op">=</span><span class="st">"prior_predictive"</span>)</span>
<span id="cb6-2"><a href="#cb6-2"></a>azp.add_lines(pc, values<span class="op">=</span>(<span class="dv">50</span>, <span class="dv">182</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-prior_pred_check_narrow" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-prior_pred_check_narrow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Prior_posterior_predictive_checks_files/figure-html/fig-prior_pred_check_narrow-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-prior_pred_check_narrow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.2: The prior predictive check for the model of heights with a more narrower prior than <a href="#fig-prior_pred_check_wide" class="quarto-xref">Figure&nbsp;<span>5.1</span></a>. Predictions are closer to our domain knowledge about human heights.
</figcaption>
</figure>
</div>
</div>
</div>
<p>You are free to pick other priors and other reference values and make new prior predictive checks. Maybe you can use the historical record for the taller and shorter persons in the world as reference values.</p>
<p>When plotting many distributions, where each one spans a narrow range of values compared to the range spanned but the entire collection of distributions, it is usually a good idea to plot the cumulative distribution instead of KDEs, histograms, or quantile dot plots.</p>
<div id="cell-fig-prior_pred_check_narrow_ecdf" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a>pc <span class="op">=</span> azp.plot_ppc_dist(dt, group<span class="op">=</span><span class="st">"prior_predictive"</span>, kind<span class="op">=</span><span class="st">"ecdf"</span>)</span>
<span id="cb7-2"><a href="#cb7-2"></a>azp.add_lines(pc, values<span class="op">=</span>(<span class="dv">50</span>, <span class="dv">182</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-prior_pred_check_narrow_ecdf" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-prior_pred_check_narrow_ecdf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Prior_posterior_predictive_checks_files/figure-html/fig-prior_pred_check_narrow_ecdf-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-prior_pred_check_narrow_ecdf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.3: The prior predictive check for the model of heights. Same as <a href="#fig-prior_pred_check_narrow" class="quarto-xref">Figure&nbsp;<span>5.2</span></a> but using empirical CDFs instead of KDEs.
</figcaption>
</figure>
</div>
</div>
</div>
<section id="a-final-note-about-priors" class="level3" data-number="5.1.1">
<h3 data-number="5.1.1" class="anchored" data-anchor-id="a-final-note-about-priors"><span class="header-section-number">5.1.1</span> A final note about priors</h3>
<p>Before moving on to the next section, we would like to share one last thought on priors. If you have access to reliable prior knowledge, you should use it, there’s no good reason to discard valid information. But in many real-world scenarios, turning that knowledge into informative priors often require considerable effort and time. And in some cases, they may lead to results that are nearly indistinguishable from those produced using less carefully chosen priors.</p>
<p>In practice weakly informative priors can offer meaningful advantages over both vague and informative priors. Even a modest amount of prior information is often better than none at all, as it helps guard against implausible or misleading results and it could provide computational benefits, such as improved sampling efficiency while being usually easier and less time-consuming to elicit than fully informative priors.</p>
<p>Finally, one benefit that’s often underappreciated is that running prior predictive checks and <em>playing around</em> with different priors can give you valuable insights into your model and the problem you’re trying to solve, regardless of their impact on their direct impact on the posterior. To learn more about prior elicitation, check out <a href="Prior_elicitation.html" class="quarto-xref"><span>Chapter 10</span></a>.</p>
</section>
</section>
<section id="posterior-predictive-checks" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="posterior-predictive-checks"><span class="header-section-number">5.2</span> Posterior predictive checks</h2>
<p>The idea behind posterior predictive checks is very general and simple: if a model is good it should be able generate data resembling the observed data. We call these checks, <em>posterior predictive</em> because we are generating synthetic data after seeing the data.</p>
<p>The general algorithm for posterior predictive checks is:</p>
<ol type="1">
<li>Draw <span class="math inline">\(N\)</span> realizations from the posterior distribution.</li>
<li>For each draw, simulate new data from the likelihood.</li>
<li>Plot the results.</li>
<li>Use observed data to assess whether simulated values agree with observed values.</li>
<li>If simulated values do not agree with observations, change the prior distribution, likelihood, or both and repeat the simulation from step 1.</li>
<li>If simulated values reflect prior knowledge, compute the posterior.</li>
</ol>
<p>Notice that in contrast with prior predictive checks, we use observations here. Of course, we can also include domain knowledge to assess whether the simulated values are reasonable, but because we are using observations we do more stringent evaluations.</p>
<p>In steps 1 and 2 what we are doing is approximating this integral: <span class="math display">\[
p(\tilde y) = \int_{\Theta} p(\tilde y \mid \theta) \; p(\theta \mid y) \; d\theta
\]</span></p>
<p>where <span class="math inline">\(\tilde y\)</span> represents new observations, according to our model. The data generated is predictive since it is the data that the model expects to see.</p>
<p>Notice that what we are doing is marginalizing the likelihood by integrating all possible values ​​of the posterior. Therefore, from the perspective of our model, we are describing the marginal distribution of data, that is, regardless of the values of the parameters.</p>
<p>Continuing with our height example, we can generate synthetic data from the posterior predictive distribution.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-3-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-1" role="tab" aria-controls="tabset-3-1" aria-selected="true">PyMC</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-3-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-2" role="tab" aria-controls="tabset-3-2" aria-selected="false">CmdStanPy</a></li></ul>
<div class="tab-content">
<div id="tabset-3-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-3-1-tab">
<div id="cc2044be" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a><span class="cf">with</span> model: </span>
<span id="cb8-2"><a href="#cb8-2"></a>    dt <span class="op">=</span> pm.sample(idata_kwargs<span class="op">=</span>{<span class="st">"log_likelihood"</span>: <span class="va">True</span>}, random_seed<span class="op">=</span>SEED)</span>
<span id="cb8-3"><a href="#cb8-3"></a>    pm.sample_posterior_predictive(dt, random_seed<span class="op">=</span>SEED, extend_inferencedata<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div id="tabset-3-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-3-2-tab">
<div class="sourceCode" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a>stan_code <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb9-2"><a href="#cb9-2"></a><span class="st">data {</span></span>
<span id="cb9-3"><a href="#cb9-3"></a><span class="st">    int&lt;lower=0&gt; N;</span></span>
<span id="cb9-4"><a href="#cb9-4"></a><span class="st">    array[N] real y;</span></span>
<span id="cb9-5"><a href="#cb9-5"></a><span class="st">}</span></span>
<span id="cb9-6"><a href="#cb9-6"></a><span class="st">parameters {</span></span>
<span id="cb9-7"><a href="#cb9-7"></a><span class="st">    real mu;</span></span>
<span id="cb9-8"><a href="#cb9-8"></a><span class="st">    real&lt;lower=0&gt; sigma;</span></span>
<span id="cb9-9"><a href="#cb9-9"></a><span class="st">}</span></span>
<span id="cb9-10"><a href="#cb9-10"></a><span class="st">model {</span></span>
<span id="cb9-11"><a href="#cb9-11"></a><span class="st">    // Priors</span></span>
<span id="cb9-12"><a href="#cb9-12"></a><span class="st">    mu ~ normal(0, 10);</span></span>
<span id="cb9-13"><a href="#cb9-13"></a><span class="st">    sigma ~ normal(0, 10);</span></span>
<span id="cb9-14"><a href="#cb9-14"></a><span class="st">    </span></span>
<span id="cb9-15"><a href="#cb9-15"></a><span class="st">    // Likelihood</span></span>
<span id="cb9-16"><a href="#cb9-16"></a><span class="st">    y ~ normal(mu, sigma);</span></span>
<span id="cb9-17"><a href="#cb9-17"></a><span class="st">}</span></span>
<span id="cb9-18"><a href="#cb9-18"></a><span class="st">generated quantities {</span></span>
<span id="cb9-19"><a href="#cb9-19"></a><span class="st">    real prior_mu = normal_rng(0, 10);</span></span>
<span id="cb9-20"><a href="#cb9-20"></a><span class="st">    real&lt;lower=0&gt; prior_sigma = abs(normal_rng(0, 10));</span></span>
<span id="cb9-21"><a href="#cb9-21"></a><span class="st">    array[N] real y_prior_pred;</span></span>
<span id="cb9-22"><a href="#cb9-22"></a><span class="st">    for (i in 1:N) {</span></span>
<span id="cb9-23"><a href="#cb9-23"></a><span class="st">        y_prior_pred[i] = normal_rng(prior_mu, prior_sigma);</span></span>
<span id="cb9-24"><a href="#cb9-24"></a><span class="st">    }</span></span>
<span id="cb9-25"><a href="#cb9-25"></a></span>
<span id="cb9-26"><a href="#cb9-26"></a><span class="st">    array[N] real log_lik;</span></span>
<span id="cb9-27"><a href="#cb9-27"></a><span class="st">    for (i in 1:N) {</span></span>
<span id="cb9-28"><a href="#cb9-28"></a><span class="st">        log_lik[i] = normal_lpdf(y[i] | mu, sigma);</span></span>
<span id="cb9-29"><a href="#cb9-29"></a><span class="st">    }</span></span>
<span id="cb9-30"><a href="#cb9-30"></a></span>
<span id="cb9-31"><a href="#cb9-31"></a><span class="st">    array[N] real y_rep;</span></span>
<span id="cb9-32"><a href="#cb9-32"></a><span class="st">    for (i in 1:N) {</span></span>
<span id="cb9-33"><a href="#cb9-33"></a><span class="st">        y_rep[i] = normal_rng(mu, sigma);</span></span>
<span id="cb9-34"><a href="#cb9-34"></a><span class="st">    }</span></span>
<span id="cb9-35"><a href="#cb9-35"></a><span class="st">}</span></span>
<span id="cb9-36"><a href="#cb9-36"></a><span class="st">"""</span></span>
<span id="cb9-37"><a href="#cb9-37"></a></span>
<span id="cb9-38"><a href="#cb9-38"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">"./stan_code.stan"</span>, <span class="st">"w"</span>) <span class="im">as</span> f:</span>
<span id="cb9-39"><a href="#cb9-39"></a>    <span class="bu">print</span>(stan_code, <span class="bu">file</span><span class="op">=</span>f)</span>
<span id="cb9-40"><a href="#cb9-40"></a></span>
<span id="cb9-41"><a href="#cb9-41"></a><span class="co"># Compile the model</span></span>
<span id="cb9-42"><a href="#cb9-42"></a>model <span class="op">=</span> cmdstanpy.CmdStanModel(stan_file<span class="op">=</span><span class="st">"./stan_code.stan"</span>)</span>
<span id="cb9-43"><a href="#cb9-43"></a></span>
<span id="cb9-44"><a href="#cb9-44"></a></span>
<span id="cb9-45"><a href="#cb9-45"></a><span class="co"># Prepare the data</span></span>
<span id="cb9-46"><a href="#cb9-46"></a>stan_data <span class="op">=</span> {</span>
<span id="cb9-47"><a href="#cb9-47"></a>    <span class="st">'N'</span>: <span class="bu">len</span>(y),</span>
<span id="cb9-48"><a href="#cb9-48"></a>    <span class="st">'y'</span>: y</span>
<span id="cb9-49"><a href="#cb9-49"></a>}</span>
<span id="cb9-50"><a href="#cb9-50"></a></span>
<span id="cb9-51"><a href="#cb9-51"></a></span>
<span id="cb9-52"><a href="#cb9-52"></a>prior_samples <span class="op">=</span> model.sample(</span>
<span id="cb9-53"><a href="#cb9-53"></a>    data<span class="op">=</span>stan_data,</span>
<span id="cb9-54"><a href="#cb9-54"></a>    fixed_param<span class="op">=</span><span class="va">True</span>,  <span class="co"># Sample from priors only</span></span>
<span id="cb9-55"><a href="#cb9-55"></a>    iter_sampling<span class="op">=</span><span class="dv">500</span>,</span>
<span id="cb9-56"><a href="#cb9-56"></a>    seed<span class="op">=</span>SEED</span>
<span id="cb9-57"><a href="#cb9-57"></a>)</span>
<span id="cb9-58"><a href="#cb9-58"></a></span>
<span id="cb9-59"><a href="#cb9-59"></a>posterior <span class="op">=</span> model.sample(data<span class="op">=</span>stan_data)</span>
<span id="cb9-60"><a href="#cb9-60"></a></span>
<span id="cb9-61"><a href="#cb9-61"></a><span class="co"># Convert to ArviZ</span></span>
<span id="cb9-62"><a href="#cb9-62"></a>dt <span class="op">=</span> azp.from_cmdstanpy(</span>
<span id="cb9-63"><a href="#cb9-63"></a>    posterior<span class="op">=</span>posterior,</span>
<span id="cb9-64"><a href="#cb9-64"></a>    prior<span class="op">=</span>prior_samples,</span>
<span id="cb9-65"><a href="#cb9-65"></a>    prior_predictive<span class="op">=</span>{<span class="st">"y"</span>:<span class="st">"y_prior_pred"</span>},</span>
<span id="cb9-66"><a href="#cb9-66"></a>    posterior_predictive<span class="op">=</span>{<span class="st">"y"</span>: <span class="st">"y_rep"</span>},</span>
<span id="cb9-67"><a href="#cb9-67"></a>    log_likelihood<span class="op">=</span>{<span class="st">"y"</span>: <span class="st">"log_lik"</span>},</span>
<span id="cb9-68"><a href="#cb9-68"></a>    observed_data<span class="op">=</span>{<span class="st">"y"</span>: y},</span>
<span id="cb9-69"><a href="#cb9-69"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<p>And then we use ArviZ to render the comparison. We can see that the model is doing a good job at predicting the data. The observed data (black line) is within the bulk of the posterior predictive distribution (blue lines).</p>
<div id="cell-fig-post_pred" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a>azp.plot_ppc_dist(dt, num_samples<span class="op">=</span><span class="dv">200</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-post_pred" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-post_pred-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Prior_posterior_predictive_checks_files/figure-html/fig-post_pred-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-post_pred-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.4: Posterior predictive check for the model of heights.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Other common visualizations to compare observed and predictive values are empirical CDFs, histograms and less often quantile dotplots. Like with other types of visualizations, you may want to try different options, to be sure visualizations are not misleading and you may also want to adapt the visualization to your audience.</p>
<section id="using-summary-statistics" class="level3" data-number="5.2.1">
<h3 data-number="5.2.1" class="anchored" data-anchor-id="using-summary-statistics"><span class="header-section-number">5.2.1</span> Using summary statistics</h3>
<p>Besides directly comparing observations and predictions in terms of their densities, we can do comparisons in terms of summary statistics, like the median, the interquartile range, the standard deviation etc. Which ones we decide to use can vary from one data-analysis problem to another, and ideally they should be informed by the data-analysis goals. As in posterior predictive checks we use the data twice, first for fitting the model and then for checking it. It is advisable to select test statistics that are orthogonal to the model parameters <span class="citation" data-cites="Gabry_2019">(<a href="References.html#ref-Gabry_2019" role="doc-biblioref">Gabry et al. 2019</a>)</span>. For example, in a Normal model with a location parameter, the mean should be easy to recover, so a posterior predictive check using the mean as a test statistic would not be a particularly stringent test. As in many common models there is a location parameter, then the mean is usually not a good test statistic.</p>
<p>We can use the <code>plot_ppc_tstat</code> function to display the posterior predictive distribution of a test statistic. The function takes as input a DataTree with a <code>posterior_predictive</code> and <code>observed_data</code> groups and a name of the test statistic (custom function are also allowed), and it will compute the posterior predictive distribution of that statistic. The function also computes the observed value of the test statistic and plots it as a dot at the bottom of each subplot.</p>
<p>The following figure shows a comparison in terms of the mean, median and interquartile range (IQR). The dots at the bottom of each subplots corresponds to the summary statistics computed for the observed data and the KDE is for the model’s predictions.</p>
<div id="cell-fig-post_pred_check_stats" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a>pc <span class="op">=</span> azp.combine_plots(dt,</span>
<span id="cb11-2"><a href="#cb11-2"></a>                 plots<span class="op">=</span>[</span>
<span id="cb11-3"><a href="#cb11-3"></a>                  (azp.plot_ppc_tstat, {<span class="st">"t_stat"</span>:<span class="st">"median"</span>}),</span>
<span id="cb11-4"><a href="#cb11-4"></a>                  (azp.plot_ppc_tstat, {<span class="st">"t_stat"</span>:<span class="st">"mad"</span>}),</span>
<span id="cb11-5"><a href="#cb11-5"></a>                  (azp.plot_ppc_tstat, {<span class="st">"t_stat"</span>:<span class="st">"iqr"</span>}),                   </span>
<span id="cb11-6"><a href="#cb11-6"></a>                 ],</span>
<span id="cb11-7"><a href="#cb11-7"></a>                group<span class="op">=</span><span class="st">"posterior_predictive"</span>,</span>
<span id="cb11-8"><a href="#cb11-8"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-post_pred_check_stats" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-post_pred_check_stats-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Prior_posterior_predictive_checks_files/figure-html/fig-post_pred_check_stats-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-post_pred_check_stats-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.5: Posterior predictive check for the model of heights using summary statistics.
</figcaption>
</figure>
</div>
</div>
</div>
<p>If we want a numerical summary of a posterior predictive checks using test statistics we can compute the proportion of simulated data that is less than or equal to the observed data: <span class="math display">\[
p(T_{\text{sim}} \le T_{\text{obs}} \mid \tilde y)
\]</span></p>
<p>Where <span class="math inline">\(T\)</span> is the summary statistic of our choice, computed for both the observed data <span class="math inline">\(T_{\text{obs}}\)</span> and the simulated data <span class="math inline">\(T_{\text{sim}}\)</span>.</p>
<p>This is known as a posterior predictive p-value (or Bayesian p-value). This is similar to the frequentist p-value, but computed with respect to the posterior predictive distribution, instead of the sample distribution under the null hypothesis.</p>
<p>A posterior predictive p-value of 0.5 indicates that half of the predictions are below the observed values and half above. Posterior predictive p-values do not in general have uniform distributions under the null hypothesis but instead tend to have distributions more concentrated near 0.5 <span class="citation" data-cites="Gelman_2013b">(<a href="References.html#ref-Gelman_2013b" role="doc-biblioref">Gelman 2013</a>)</span>. For instance, we already mentioned that the mean is easy to recover for many models and thus the posterior predictive p-value for the mean is often concentrated around 0.5.</p>
<p>The term “Bayesian p-values” may sound like an oxymoron or paradoxical <span class="citation" data-cites="Meng_1994">(<a href="References.html#ref-Meng_1994" role="doc-biblioref">Meng 1994</a>)</span>. The Bayesian p-values are defined similar to their frequentist cousins and hence the name. But they are used in a very different way. We use posterior predictive p-values as a diagnostic tool to asses potential mismatches between model and data rather than as a measure of “statistical significance” or as a dichotomy decision tool. The null hypothesis is that the predictions from the model and the observed data are drawn from the same data-generating process, but in practice we are not interested in rejecting this hypothesis. We already know is not true! Instead, we are interested in understanding how well the model is doing at predicting the data, detecting potential problems, an if possible or desirable improving the model.</p>
</section>
<section id="pit-ecdfs" class="level3" data-number="5.2.2">
<h3 data-number="5.2.2" class="anchored" data-anchor-id="pit-ecdfs"><span class="header-section-number">5.2.2</span> PIT-ECDFs</h3>
<p>Instead of using a summary statistics, as before, we can directly compare observations and predictions by computing: <span class="math display">\[
p(\tilde y_i \le y_i \mid y)
\]</span></p>
<p>This is often called the marginal p-value and the ideal distribution is the standard uniform distribution. The intuition is that if the model can generate predictions from the same distribution as the observed data, then the observed data can be thought of as just one random sample from the posterior predictive distribution. In this case, the observed data point is equally likely to appear anywhere within the range of the predicted values. This means there’s no systematic bias in where the observation falls, and the p-values derived from comparing the observed data to the predictions will be uniformly distributed.</p>
<p>A more formal justification for this result is provided by the <a href="https://en.wikipedia.org/wiki/Probability_integral_transform">Probability Integral Transform</a> (PIT). This property, also known as the universality of the Uniform distribution, states that if <span class="math inline">\(Y\)</span> is a random variable with a continuous distribution and cumulative distribution function (CDF) <span class="math inline">\(F_Y\)</span>, then the transformed variable</p>
<p><span class="math display">\[
U = F_Y(Y)
\]</span></p>
<p>follows a standard Uniform distribution. A proof of this result can be found in the <a href="https://statproofbook.github.io/P/cdf-pit.html">The Book of Statistical Proofs</a>.</p>
<p>In other words if we apply the CDF of any continuous distribution to a random variable with that distribution, the result will be a random variable with a standard uniform distribution. This is a very powerful result, as it allows us to use the standard uniform distribution as a reference distribution for many statistical tests, including posterior predictive checks.</p>
<p>As mentioned earlier, the marginal p-value is given by</p>
<p><span class="math display">\[
p(\tilde y_i \leq y_i \mid y).
\]</span></p>
<p>If the observed data and predictions are drawn from the same distribution, this expression is then equivalent to the definition of the CDF:</p>
<p><span class="math display">\[
F_Y(y) = \mathrm{Pr}(Y \leq y).
\]</span></p>
<p>Thus, we can see the computation of the marginal p-value as an application of the Probability Integral Transform.</p>
<p>In practice we don’t have the CDF, but this is no problem as we have samples from the posterior predictive and hence we can compute the empirical CDF (ECDF). The CDF of the standard Uniform distribution is a diagonal line that goes from (0, 0) to (1,1), as shown in <a href="#fig-post_pred_uniform_cdf" class="quarto-xref">Figure&nbsp;<span>5.6</span></a>. Deviations from this line may indicate problems with the model. This is a very simple to interpret visualization.</p>
<div id="cell-fig-post_pred_uniform_cdf" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a>pz.Uniform(<span class="dv">0</span>, <span class="dv">1</span>).plot_cdf()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-post_pred_uniform_cdf" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-post_pred_uniform_cdf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Prior_posterior_predictive_checks_files/figure-html/fig-post_pred_uniform_cdf-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-post_pred_uniform_cdf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.6: The CDF of the standard Uniform distribution.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The disadvantage of such visualization is that all the “action” is close to the diagonal line and most of the plot is just blank space. A simple trick to improve the <em>data-ink</em> ratio is to render the difference between the observed and expected cumulative distribution functions, the <span class="math inline">\(\Delta\)</span>-ECDF, as shown in Figure <a href="#fig-post_ppc_pit_ecdf" class="quarto-xref">Figure&nbsp;<span>5.7</span></a>. The last ingredient to improve this visual diagnostic is to add a confidence band. Due to finite sample size we should expect deviations from uniformity, so a confidence band gives us an idea of how much deviation is expected by chance.</p>
<div id="cell-fig-post_ppc_pit_ecdf" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a>azp.plot_ppc_pit(dt)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-post_ppc_pit_ecdf" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-post_ppc_pit_ecdf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Prior_posterior_predictive_checks_files/figure-html/fig-post_ppc_pit_ecdf-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-post_ppc_pit_ecdf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.7: Posterior predictive check for the model of heights using marginal Bayesian p-values, also know as u-values.
</figcaption>
</figure>
</div>
</div>
</div>
<p>In ArviZ, we use the simultaneous confidence bands described by <span class="citation" data-cites="sailynoja_2022">Säilynoja, Bürkner, and Vehtari (<a href="References.html#ref-sailynoja_2022" role="doc-biblioref">2022</a>)</span>. The simultaneous confidence bands take into account the probability of observing deviations of the entire curve, as opposed to independent pointwise deviations. The band or envelope has an oval shape because the probability of observing a deviation is null at 0 and 1, all ECDFs must start at 0 and end at 1, and is higher in the middle of the curve.</p>
<p>To build intuition on how to interpret the PIT-ECDF plots we are going to explore four common patterns using synthetic data. The following three figures show four different scenarios, where the observed data follows a standard normal distribution (<span class="math inline">\(\mu=0, \sigma^2=1\)</span>). In each case, we compare the observed data to predictions where:</p>
<ul>
<li>The mean of the predictions is shifted to the right. The model is overpredicting the data.</li>
<li>The mean of the predictions is shifted to the left. The model is underpredicting the data.</li>
<li>The predictions have a wider spread. The predictions are too uncertain.</li>
<li>The predictions have a narrower spread. The predictions are too certain.</li>
</ul>
<p>First we show the KDEs of the observed data and the predictions.</p>
<div id="cell-fig-post_ppc_pit_ecdf_alt_0" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a>observed <span class="op">=</span> pz.Normal(<span class="dv">0</span>, <span class="dv">1</span>).rvs(<span class="dv">500</span>)</span>
<span id="cb14-2"><a href="#cb14-2"></a></span>
<span id="cb14-3"><a href="#cb14-3"></a>predictions <span class="op">=</span> {}</span>
<span id="cb14-4"><a href="#cb14-4"></a><span class="cf">for</span> i, (mu, sigma) <span class="kw">in</span> <span class="bu">enumerate</span>([</span>
<span id="cb14-5"><a href="#cb14-5"></a>                                (<span class="fl">0.5</span>, <span class="dv">1</span>),  <span class="co"># shifted to the right</span></span>
<span id="cb14-6"><a href="#cb14-6"></a>                                (<span class="op">-</span><span class="fl">0.5</span>, <span class="dv">1</span>), <span class="co"># shifted to the left</span></span>
<span id="cb14-7"><a href="#cb14-7"></a>                                (<span class="dv">0</span>, <span class="dv">2</span>),    <span class="co"># wider </span></span>
<span id="cb14-8"><a href="#cb14-8"></a>                                (<span class="dv">0</span>, <span class="fl">0.5</span>),  <span class="co"># narrower</span></span>
<span id="cb14-9"><a href="#cb14-9"></a>                                ]):</span>
<span id="cb14-10"><a href="#cb14-10"></a>    predictions[<span class="ss">f"y</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>] <span class="op">=</span>  pz.Normal(mu, sigma).rvs((<span class="dv">4</span>, <span class="dv">500</span>, <span class="dv">100</span>))</span>
<span id="cb14-11"><a href="#cb14-11"></a></span>
<span id="cb14-12"><a href="#cb14-12"></a>dt_i <span class="op">=</span> azp.from_dict({</span>
<span id="cb14-13"><a href="#cb14-13"></a>    <span class="st">"posterior_predictive"</span>:predictions,</span>
<span id="cb14-14"><a href="#cb14-14"></a>    <span class="st">"observed_data"</span>: {<span class="ss">f"y</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>: observed <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(predictions))}</span>
<span id="cb14-15"><a href="#cb14-15"></a>})</span>
<span id="cb14-16"><a href="#cb14-16"></a></span>
<span id="cb14-17"><a href="#cb14-17"></a>azp.plot_ppc_dist(dt_i,</span>
<span id="cb14-18"><a href="#cb14-18"></a>                  kind<span class="op">=</span><span class="st">"kde"</span>,  </span>
<span id="cb14-19"><a href="#cb14-19"></a>                  visuals<span class="op">=</span>{<span class="st">"remove_axis"</span>:<span class="va">False</span>},</span>
<span id="cb14-20"><a href="#cb14-20"></a>                  figure_kwargs<span class="op">=</span>{<span class="st">"sharey"</span>:<span class="va">True</span>},             </span>
<span id="cb14-21"><a href="#cb14-21"></a>                 )<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-post_ppc_pit_ecdf_alt_0" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-post_ppc_pit_ecdf_alt_0-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Prior_posterior_predictive_checks_files/figure-html/fig-post_ppc_pit_ecdf_alt_0-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-post_ppc_pit_ecdf_alt_0-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.8: Posterior predictive check with KDEs showing four alternative scenarios.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Then we show the ECDFs of the observed data and the predictions.</p>
<div id="cell-fig-post_ppc_pit_ecdf_alt_1" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a>azp.plot_ppc_dist(dt_i,</span>
<span id="cb15-2"><a href="#cb15-2"></a>                  kind<span class="op">=</span><span class="st">"ecdf"</span>,</span>
<span id="cb15-3"><a href="#cb15-3"></a>                  figure_kwargs<span class="op">=</span>{<span class="st">"sharey"</span>:<span class="va">True</span>},        </span>
<span id="cb15-4"><a href="#cb15-4"></a>                 )<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-post_ppc_pit_ecdf_alt_1" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-post_ppc_pit_ecdf_alt_1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Prior_posterior_predictive_checks_files/figure-html/fig-post_ppc_pit_ecdf_alt_1-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-post_ppc_pit_ecdf_alt_1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.9: Posterior predictive check with ECDFs showing four alternative scenarios.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Finally, we show the PIT-ECDFs.</p>
<div id="cell-fig-post_ppc_pit_ecdf_alt_2" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a>azp.plot_ppc_pit(dt_i,</span>
<span id="cb16-2"><a href="#cb16-2"></a>                 visuals<span class="op">=</span>{<span class="st">"ylabel"</span>:<span class="va">False</span>},</span>
<span id="cb16-3"><a href="#cb16-3"></a>                 figure_kwargs<span class="op">=</span>{<span class="st">"sharey"</span>:<span class="va">True</span>},        </span>
<span id="cb16-4"><a href="#cb16-4"></a>                 )<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-post_ppc_pit_ecdf_alt_2" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-post_ppc_pit_ecdf_alt_2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Prior_posterior_predictive_checks_files/figure-html/fig-post_ppc_pit_ecdf_alt_2-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-post_ppc_pit_ecdf_alt_2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.10: Posterior predictive check with PIT-ECDFs showing four alternative scenarios.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="coverage" class="level3" data-number="5.2.3">
<h3 data-number="5.2.3" class="anchored" data-anchor-id="coverage"><span class="header-section-number">5.2.3</span> Coverage</h3>
<p>The coverage is the proportion of true values that fall within a given prediction interval. For a well-calibrated model, the coverage should match the intended interval width. For example, a 95% credible interval should contain the true value 95% of the time.</p>
<p>For equal-tailed intervals (ETI), the coverage can be obtained by transforming the PIT values, we just need to replace the PIT with two times the absolute difference between the PIT values and 0.5. As with the PIT-ECDFs we saw in the previous section, for a well calibrated model, we should expect the coverage to be uniform and within the confidence envelope.</p>
<p>Using ArviZ we can visualize the ETI coverage by setting <code>coverage=True</code> in the <code>plot_ppc_pit</code> function.</p>
<div id="cell-fig-post_ppc_pit_coverage" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a>azp.plot_ppc_pit(dt_i,</span>
<span id="cb17-2"><a href="#cb17-2"></a>                 coverage<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb17-3"><a href="#cb17-3"></a>                 visuals<span class="op">=</span>{<span class="st">"ylabel"</span>:<span class="va">False</span>},</span>
<span id="cb17-4"><a href="#cb17-4"></a>                 figure_kwargs<span class="op">=</span>{<span class="st">"sharey"</span>:<span class="va">True</span>},</span>
<span id="cb17-5"><a href="#cb17-5"></a>                 )<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-post_ppc_pit_coverage" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-post_ppc_pit_coverage-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Prior_posterior_predictive_checks_files/figure-html/fig-post_ppc_pit_coverage-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-post_ppc_pit_coverage-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.11: Coverage check showing four alternative scenarios.
</figcaption>
</figure>
</div>
</div>
</div>
<ul>
<li>If the difference is <strong>positive</strong>, the model is <strong>under-confident</strong>: the predictions have a wider spread than the data – they are <strong>too uncertain</strong>.</li>
<li>If the difference is <strong>negative</strong>, the model is <strong>over-confident</strong>: the predictions have a narrower spread than the data – they are <strong>too certain</strong>.</li>
</ul>
</section>
<section id="sec-avoid-double-dipping" class="level3" data-number="5.2.4">
<h3 data-number="5.2.4" class="anchored" data-anchor-id="sec-avoid-double-dipping"><span class="header-section-number">5.2.4</span> Avoiding double-dipping</h3>
<p>So far we have being using the data twice, first to fit the model and then to evaluate it. This is a common practice in Bayesian data analysis and it is not a problem as long as we are aware of it. The main goal is to understand how well the model is doing at predicting the data, detecting potential problems, and if possible or desirable improving the model.</p>
<p>Still, we may want to avoid double-dipping. So instead of computing:</p>
<p><span class="math display">\[
p(\tilde y_i \leq y_i \mid y)
\]</span></p>
<p>We may want to compute:</p>
<p><span class="math display">\[
p(\tilde y_i \leq y_i \mid y_{-i})
\]</span></p>
<p>where <span class="math inline">\(y_{-i}\)</span> is the observed data without the <span class="math inline">\(i\)</span>-th observation.</p>
<p>This is a more stringent test, as we are not using the <span class="math inline">\(i\)</span>-th observation to compute the posterior predictive distribution. This is known as the leave-one-out cross-validation (LOO-CV) and it is a very popular method to assess the predictive performance of a model.</p>
<p>In principle computing this will be too costly, as we need to compute the posterior predictive distribution <span class="math inline">\(n\)</span> times, where <span class="math inline">\(n\)</span> is the number of observations. However, we can use a method called Pareto-smoothed importance sampling (PSIS) to approximate the LOO-CV from a single posterior computation. This is a topic we will discuss in more detail in <a href="Model_comparison.html" class="quarto-xref"><span>Chapter 7</span></a>. ArviZ offers many functions based on this method, one of them is <code>loo_pit</code>.</p>
<div id="cell-fig-post_loo_pit" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a>azp.plot_loo_pit(dt)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-post_loo_pit" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-post_loo_pit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Prior_posterior_predictive_checks_files/figure-html/fig-post_loo_pit-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-post_loo_pit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.12: Posterior predictive check with LOO-PIT-ECDF.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="hypothetical-outcome-plots" class="level3" data-number="5.2.5">
<h3 data-number="5.2.5" class="anchored" data-anchor-id="hypothetical-outcome-plots"><span class="header-section-number">5.2.5</span> Hypothetical Outcome Plots</h3>
<p>Another strategy that can be useful for posterior predictive plots is to use animations. For example instead of showing all draws from the posterior predictive at the same time Hypothetical Outcome Plots (HOPs) shows them animated with one just a few draws per frame. HOPs enable a user to experience uncertainty in terms of countable events, just like we experience probability in our day to day lives. You can read more about HOPs <a href="https://medium.com/hci-design-at-uw/hypothetical-outcomes-plots-experiencing-the-uncertain-b9ea60d7c740">here</a>.</p>
</section>
</section>
<section id="posterior-predictive-checks-for-discrete-data" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="posterior-predictive-checks-for-discrete-data"><span class="header-section-number">5.3</span> Posterior predictive checks for discrete data</h2>
<p>So far we have show examples with continuous data. Many of the tools can still be used for discrete data, while KDEs are not useful for discrete data (unless the number of discrete values is large enough to assume continuity), histograms with properly specified bins (like one bin per discrete value) and ECDFs can be used for discrete data. Still, there are some posterior predictive plots that has been specifically designed for discrete data. In the next sections we discuss some of them.</p>
<section id="posterior-predictive-checks-for-count-data" class="level3" data-number="5.3.1">
<h3 data-number="5.3.1" class="anchored" data-anchor-id="posterior-predictive-checks-for-count-data"><span class="header-section-number">5.3.1</span> Posterior predictive checks for count data</h3>
<p>Count data is a type of discrete data that is very common in many fields. For instance, the number of iguanas per square meter in a rainforest, the number of bikes in a bike-sharing station, the number of calls to a call center, the number of emails you got last year, etc. When assessing the fit of a model to count data we need to consider the discreetness of the data and that we usually care about the amount of (over/under-)dispersion.</p>
<p>Rootograms are a graphical tool to assess the fit of count data models <span class="citation" data-cites="tukey_1977 Kleiber_2016">(<a href="References.html#ref-tukey_1977" role="doc-biblioref">Tukey 1977</a>; <a href="References.html#ref-Kleiber_2016" role="doc-biblioref">Kleiber and Zeileis 2016</a>)</span>. There are a few variations of rootograms, but traditionally rootograms use bars for the predicted data, and lines (plus markers) for the observed data. Finally, instead of plotting the raw data, they show the square <em>root</em> of the observed and predicted counts, which explain the name of the plots. The reason to square root the data is to make easier to compare observed and expected frequencies even for low frequencies. Often the uncertainty in the predictions is omitted.</p>
<p>Here we are going to discuss the rootograms presented by <span class="citation" data-cites="Säilynoja_2025">Säilynoja et al. (<a href="References.html#ref-Säilynoja_2025" role="doc-biblioref">2025</a>)</span>. These rootograms emphasises the discreteness of the data and and predictions by using points. The uncertainty in the predictions is encoded using intervals. Instead of square-rooting the data, it set the y-axis on the square root scale, this makes easier to interpret the data, because we can directly read the frecuencies from the y-axis (instead of reading the square roots) while keeping the advantage of being able to discriminate details at lower frecuencies.</p>
<p>To illustrate rootograms we are going to use the Horseshoe crabs dataset <span class="citation" data-cites="Brockmann_1996">(<a href="References.html#ref-Brockmann_1996" role="doc-biblioref">Brockmann 1996</a>)</span>. Very briefly, horseshoe crabs arrive at the beach in pairs for their spawning ritual. Solitary males gather around the nesting couples and vying to fertilize the eggs. These individuals, known as satellite males, often congregate near certain nesting pairs while disregarding others. We used Bambi to create two models a poisson model and a hurdle-negative binomial model for the number of male satellites as a function of the carapace width and color of the female.</p>
<p>We are going to omit the modelling details, and just upload prefitted models.</p>
<div id="3678e127" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a>crabs_poisson <span class="op">=</span> azp.load_arviz_data(<span class="st">'crabs_poisson'</span>)</span>
<span id="cb19-2"><a href="#cb19-2"></a>crabs_hurdle_nb <span class="op">=</span> azp.load_arviz_data(<span class="st">'crabs_hurdle_nb'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s first check the Poisson model. We can see that the overall fit is not that bad, but the zeros are underpredicted, and counts 1 to 4 are overpredicted. Most counts from 6 onward are also underpredicted. This pattern is an indication of overdispersion in the data, and the huge difference for 0 indicates an excess of zeros.</p>
<div id="cell-fig-ppc_rootogram_crab_poisson" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1"></a>pc <span class="op">=</span> azp.plot_ppc_rootogram(crabs_poisson)</span>
<span id="cb20-2"><a href="#cb20-2"></a>pc.viz.plot[<span class="st">"satellite"</span>].item().set_xlim(<span class="op">-</span><span class="fl">0.5</span>, <span class="dv">20</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-ppc_rootogram_crab_poisson" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ppc_rootogram_crab_poisson-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Prior_posterior_predictive_checks_files/figure-html/fig-ppc_rootogram_crab_poisson-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ppc_rootogram_crab_poisson-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.13: Rootogram showing the uncertainty in the predictions for a Poisson model.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Now we will check the fit for the hurdle model. As expected for a hurdle model we get a perfect fit for the zeros. For the positive values, we still get some deviations, but the fit is better than with the Poisson model.</p>
<div id="cell-fig-ppc_rootogram_crab_hurdle_nb" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a>pc <span class="op">=</span> azp.plot_ppc_rootogram(crabs_hurdle_nb)</span>
<span id="cb21-2"><a href="#cb21-2"></a>pc.viz.plot[<span class="st">"satellite"</span>].item().set_xlim(<span class="op">-</span><span class="fl">0.5</span>, <span class="dv">20</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-ppc_rootogram_crab_hurdle_nb" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ppc_rootogram_crab_hurdle_nb-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Prior_posterior_predictive_checks_files/figure-html/fig-ppc_rootogram_crab_hurdle_nb-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ppc_rootogram_crab_hurdle_nb-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.14: Rootogram showing the uncertainty in the predictions for a Hurdle Negative Binomial model.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Both models predict more values in the tail than observed, even if with low probability. For both plots, we restrict the x-range to (0, 20).</p>
</section>
<section id="posterior-predictive-checks-for-binary-data" class="level3" data-number="5.3.2">
<h3 data-number="5.3.2" class="anchored" data-anchor-id="posterior-predictive-checks-for-binary-data"><span class="header-section-number">5.3.2</span> Posterior predictive checks for binary data</h3>
<p>Binary data is a common form of discrete data, often used to represent outcomes like yes/no, success/failure, or 0/1. We may be tempted to asses the fit of a binary model using a bar plot, or a plot similar to the rootogram we showed in the previous section, but this is not a good idea. The reason is that even a very simple model with one parameter for the proportion of one class (like an intercept), can perfectly model that proportion <span class="citation" data-cites="Säilynoja_2025">(<a href="References.html#ref-Säilynoja_2025" role="doc-biblioref">Säilynoja et al. 2025</a>)</span>. Then a bar plot will not be able to tell much about the quality of our model.</p>
<p>One solution to this challenge is to use the so call calibration or reliability plots. To create this kind of plot we first bin the predicted probabilities (e.g., [0.0–0.1], [0.1–0.2], …, [0.9–1.0]) and then for each bin we compute the fraction of observed positive outcomes. In this way we can compare the predicted probabilities to the observed frequencies per bin. The ideal calibration plot is a diagonal line, where the predicted probabilities are equal to the observed frequencies. The problem with this approach is that in practice we don’t have good rules to select the bins and different bins can result in plots that look drastically different.</p>
<p>A more robust and simple to use method, that does not rely on binning the data has been proposed by <span class="citation" data-cites="Dimitriadis_2021">Dimitriadis, Gneiting, and Jordan (<a href="References.html#ref-Dimitriadis_2021" role="doc-biblioref">2021</a>)</span>. <a href="#fig-ppc_pava" class="quarto-xref">Figure&nbsp;<span>5.15</span></a> shows one example of this method. As previously mentioned, the ideal calibration plot is a diagonal line, where the predicted probabilities are equal to the observed frequencies. If the line is above the diagonal, the model is underestimating the probabilities, and if the line is below the diagonal, the model is overestimating the probabilities.The confidence bands are computed using the method proposed by <span class="citation" data-cites="Dimitriadis_2021">Dimitriadis, Gneiting, and Jordan (<a href="References.html#ref-Dimitriadis_2021" role="doc-biblioref">2021</a>)</span>.</p>
<div id="cell-fig-ppc_pava" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1"></a>dt <span class="op">=</span> azp.load_arviz_data(<span class="st">'anes'</span>)</span>
<span id="cb22-2"><a href="#cb22-2"></a></span>
<span id="cb22-3"><a href="#cb22-3"></a>azp.plot_ppc_pava(dt)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-ppc_pava" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ppc_pava-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Prior_posterior_predictive_checks_files/figure-html/fig-ppc_pava-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ppc_pava-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.15: PAV-adjusted Calibration plot for a logistic regression model.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The y-axis of <a href="#fig-ppc_pava" class="quarto-xref">Figure&nbsp;<span>5.15</span></a> is labeled as CEP, short for “conditional event probabilities.” A CEP represents the probability that an event occurs, given that the classifier assigned a particular predicted probability. These probabilities are computed using the pool adjacent violators algorithm <span class="citation" data-cites="Ayer_1955">(<a href="References.html#ref-Ayer_1955" role="doc-biblioref">Ayer et al. 1955</a>)</span>, also known as the PAV-adjusted method—hence the name of the corresponding function in ArviZ. This algorithm ensures that CEPs are monotonic: they either increase or remain constant as the predicted probabilities increase, but never decrease. This monotonicity assumption is reasonable for calibrated models, where higher predicted probabilities should correspond to higher actual event probabilities.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-Ayer_1955" class="csl-entry" role="listitem">
Ayer, Miriam, H. D. Brunk, G. M. Ewing, W. T. Reid, and Edward Silverman. 1955. <span>“<span class="nocase">An Empirical Distribution Function for Sampling with Incomplete Information</span>.”</span> <em>The Annals of Mathematical Statistics</em> 26 (4): 641–47. <a href="https://doi.org/10.1214/aoms/1177728423">https://doi.org/10.1214/aoms/1177728423</a>.
</div>
<div id="ref-Brockmann_1996" class="csl-entry" role="listitem">
Brockmann, H. Jane. 1996. <span>“Satellite Male Groups in Horseshoe Crabs, Limulus Polyphemus.”</span> <em>Ethology</em> 102 (1): 1–21. <a href="https://doi.org/10.1111/j.1439-0310.1996.tb01099.x">https://doi.org/10.1111/j.1439-0310.1996.tb01099.x</a>.
</div>
<div id="ref-Dimitriadis_2021" class="csl-entry" role="listitem">
Dimitriadis, Timo, Tilmann Gneiting, and Alexander I. Jordan. 2021. <span>“Stable Reliability Diagrams for Probabilistic Classifiers.”</span> <em>Proceedings of the National Academy of Sciences</em> 118 (8): e2016191118. <a href="https://doi.org/10.1073/pnas.2016191118">https://doi.org/10.1073/pnas.2016191118</a>.
</div>
<div id="ref-Gabry_2019" class="csl-entry" role="listitem">
Gabry, Jonah, Daniel Simpson, Aki Vehtari, Michael Betancourt, and Andrew Gelman. 2019. <span>“Visualization in Bayesian Workflow.”</span> <em>Journal of the Royal Statistical Society Series A: Statistics in Society</em> 182 (2): 389–402. <a href="https://doi.org/10.1111/rssa.12378">https://doi.org/10.1111/rssa.12378</a>.
</div>
<div id="ref-Gelman_2013b" class="csl-entry" role="listitem">
Gelman, Andrew. 2013. <span>“<span class="nocase">Two simple examples for understanding posterior p-values whose distributions are far from uniform</span>.”</span> <em>Electronic Journal of Statistics</em> 7 (none): 2595–2602. <a href="https://doi.org/10.1214/13-EJS854">https://doi.org/10.1214/13-EJS854</a>.
</div>
<div id="ref-Kleiber_2016" class="csl-entry" role="listitem">
Kleiber, Christian, and Achim Zeileis. 2016. <span>“Visualizing Count Data Regressions Using Rootograms.”</span> <em>The American Statistician</em> 70 (3): 296–303. <a href="https://doi.org/10.1080/00031305.2016.1173590">https://doi.org/10.1080/00031305.2016.1173590</a>.
</div>
<div id="ref-Meng_1994" class="csl-entry" role="listitem">
Meng, Xiao-Li. 1994. <span>“<span>Posterior Predictive <span class="math inline">\(p\)</span>-Values</span>.”</span> <em>The Annals of Statistics</em> 22 (3): 1142–60. <a href="https://doi.org/10.1214/aos/1176325622">https://doi.org/10.1214/aos/1176325622</a>.
</div>
<div id="ref-sailynoja_2022" class="csl-entry" role="listitem">
Säilynoja, Teemu, Paul-Christian Bürkner, and Aki Vehtari. 2022. <span>“Graphical Test for Discrete Uniformity and Its Applications in Goodness-of-Fit Evaluation and Multiple Sample Comparison.”</span> <em>Statistics and Computing</em> 32 (2): 32. <a href="https://doi.org/10.1007/s11222-022-10090-6">https://doi.org/10.1007/s11222-022-10090-6</a>.
</div>
<div id="ref-Säilynoja_2025" class="csl-entry" role="listitem">
Säilynoja, Teemu, Andrew R. Johnson, Osvaldo A. Martin, and Aki Vehtari. 2025. <span>“Recommendations for Visual Predictive Checks in Bayesian Workflow.”</span> <a href="https://arxiv.org/abs/2503.01509">https://arxiv.org/abs/2503.01509</a>.
</div>
<div id="ref-tukey_1977" class="csl-entry" role="listitem">
Tukey, John W. 1977. <em>Exploratory <span>Data</span> <span>Analysis</span></em>. 1 edition. Pearson.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../Chapters/MCMC_diagnostics.html" class="pagination-link" aria-label="MCMC Diagnostics">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">MCMC Diagnostics</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../Chapters/Sensitivity_checks.html" class="pagination-link" aria-label="Prior and likelihood sensitivity checks">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Prior and likelihood sensitivity checks</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>