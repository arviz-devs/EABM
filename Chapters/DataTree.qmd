
# Working with DataTree  {#sec-working-datatree}

```{python}
#| echo : false
#| warning: false
import arviz.preview as azp
import numpy as np
import preliz as pz
import matplotlib.pyplot as plt
import xarray as xr
xr.set_options(display_expand_data=False, display_expand_attrs=False)
np.random.seed(19)
azp.style.use("arviz-variat")
plt.rcParams["figure.dpi"] = 100
```



During a modern Bayesian analysis we usually generate many sets of data including posterior samples, prior/posterior predictive samples, statistics generated by the sampling method, etc. To keep all this data tidy and avoid confusion ArviZ relies on the data-structures provided by xarray [@xarray_2017]. If you are not familiar with xarray this chapter introduces some basic elements in the context of Bayesian stats. For a deeper understanding we recommend that you check their [xarray's documentation](https://docs.xarray.dev/en/stable/), you may find xarray useful for problems outside Bayesian analysis.

We need to become familiar with 3 Data Structures:

* `DataArray`: A labelled, N-dimensional array. In other words this is like NumPy but you can access the data using meaningful labels instead of numerical indexes. You may also think of this as the N-D generalization of a pandas or polars `Series`.
* `DataSet`: It is a dict-like container of DataArray objects aligned along any number of shared dimensions. You may also think of this as the N-D generalization of a pandas or polars `DataFrame`.
* `DataTree`: This is a container of DataSets, each DataSet is associated with a group.

The best way to understand these data-structures is to explore them. In a real scenario DataTree objects will be generated by probabilistic programming languages (ppls), or ArviZ will be used to transform the output of ppls into DataTree. But we don't need to fit a model to play with DataTree objects, ArviZ comes equipped with a few DataTree objects. Let's start by loading the `centered_eight` DataTree. 

```{python}
dt = azp.load_arviz_data("centered_eight")
```

In the context of Bayesian Stats a `DataTree` has groups like `posterior`, `observed_data`, `posterior_predictive`, `log_likelihood`, etc. 

```{python}
dt.groups
```

DataTree/DataSet/DataArray objects have a nice representation in Jupyter notebooks, you can see the content of the `dt` object by just writing its name in a cell.

```{python}
dt
```

This is an HTML representation of a DataTree, so if you are reading this from a browser you should be able to interact with it. If you click on `Groups`, all the groups will be expanded. 
 
An important concept is that of `dimensions` and `coordinates`. If your data was geographical data, like things related to maps, then `dimensions` would be like latitude and longitude, and `coordinates` would be the actual values of latitude and longitude. The xarray documentation is full of examples related to maps. But the idea is very general and applies to any kind of data.

Let's see the dimensions and coordinates for the `posterior` in our `dt` object. We can see 3 dimensions `chain`, `draw`, and `school`. As usual the posterior group of a DataTree object will be generated from a MCMC sampler. The `chain` dimension is used to index the different chains of the MCMC sampler, the `draw` dimension is used to index the different samples generated by the MCMC sampler. The `chain` and `draw` dimensions are ubiquitous, you will see them essentially in any DataTree object when working with ArviZ. The coordinates for `chain` are the integers `[0, 1, 2, 3]` and for `draw` are the integers `[0, 1, 2, ..., 499]`. Then, we also have the `school` dimension. This problem-specific, we have it here because the model from which the posterior was generated has a parameter conditional on school. The coordinates for `school` are the names of the schools,  if you click on the {{< fa database >}} symbol by the `school` coordinate, you will be able to see the names of each school. They are: `['Choate', 'Deerfield', 'Phillips Andover', 'Phillips Exeter', 'Hotchkiss', 'Lawrenceville', "St. Paul's", 'Mt. Hermon']`


### Get the dataset corresponding to a single group

We can access each group using a dictionary-like notation:

```{python}
dt["posterior"]
```

Alternatively, we can use the dot notation, as groups are attributes of the DataTree. For instance, to access the posterior group we can write: 

```{python}
dt.posterior;
```

The dot notation works at the group level and for DataSets and DataArrays as long as there is no conflict with a method or attribute of these objects. If there is a conflict, you can always use the dictionary-like notation.

Notice that we still get a DataTree, but with 0 groups. If you want the DataSet you can do.

```{python}
dt["posterior"].to_dataset()
```

### Get coordinate values

As we have seen, we have 8 schools with their names. If we want to programmatically access the names we can do

```{python}
dt.posterior.school
```

Which returns a DataArray with the names of the schools. To obtain a NumPy array we can do


```{python}
dt.posterior.school.values
```

If we want to get the number of schools we can write:

```{python}
len(dt.observed_data.school)
```

Notice that we do not need to first obtain the NumPy array and then compute the length. When working with DataTree/Sets/Arrays, you may feel tempted to reduce them to NumPy arrays, as you are more familiar with those. But for many problems that is not needed and for many other that is not even recommended as you may loose the benefit of working with labeled array-like structures.

### Get a subset of chains

Because we have labels for the names of the schools we can use them to access their associated information. Labels are usually much easier to remember than numerical indices. For instance, to access the posterior samples of the school `Choate` we can write:

```{python}
dt.posterior.sel(school="Choate")
```

The `draw` and `chain` coordinates are indexed using numbers, the following code will return the last draw from chain 1 and chain 2:

```{python}
dt.posterior.sel(draw=499, chain=[1, 2])
```

Usually, in Bayesian statistics, we don't need to access individual draws or chains, a more common operation is to select a range. For that purpose, we can use Python's `slice` function. For example, the following line of code returns the first 200 draws from all chains:

```{python}
dt.posterior.sel(draw=slice(0, 200))
```

Using the `slice` function we can also remove the first 100 draws.

```{python}
dt.posterior.sel(draw=slice(100, None))
```
If we try the same operation with the entire DataTree, we will get an error. The reason being that some groups don't have the `draw` dimension, for instance the `observed_data` group has only the `school` dimension. What we can do instead is to filter those group that have  `draw` and then perform the selection and slicing.

```{python}
dt.filter(lambda node: "draw" in node.dims).sel(draw=slice(100, None))
```

If you check the object you will see that the groups `posterior`, `posterior_predictive`, `log_likelihood`, `sample_stats`, `prior`, and `prior_predictive` have now 400 draws compared to the original 500, while the group `observed_data` remains unaffected.

### Computing the posterior mean 

A common operation is to take the mean of the posterior samples. We can do this by calling the `mean` method. For instance, to compute the mean of the first 200 draws we can write:

```{python}
dt.posterior.sel(draw=slice(0, 200)).mean()
```
Other common operations like `std`, `median`, `min`, `max`, `sum`, etc. are also available. 

In NumPy, it is common to perform operations along given axis. We can do the same by specifying the dimension along which we want to operate. For instance, to compute the mean along the `draw` dimension we can write:

```{python}
dt.posterior.mean("draw")
```

This returns the mean for each chain and school. Can you anticipate what is going to be the result if we use  `chain` instead of `draw`? And what about if we use `school`? 

We can also specify multiple dimensions. For instance, a very useful operation is to compute the mean along the `draw` and `chain`, as this will give us the mean for each school.

```{python}
dt.posterior.mean(["chain", "draw"])
```

### Combine chains and draws

Our primary goal usually is to obtain posterior samples and thus we aren't concerned with chains and draws. In those cases, we can use the `azp.extract` function. This combines the `chain` and `draw` into a `sample` coordinate which can make further operations easier. By default, `azp.extract` works on the posterior, but you can specify other groups using the `group` argument. 

```{python}
azp.extract(dt)
```

You can achieve the same result using `dt.posterior.stack(sample=("chain", "draw"))`. But `extract` can be more flexible because it takes care of the most common subsetting operations with MCMC samples. It can:

* Combine `chains` and `draws`
* Return a subset of variables (with optional filtering with regular expressions or string matching)
* Return a subset of samples. Moreover, by default, it returns a random subset to prevent getting non-representative samples due to bad mixing.
* Access any group

To get a subsample we can specify the number of samples we want with the `num_samples` argument. For instance, to get 100 samples we can write:

```{python}
azp.extract(dt, num_samples=100);
```

If you need to extract subsets from multiple groups, you should use a random seed. This will ensure that subsamples match. For example, if you do

```{python}
posterior = azp.extract(dt, num_samples=100, random_seed=124)
ll = azp.extract(dt, group="log_likelihood", num_samples=100, random_seed=124)
```

You can inspect the samples in the `posterior` and `ll` variables and see that they match.

## Ploting

Xarray has some plotting capabilities, for instance, we can do:

```{python}
dt.posterior["mu"].plot.hist(figsize=(9, 3));
```

But in most scenarios calling a plotting function from ArviZ and passing the DataTree as an argument will be a much better idea.


## Add a new variable

We can add variables to existing groups. For instance, we may want to transform a parameter from the posterior. Like computing and adding the $\log$ of the parameter $\tau$ to the posterior group.

```{python}
posterior["log_tau"] = np.log(posterior["tau"])
posterior
```

## Advance operations with DataTrees

Now we delve into more advanced operations with DataTree. While these operations are not essential to use ArviZ, they can be useful in some cases. Exploring these advanced functionalities will help you become more familiar with DataTree and provide additional insights that may enhance your overall experience with ArviZ.


### Compute and store posterior pushforward quantities

We use "posterior push-forward quantities" to refer to quantities that are not variables in the posterior but deterministic computations using posterior variables. 

You can use xarray for these push-forward operations and store them as a new variable in the posterior group. You'll then be able to plot them with ArviZ functions, calculate stats and diagnostics on them (like `mcse`), or save and share the DataTree object with the push forward quantities included. 

The first thing we are going to do is to store the `posterior` group in a variable called `post` to make the code more readable. And to compute the log of $\tau$.

```{python}
post = dt.posterior
post["log_tau"] = np.log(post["tau"])
```

Compute the rolling mean of $\log(\tau)$ with `xarray.DataArray.rolling`, storing the result in the posterior:


```{python}
post["mlogtau"] = post["log_tau"].rolling({"draw": 50}).mean()
``` 

Using xarray for push-forward calculations has all the advantages of working with xarray. It also inherits the disadvantages of working with xarray, but we believe those to be outweighed by the advantages, and we have already shown how to extract the data as NumPy arrays. 

Some examples of these advantages are specifying operations with named dimensions instead of positional ones (as seen in some previous sections),  automatic alignment and broadcasting of arrays (as we'll see now), or integration with Dask (as shown in the [dask_for_arviz](https://python.arviz.org/en/latest/user_guide/Dask.html) guide).

In this cell, you will compute pairwise differences between schools on their mean effects (variable `theta`).
To do so, subtract the variable theta after renaming the school dimension to the original variable. Xarray then aligns and broadcasts the two variables because they have different dimensions, and the result is a 4D variable with all the pointwise differences.

Eventually, store the result in the `theta_school_diff` variable. Notice that the `theta_school_diff` variable in the posterior has kept the named dimensions and coordinates:

```{python}
post["theta_school_diff"] = post.theta - post.theta.rename(school="school_bis")
post
```

::: {.callout-note}
This same operation using NumPy would require manual alignment of the two arrays to make sure they broadcast correctly. The code could be something like:

```python
theta_school_diff = theta[:, :, :, None] - theta[:, :, None, :]
```
:::

### Advanced subsetting

To select the value corresponding to the difference between the Choate and Deerfield schools do:

```{python}
post["theta_school_diff"].sel(school="Choate", school_bis="Deerfield")
```

For more advanced subsetting (the equivalent to what is sometimes called "fancy indexing" in NumPy) you need to provide the indices as `DataArray` objects:

```{python}
school_idx = xr.DataArray(["Choate", "Hotchkiss", "Mt. Hermon"], dims=["pairwise_school_diff"])
school_bis_idx = xr.DataArray(
    ["Deerfield", "Choate", "Lawrenceville"], dims=["pairwise_school_diff"]
)
post["theta_school_diff"].sel(school=school_idx, school_bis=school_bis_idx)
```

Using lists or NumPy arrays instead of DataArrays does column/row-based indexing. As you can see, the result has 9 values of `theta_school_diff` instead of the 3 pairs of difference we selected in the previous cell:

```{python}
post["theta_school_diff"].sel(
    school=["Choate", "Hotchkiss", "Mt. Hermon"],
    school_bis=["Deerfield", "Choate", "Lawrenceville"],
)
```


### Add new chains using concat

After checking the `mcse` and realizing you need more samples, you rerun the model with two chains and obtain an `dt_rerun` object.


```{python}
dt_rerun = (
    dt.posterior.sel(chain=[0, 1])
    .copy()
    .to_dataset()
    .assign_coords(coords={"chain": [4, 5]})
)
```

You can combine the two into a single DataTree object using the `concat` function from ArviZ:

```{python}
dt_complete = xr.concat([dt.posterior.to_dataset(), dt_rerun], dim="chain")
dt_complete.dims
```


###  Add groups to DataTrees

This will be simplified in the future, but for now, you can add groups to a DataTree by converting the DataTree to a dictionary, adding the new group, and then converting the dictionary back to a DataTree.

```{python}
rng = np.random.default_rng(3)
ds = azp.dict_to_dataset(
    {"obs": rng.normal(size=(4, 500, 2))},
    dims={"obs": ["new_school"]},
    coords={"new_school": ["Essex College", "Moordale"]},
)
dicto = {k:v for k,v in dt.items()}
dicto["predictions"] = ds

new_dt = xr.DataTree.from_dict(dicto)
new_dt
```
